<doc id="10916" url="https://en.wikipedia.org/wiki?curid=10916" title="FAQ">
FAQ

A frequently asked questions (FAQ) forum is often used in articles, websites, email lists, and online forums where common questions tend to recur, for example through posts or queries by new users related to common knowledge gaps. The purpose of an FAQ is generally to provide information on frequent questions or concerns; however, the format is a useful means of organizing information, and text consisting of questions and their answers may thus be called an FAQ regardless of whether the questions are actually "frequently" asked.
Since the acronym "FAQ" originated in textual media, its pronunciation varies. FAQ is most commonly pronounced as an initialism, "F-A-Q", but may also be pronounced as an acronym, "FAQ". Web page designers often label a single list of questions as an "FAQ", such as on Google Search, while using "FAQs" to denote multiple lists of questions such as on United States Treasury sites. Use of "FAQ" to refer to a single frequently asked question, in and of itself, is less common.
While the name may be recent, the FAQ format itself is quite old. For example, Matthew Hopkins wrote "The Discovery of Witches" in 1647 as a list of questions and answers, introduced as "Certain Queries answered". Many old catechisms are in a question-and-answer (Q&A) format. "Summa Theologica", written by Thomas Aquinas in the second half of the 13th century, is a series of common questions about Christianity to which he wrote a series of replies. Plato's dialogues are even older.
The "FAQ" is an Internet textual tradition originating from the technical limitations of early mailing lists from NASA in the early 1980s. The first FAQ developed over several pre-Web years, starting from 1982 when storage was expensive. On ARPANET's SPACE mailing list, the presumption was that new users would download archived past messages through FTP. In practice this rarely happened, and the users tended to post questions to the mailing list instead of searching its archives. Repeating the "right" answers became tedious, and went against developing netiquette. A series of different measures were set up by loosely affiliated groups of computer system administrators, from regularly posted messages to netlib-like query email daemons. The acronym "FAQ" was developed between 1982 and 1985 by Eugene Miya of NASA for the SPACE mailing list. The format was then picked up on other mailing lists and Usenet newsgroups. Posting frequency changed to monthly, and finally weekly and daily across a variety of mailing lists and newsgroups. The first person to post a weekly FAQ was Jef Poskanzer to the Usenet net.graphics/comp.graphics newsgroups. Eugene Miya experimented with the first daily FAQ.
In some cases, informative documents not in the traditional FAQ style have also been described as FAQs, particularly the video game FAQ, which is often a detailed description of gameplay, including tips, secrets, and beginning-to-end guidance. Rarely are videogame FAQs in a question-and-answer format, although they may contain a short section of questions and answers.
Over time, the accumulated FAQs across all Usenet newsgroups sparked the creation of the "*.answers" moderated newsgroups such as comp.answers, misc.answers and sci.answers for crossposting and collecting FAQ across respective comp.*, misc.*, sci.* newsgroups.
The FAQ has become an important component of websites, either as a stand-alone page or as a website section with multiple subpages per question or topic. Embedded links to FAQ pages have become commonplace in website navigation bars, bodies, or footers. The FAQ page is an important consideration in web design, in order to achieve several goals of customer service and search engine optimization (SEO), including
Some content providers discourage the use of FAQs in place of restructuring content under logical headings. For example, the UK Government Digital Service does not use FAQs.

</doc>
<doc id="10918" url="https://en.wikipedia.org/wiki?curid=10918" title="Fibonacci number">
Fibonacci number

In mathematics, the Fibonacci numbers, commonly denoted , form a sequence, called the Fibonacci sequence, such that each number is the sum of the two preceding ones, starting from 0 and 1. That is,
and
for .
The beginning of the sequence is thus:
In some older books, the value formula_4 is omitted, so that the sequence starts with formula_5 and the recurrence formula_6 is valid for . 
Fibonacci numbers are strongly related to the golden ratio: Binet's formula expresses the th Fibonacci number in terms of and the golden ratio, and implies that the ratio of two consecutive Fibonacci numbers tends to the golden ratio as increases.
Fibonacci numbers are named after Italian mathematician Leonardo of Pisa, later known as Fibonacci. In his 1202 book "Liber Abaci", Fibonacci introduced the sequence to Western European mathematics, although the sequence had been described earlier in Indian mathematics, as early as 200 BC in work by Pingala on enumerating possible patterns of Sanskrit poetry formed from syllables of two lengths. 
Fibonacci numbers appear unexpectedly often in mathematics, so much so that there is an entire journal dedicated to their study, the "Fibonacci Quarterly". Applications of Fibonacci numbers include computer algorithms such as the Fibonacci search technique and the Fibonacci heap data structure, and graphs called Fibonacci cubes used for interconnecting parallel and distributed systems.
They also appear in biological settings, such as branching in trees, the arrangement of leaves on a stem, the fruit sprouts of a pineapple, the flowering of an artichoke, an uncurling fern, and the arrangement of a pine cone's bracts.
Fibonacci numbers are also closely related to Lucas numbers formula_7, in that the Fibonacci and Lucas numbers form a complementary pair of Lucas sequences: formula_8 and formula_9.
The Fibonacci sequence appears in Indian mathematics in connection with Sanskrit prosody, as pointed out by Parmanand Singh in 1986. In the Sanskrit poetic tradition, there was interest in enumerating all patterns of long (L) syllables of 2 units duration, juxtaposed with short (S) syllables of 1 unit duration. Counting the different patterns of successive L and S with a given total duration results in the Fibonacci numbers: the number of patterns of duration units is .
Knowledge of the Fibonacci sequence was expressed as early as Pingala ( 450 BC–200 BC). Singh cites Pingala's cryptic formula "misrau cha" ("the two are mixed") and scholars who interpret it in context as saying that the number of patterns for beats () is obtained by adding one [S] to the cases and one [L] to the cases.
Bharata Muni also expresses knowledge of the sequence in the "Natya Shastra" (c. 100 BC–c. 350 AD).
However, the clearest exposition of the sequence arises in the work of Virahanka (c. 700 AD), whose own work is lost, but is available in a quotation by Gopala (c. 1135):
Variations of two earlier meters [is the variation]... For example, for [a meter of length] four, variations of meters of two [and] three being mixed, five happens. [works out examples 8, 13, 21]... In this way, the process should be followed in all "mātrā-vṛttas" [prosodic combinations].
Hemachandra (c. 1150) is credited with knowledge of the sequence as well, writing that "the sum of the last and the one before the last is the number ... of the next mātrā-vṛtta."
Outside India, the Fibonacci sequence first appears in the book "Liber Abaci" (1202) by Fibonacci where it is used to calculate the growth of rabbit populations. Fibonacci considers the growth of an idealized (biologically unrealistic) rabbit population, assuming that: a newly born breeding pair of rabbits are put in a field; each breeding pair mates at the age of one month, and at the end of their second month they always produce another pair of rabbits; and rabbits never die, but continue breeding forever. Fibonacci posed the puzzle: how many pairs will there be in one year?
At the end of the th month, the number of pairs of rabbits is equal to the number of mature pairs (that is, the number of pairs in month ) plus the number of pairs alive last month (month ). The number in the th month is the th Fibonacci number.
The name "Fibonacci sequence" was first used by the 19th-century number theorist Édouard Lucas.
Joseph Schillinger (1895–1943) developed a system of composition which uses Fibonacci intervals in some of its melodies; he viewed these as the musical counterpart to the elaborate harmony evident within nature.
Fibonacci sequences appear in biological settings, such as branching in trees, arrangement of leaves on a stem, the fruitlets of a pineapple, the flowering of artichoke, an uncurling fern and the arrangement of a pine cone, and the family tree of honeybees. Kepler pointed out the presence of the Fibonacci sequence in nature, using it to explain the (golden ratio-related) pentagonal form of some flowers. Field daisies most often have petals in counts of Fibonacci numbers. In 1754, Charles Bonnet discovered that the spiral phyllotaxis of plants were frequently expressed in Fibonacci number series.
Przemysław Prusinkiewicz advanced the idea that real instances can in part be understood as the expression of certain algebraic constraints on free groups, specifically as certain Lindenmayer grammars.
A model for the pattern of florets in the head of a sunflower was proposed by in 1979. This has the form
where is the index number of the floret and is a constant scaling factor; the florets thus lie on Fermat's spiral. The divergence angle, approximately 137.51°, is the golden angle, dividing the circle in the golden ratio. Because this ratio is irrational, no floret has a neighbor at exactly the same angle from the center, so the florets pack efficiently. Because the rational approximations to the golden ratio are of the form , the nearest neighbors of floret number are those at for some index , which depends on , the distance from the center. Sunflowers and similar flowers most commonly have spirals of florets in clockwise and counter-clockwise directions in the amount of adjacent Fibonacci numbers, typically counted by the outermost range of radii.
Fibonacci numbers also appear in the pedigrees of idealized honeybees, according to the following rules:
Thus, a male bee always has one parent, and a female bee has two. If one traces the pedigree of any male bee (1 bee), he has 1 parent (1 bee), 2 grandparents, 3 great-grandparents, 5 great-great-grandparents, and so on. This sequence of numbers of parents is the Fibonacci sequence. The number of ancestors at each level, , is the number of female ancestors, which is , plus the number of male ancestors, which is . This is under the unrealistic assumption that the ancestors at each level are otherwise unrelated.
It has been noticed that the number of possible ancestors on the human X chromosome inheritance line at a given ancestral generation also follows the Fibonacci sequence. A male individual has an X chromosome, which he received from his mother, and a Y chromosome, which he received from his father. The male counts as the "origin" of his own X chromosome (formula_12), and at his parents' generation, his X chromosome came from a single parent (formula_13). The male's mother received one X chromosome from her mother (the son's maternal grandmother), and one from her father (the son's maternal grandfather), so two grandparents contributed to the male descendant's X chromosome (formula_14). The maternal grandfather received his X chromosome from his mother, and the maternal grandmother received X chromosomes from both of her parents, so three great-grandparents contributed to the male descendant's X chromosome (formula_15). Five great-great-grandparents contributed to the male descendant's X chromosome (formula_16), etc. (This assumes that all ancestors of a given descendant are independent, but if any genealogy is traced far enough back in time, ancestors begin to appear on multiple lines of the genealogy, until eventually a population founder appears on all lines of the genealogy.)
The pathways of tubulins on intracellular microtubules arrange in patterns of 3, 5, 8 and 13.
The Fibonacci numbers occur in the sums of "shallow" diagonals in Pascal's triangle (see binomial coefficient):
These numbers also give the solution to certain enumerative problems, the most common of which is that of counting the number of ways of writing a given number as an ordered sum of 1s and 2s (called compositions); there are ways to do this. For example, if , then counts the eight compositions summing to 5:
The Fibonacci numbers can be found in different ways among the set of binary strings, or equivalently, among the subsets of a given set.
The first 21 Fibonacci numbers are:
The sequence can also be extended to negative index using the re-arranged recurrence relation
Thus the bidirectional sequence is
Like every sequence defined by a linear recurrence with constant coefficients, the Fibonacci numbers have a closed form expression. It has become known as Binet's formula, named after French mathematician Jacques Philippe Marie Binet, though it was already known by Abraham de Moivre and Daniel Bernoulli:
where
is the golden ratio (), and
Since formula_23, this formula can also be written as
formula_24
To see this, note that and are both solutions of the equations
so the powers of and satisfy the Fibonacci recursion. In other words,
and
It follows that for any values and , the sequence defined by
satisfies the same recurrence
If and are chosen so that and then the resulting sequence must be the Fibonacci sequence. This is the same as requiring and satisfy the system of equations:
which has solution
producing the required formula.
Taking the starting values and to be arbitrary constants, a more general solution is:
where
Since
for all , the number is the closest integer to formula_36. Therefore, it can be found by rounding, using the nearest integer function:
In fact, the rounding error is very small, being less than 0.1 for , and less than 0.01 for .
Fibonacci number can also be computed by truncation, in terms of the floor function:
As the floor function is monotonic, the latter formula can be inverted for finding the index of the largest Fibonacci number that is not greater than a real number :
where formula_40
Johannes Kepler observed that the ratio of consecutive Fibonacci numbers converges. He wrote that "as 5 is to 8 so is 8 to 13, practically, and as 8 is to 13, so is 13 to 21 almost", and concluded that these ratios approach the golden ratio formula_41
This convergence holds regardless of the starting values, excluding 0 and 0, or any pair in the conjugate golden ratio, formula_43 This can be verified using Binet's formula. For example, the initial values 3 and 2 generate the sequence 3, 2, 5, 7, 12, 19, 31, 50, 81, 131, 212, 343, 555, ... The ratio of consecutive terms in this sequence shows the same convergence towards the golden ratio.
Since the golden ratio satisfies the equation
this expression can be used to decompose higher powers formula_45 as a linear function of lower powers, which in turn can be decomposed all the way down to a linear combination of formula_46 and 1. The resulting recurrence relationships yield Fibonacci numbers as the linear coefficients:
This equation can be proved by induction on "n".
This expression is also true for "n" < 1 if the Fibonacci sequence "F" is extended to negative integers using the Fibonacci rule formula_48
A 2-dimensional system of linear difference equations that describes the Fibonacci sequence is
alternatively denoted
which yields formula_51. The eigenvalues of the matrix are formula_52 and formula_53 corresponding to the respective eigenvectors 
and
As the initial value is
it follows that the th term is
From this, the th element in the Fibonacci series
may be read off directly as a closed-form expression:
Equivalently, the same computation may performed by diagonalization of through use of its eigendecomposition:
where formula_60 and formula_61
The closed-form expression for the th element in the Fibonacci series is therefore given by
which again yields
The matrix has a determinant of −1, and thus it is a 2×2 unimodular matrix.
This property can be understood in terms of the continued fraction representation for the golden ratio:
The Fibonacci numbers occur as the ratio of successive convergents of the continued fraction for , and the matrix formed from successive convergents of any continued fraction has a determinant of +1 or −1. The matrix representation gives the following closed-form expression for the Fibonacci numbers:
Taking the determinant of both sides of this equation yields Cassini's identity,
Moreover, since for any square matrix , the following identities can be derived (they are obtained from two different coefficients of the matrix product, and one may easily deduce the second one from the first one by changing into ),
In particular, with ,
These last two identities provide a way to compute Fibonacci numbers recursively in arithmetic operations and in time , where is the time for the multiplication of two numbers of digits. This matches the time for computing the th Fibonacci number from the closed-form matrix formula, but with fewer redundant steps if one avoids recomputing an already computed Fibonacci number (recursion with memoization).
The question may arise whether a positive integer "x" is a Fibonacci number. This is true if and only if at least one of formula_69 or formula_70 is a perfect square. This is because Binet's formula above can be rearranged to give
which allows one to find the position in the sequence of a given Fibonacci number.
This formula must return an integer for all "n", so the radical expression must be an integer (otherwise the logarithm does not even return a rational number).
Most identities involving Fibonacci numbers can be proved using combinatorial arguments using the fact that "F" can be interpreted as the number of sequences of 1s and 2s that sum to "n" − 1. This can be taken as the definition of "F", with the convention that "F" = 0, meaning no sum adds up to −1, and that "F" = 1, meaning the empty sum "adds up" to 0. Here, the order of the summand matters. For example, 1 + 2 and 2 + 1 are considered two different sums.
For example, the recurrence relation
or in words, the "n"th Fibonacci number is the sum of the previous two Fibonacci numbers, may be shown by dividing the "F" sums of 1s and 2s that add to "n" − 1 into two non-overlapping groups. One group contains those sums whose first term is 1 and the other those sums whose first term is 2. In the first group the remaining terms add to "n" − 2, so it has "F" sums, and in the second group the remaining terms add to "n" − 3, so there are "F" sums. So there are a total of "F" + "F" sums altogether, showing this is equal to "F".
Similarly, it may be shown that the sum of the first Fibonacci numbers up to the "n"th is equal to the ("n" + 2)-nd Fibonacci number minus 1. In symbols:
This is done by dividing the sums adding to "n" + 1 in a different way, this time by the location of the first 2. Specifically, the first group consists of those sums that start with 2, the second group those that start 1 + 2, the third 1 + 1 + 2, and so on, until the last group, which consists of the single sum where only 1's are used. The number of sums in the first group is "F"("n"), "F"("n" − 1) in the second group, and so on, with 1 sum in the last group. So the total number of sums is "F"("n") + "F"("n" − 1) + ... + "F"(1) + 1 and therefore this quantity is equal to "F"("n" + 2).
A similar argument, grouping the sums by the position of the first 1 rather than the first 2, gives two more identities:
and
In words, the sum of the first Fibonacci numbers with odd index up to "F" is the (2"n")th Fibonacci number, and the sum of the first Fibonacci numbers with even index up to "F" is the (2"n" + 1)th Fibonacci number minus 1.
A different trick may be used to prove
or in words, the sum of the squares of the first Fibonacci numbers up to "F" is the product of the "n"th and ("n" + 1)th Fibonacci numbers. In this case Fibonacci rectangle of size "F" by "F"("n" + 1) can be decomposed into squares of size "F", "F", and so on to "F" = 1, from which the identity follows by comparing areas.
The sequence formula_77 is also considered using the symbolic method. More precisely, this sequence corresponds to a specifiable combinatorial class. The specification of this sequence is formula_78. Indeed, as stated above, the formula_79-th Fibonacci number equals the number of combinatorial compositions (ordered partitions) of formula_80 using terms 1 and 2.
It follows that the ordinary generating function of the Fibonacci sequence, i.e. formula_81, is the complex function formula_82.
Numerous other identities can be derived using various methods. Some of the most noteworthy are:
Cassini's identity states that
Catalan's identity is a generalization:
where "L" is the "n"'th Lucas number. The last is an identity for doubling "n"; other identities of this type are
by Cassini's identity.
These can be found experimentally using lattice reduction, and are useful in setting up the special number field sieve to factorize a Fibonacci number.
More generally,
or alternatively
Putting in this formula, one gets again the formulas of the end of above section Matrix form.
The generating function of the Fibonacci sequence is the power series
This series is convergent for formula_94 and its sum has a simple closed-form:
This can be proved by using the Fibonacci recurrence to expand each coefficient in the infinite sum:
Solving the equation
for "s"("x") results in the above closed form.
Setting , the closed form of the series becomes
In particular, if is an integer greater than 1, then this series converges. Further setting yields
for all positive integers .
Some math puzzle-books present as curious the particular value that comes from , which is formula_100 Similarly, gives
Infinite sums over reciprocal Fibonacci numbers can sometimes be evaluated in terms of theta functions. For example, we can write the sum of every odd-indexed reciprocal Fibonacci number as
and the sum of squared reciprocal Fibonacci numbers as
If we add 1 to each Fibonacci number in the first sum, there is also the closed form
and there is a "nested" sum of squared Fibonacci numbers giving the reciprocal of the golden ratio,
No closed formula for the reciprocal Fibonacci constant
is known, but the number has been proved irrational by Richard André-Jeannin.
The Millin series gives the identity
which follows from the closed form for its partial sums as "N" tends to infinity:
Every third number of the sequence is even and more generally, every "k"th number of the sequence is a multiple of "F". Thus the Fibonacci sequence is an example of a divisibility sequence. In fact, the Fibonacci sequence satisfies the stronger divisibility property
Any three consecutive Fibonacci numbers are pairwise coprime, which means that, for every "n",
Every prime number "p" divides a Fibonacci number that can be determined by the value of "p" modulo 5. If "p" is congruent to 1 or 4 (mod 5), then "p" divides "F", and if "p" is congruent to 2 or 3 (mod 5), then, "p" divides "F". The remaining case is that "p" = 5, and in this case "p" divides "F".
These cases can be combined into a single, non-piecewise formula, using the Legendre symbol:
The above formula can be used as a primality test in the sense that if
where the Legendre symbol has been replaced by the Jacobi symbol, then this is evidence that "n" is a prime, and if it fails to hold, then "n" is definitely not a prime. If "n" is composite and satisfies the formula, then "n" is a "Fibonacci pseudoprime". When "m" is largesay a 500-bit numberthen we can calculate "F" (mod "n") efficiently using the matrix form. Thus
Here the matrix power "A" is calculated using modular exponentiation, which can be adapted to matrices.
A "Fibonacci prime" is a Fibonacci number that is prime. The first few are:
Fibonacci primes with thousands of digits have been found, but it is not known whether there are infinitely many.
"F" is divisible by "F", so, apart from "F" = 3, any Fibonacci prime must have a prime index. As there are arbitrarily long runs of composite numbers, there are therefore also arbitrarily long runs of composite Fibonacci numbers.
No Fibonacci number greater than "F" = 8 is one greater or one less than a prime number.
The only nontrivial square Fibonacci number is 144. Attila Pethő proved in 2001 that there is only a finite number of perfect power Fibonacci numbers. In 2006, Y. Bugeaud, M. Mignotte, and S. Siksek proved that 8 and 144 are the only such non-trivial perfect powers.
1, 3, 21, 55 are the only triangular Fibonacci numbers, which was conjectured by Vern Hoggatt and proved by Luo Ming.
No Fibonacci number can be a perfect number. More generally, no Fibonaci number other than 1 can be multiply perfect, and no ratio of two Fibonacci numbers can be perfect.
With the exceptions of 1, 8 and 144 ("F" = "F", "F" and "F") every Fibonacci number has a prime factor that is not a factor of any smaller Fibonacci number (Carmichael's theorem). As a result, 8 and 144 ("F" and "F") are the only Fibonacci numbers that are the product of other Fibonacci numbers .
The divisibility of Fibonacci numbers by a prime "p" is related to the Legendre symbol formula_114 which is evaluated as follows:
If "p" is a prime number then
For example,
It is not known whether there exists a prime "p" such that
Such primes (if there are any) would be called Wall–Sun–Sun primes.
Also, if "p" ≠ 5 is an odd prime number then:
Example 1. "p" = 7, in this case "p" ≡ 3 (mod 4) and we have:
Example 2. "p" = 11, in this case "p" ≡ 3 (mod 4) and we have:
Example 3. "p" = 13, in this case "p" ≡ 1 (mod 4) and we have:
Example 4. "p" = 29, in this case "p" ≡ 1 (mod 4) and we have:
For odd "n", all odd prime divisors of "F" are congruent to 1 modulo 4, implying that all odd divisors of "F" (as the products of odd prime divisors) are congruent to 1 modulo 4.
For example,
All known factors of Fibonacci numbers "F"("i") for all "i" < 50000 are collected at the relevant repositories.
If the members of the Fibonacci sequence are taken mod "n", the resulting sequence is periodic with period at most "6n". The lengths of the periods for various "n" form the so-called Pisano periods . Determining a general formula for the Pisano periods is an open problem, which includes as a subproblem a special instance of the problem of finding the multiplicative order of a modular integer or of an element in a finite field. However, for any particular "n", the Pisano period may be found as an instance of cycle detection.
Starting with 5, every second Fibonacci number is the length of the hypotenuse of a right triangle with integer sides, or in other words, the largest number in a Pythagorean triple. The length of the longer leg of this triangle is equal to the sum of the three sides of the preceding triangle in this series of triangles, and the shorter leg is equal to the difference between the preceding bypassed Fibonacci number and the shorter leg of the preceding triangle.
The first triangle in this series has sides of length 5, 4, and 3. Skipping 8, the next triangle has sides of length 13, 12 (5 + 4 + 3), and 5 (8 − 3). Skipping 21, the next triangle has sides of length 34, 30 (13 + 12 + 5), and 16 (21 − 5). This series continues indefinitely. The triangle sides "a", "b", "c" can be calculated directly:
These formulas satisfy formula_134 for all "n", but they only represent triangle sides when "n" > 2.
Any four consecutive Fibonacci numbers "F", "F", "F" and "F" can also be used to generate a Pythagorean triple in a different way:
These formulas satisfy formula_134 for all "n", but they only represent triangle sides when "n" > 0.
Since "F" is asymptotic to formula_137, the number of digits in "F" is asymptotic to formula_138. As a consequence, for every integer "d" > 1 there are either 4 or 5 Fibonacci numbers with "d" decimal digits.
More generally, in the base "b" representation, the number of digits in "F" is asymptotic to formula_139
The Fibonacci sequence is one of the simplest and earliest known sequences defined by a recurrence relation, and specifically by a linear difference equation. All these sequences may be viewed as generalizations of the Fibonacci sequence. In particular, Binet's formula may be generalized to any sequence that is a solution of a homogeneous linear difference equation with constant coefficients.
Some specific examples that are close, in some sense, from Fibonacci sequence include:
Footnotes
Citations

</doc>
<doc id="10923" url="https://en.wikipedia.org/wiki?curid=10923" title="Fontainebleau">
Fontainebleau

Fontainebleau (; ) is a commune in the metropolitan area of Paris, France. It is located south-southeast of the centre of Paris. Fontainebleau is a sub-prefecture of the Seine-et-Marne department, and it is the seat of the "arrondissement" of Fontainebleau. The commune has the largest land area in the Île-de-France region; it is the only one to cover a larger area than Paris itself.
Fontainebleau, together with the neighbouring commune of Avon and three other smaller communes, form an urban area of 39,713 inhabitants (according to the 2001 census). This urban area is a satellite of Paris.
Fontainebleau is renowned for the large and scenic forest of Fontainebleau, a favourite weekend getaway for Parisians, as well as for the historic Château de Fontainebleau, which once belonged to the kings of France. It is also the home of INSEAD, one of the world's most elite business schools.
Inhabitants of Fontainebleau are sometimes called "Bellifontains".
Fontainebleau was recorded in the Latinised forms "Fons Bleaudi", "Fons Bliaudi", and "Fons Blaadi" in the 12th and 13th centuries, as "Fontem blahaud" in 1137, as "Fontaine belle eau" (folk etymology "fountain of beautiful water") in the 16th century, as "Fontainebleau" and "Fontaine belle eau" in 1630, and as the invented, fanciful Latin "Fons Bellaqueus" in the 17th century, which is the origin of the fanciful name "Bellifontains" of the inhabitants. Contrary to the folk etymology, the name comes from the medieval compound noun of "fontaine", meaning spring (fountainhead) and fountain, and "blitwald", consisting of the Germanic personal name Blit and the Germanic word for forest.
This hamlet was endowed with a royal hunting lodge and a chapel by Louis VII in the middle of the twelfth century. A century later, Louis IX, also called Saint Louis, who held Fontainebleau in high esteem and referred to it as "his wilderness", had a country house and a hospital constructed there.
Philip the Fair was born there in 1268 and died there in 1314. In all, thirty-four sovereigns, from Louis VI, the Fat, (1081–1137) to Napoleon III (1808–1873), spent time at Fontainebleau.
The connection between the town of Fontainebleau and the French monarchy was reinforced with the transformation of the royal country house into a true royal palace, the Palace of Fontainebleau. This was accomplished by the great builder-king, Francis I (1494–1547), who, in the largest of his many construction projects, reconstructed, expanded, and transformed the royal château at Fontainebleau into a residence that became his favourite, as well as the residence of his mistress, Anne, duchess of Étampes.
From the sixteenth to the eighteenth century, every monarch, from Francis I to Louis XV, made important renovations at the Palace of Fontainebleau, including demolitions, reconstructions, additions, and embellishments of various descriptions, all of which endowed it with a character that is a bit heterogeneous, but harmonious nonetheless.
On 18 October 1685, Louis XIV signed the "Edict of Fontainebleau" there. Also known as the "Revocation of the Edict of Nantes", this royal fiat reversed the permission granted to the Huguenots in 1598 to worship publicly in specified locations and hold certain other privileges. The result was that a large number of Protestants were forced to convert to the Catholic faith, killed, or forced into exile, mainly in the Low Countries, Prussia and in England.
The 1762 Treaty of Fontainebleau, a secret agreement between France and Spain concerning the Louisiana territory in North America, was concluded here. Also, preliminary negotiations, held before the 1763 Treaty of Paris was signed, ending the Seven Years' War, were at Fontainebleau.
During the French Revolution, Fontainebleau was temporarily renamed Fontaine-la-Montagne, meaning "Fountain by the Mountain". (The mountain referred to is the series of rocky formations located in the forest of Fontainebleau.)
On 29 October 1807, Manuel Godoy, chancellor to the Spanish king, Charles IV and Napoleon signed the Treaty of Fontainebleau, which authorized the passage of French troops through Spanish territories so that they might invade Portugal.
On 20 June 1812, Pope Pius VII arrived at the château of Fontainebleau, after a secret transfer from Savona, accompanied by his personal physician, Balthazard Claraz. In poor health, the Pope was the prisoner of Napoleon, and he remained in his genteel prison at Fontainebleau for nineteen months. From June 1812 until 23 January 1814, the Pope never left his apartments.
On 20 April 1814, Napoleon Bonaparte, shortly before his first abdication, bid farewell to the Old Guard, the renowned "grognards" (gripers) who had served with him since his very first campaigns, in the "White Horse Courtyard" (la cour du Cheval Blanc) at the Palace of Fontainebleau. (The courtyard has since been renamed the "Courtyard of Goodbyes".) According to contemporary sources, the occasion was very moving. The 1814 Treaty of Fontainebleau stripped Napoleon of his powers (but not his title as Emperor of the French) and sent him into exile on Elba.
Until the 19th century, Fontainebleau was a village and a suburb of Avon. Later, it developed as an independent residential city.
For the 1924 Summer Olympics, the town played host to the riding portion of the modern pentathlon event. This event took place near a golf course.
In July and August 1946, the town hosted the Franco-Vietnamese Conference, intended to find a solution to the long-contested struggle for Vietnam's independence from France, but the conference ended in failure.
Fontainebleau also hosted the general staff of the Allied Forces in Central Europe (Allied Forces Center or AFCENT) and the land forces command (LANDCENT); the air forces command (AIRCENT) was located nearby at Camp Guynemer. These facilities were in place from the inception of NATO until France's partial withdrawal from NATO in 1967 when the United States returned those bases to French control. NATO moved AFCENT to Brunssum in the Netherlands and AIRCENT to Ramstein in West Germany. (Note that the Supreme Headquarters Allied Powers Europe, also known as SHAPE, was located at Rocquencourt, west of Paris, quite a distance from Fontainebleau).
In 2008, The men's World Championship of Real Tennis (Jeu de Paume) was held in the tennis court of the Chateau. The real tennis World Championship is the oldest in sport and Fontainebleau has one of only two active courts in France.
Fontainebleau is a popular tourist destination; each year, 300,000 people visit the palace and more than 13 million people visit the forest.
The forest of Fontainebleau surrounds the town and dozens of nearby villages. It is protected by France's "Office National des Forêts", and it is recognised as a French national park. It is managed in order that its wild plants and trees, such as the rare service tree of Fontainebleau, and its populations of birds, mammals, and butterflies, can be conserved. It is a former royal hunting park often visited by hikers and horse riders. The forest is also well regarded for bouldering and is particularly popular among climbers, as it is the biggest developed area of that kind in the world.
The Royal Château de Fontainebleau is a large palace where the kings of France took their ease. It is also the site where the French royal court, from 1528 onwards, entertained the body of new ideas that became known as the Renaissance.
The European (and historic) campus of the INSEAD business school is located at the edge of Fontainebleau, by the Lycee Francois Couperin. INSEAD students live in local accommodations around the Fontainebleau area, and especially in the surrounding towns.
The graves of G. I. Gurdjieff and Katherine Mansfield can be found in the cemetery at Avon.
Fontainebleau is served by two stations on the Transilien Paris–Lyon rail line: Fontainebleau–Avon and Thomery. Fontainebleau–Avon station, the station closest to the centre of Fontainebleau, is located near the dividing-line between the commune of Fontainebleau and the commune of Avon, on the Avon side of the border.
Fontainebleau has a campus of the Centre hospitalier Sud Seine et Marne.
Fontainebleau is twinned with the following cities:

</doc>
<doc id="10929" url="https://en.wikipedia.org/wiki?curid=10929" title="Fighter aircraft">
Fighter aircraft

A fighter aircraft, often referred to simply as a fighter, is a military fixed-wing aircraft designed primarily for air-to-air combat against other aircraft. The key performance features of a fighter include not only its firepower but also its high speed and maneuverability relative to the target aircraft.
The fighter's main tactical purpose is to establish air superiority over the battlefield. The success or failure of a combatant's efforts to gain air superiority hinges on several factors including the skill of its pilots, the tactical soundness of its doctrine for deploying its fighters, and the numbers and performance of those fighters.
Many fighters have secondary capabilities such as ground attack and some types, such as fighter-bombers, are designed from the outset for dual roles. Other fighter designs are highly specialized while still filling the main air superiority role, these include the interceptor, heavy fighter, and night fighter.
A fighter aircraft is primarily designed for air-to-air combat. A given type may be designed for specific combat conditions, and in some cases for additional roles such as air-to-ground fighting. Historically the British Royal Flying Corps and Royal Air Force referred to them as "scouts" until the early 1920s, while the U.S. Army called them "pursuit" aircraft until the late 1940s. The UK changed to calling them fighters in the 1920s, while the US Army did so in the 1940s. A short-range fighter designed to defend against incoming enemy aircraft is known as an interceptor.
Recognised classes of fighter include:
Of these, the Fighter-bomber, reconnaissance fighter and strike fighter classes are dual-role, possessing qualities of the fighter alongside some other battlefield role. Some fighter designs may be developed in variants performing other roles entirely, such as ground attack or unarmed reconnaissance. This may be for political or national security reasons, for advertising purposes, or other reasons.
The Sopwith Camel and other "fighting scouts" of World War I performed a great deal of ground-attack work. In World War II, the USAAF and RAF often favored fighters over dedicated light bombers or dive bombers, and types such as the Republic P-47 Thunderbolt and Hawker Hurricane that were no longer competitive as aerial combat fighters were relegated to ground attack. Several aircraft, such as the F-111 and F-117, have received fighter designations though they had no fighter capability due to political or other reasons. The F-111B variant was originally intended for a fighter role with the U.S. Navy, but it was canceled. This blurring follows the use of fighters from their earliest days for "attack" or "strike" operations against ground targets by means of strafing or dropping small bombs and incendiaries. Versatile multirole fighter-bombers such as the McDonnell Douglas F/A-18 Hornet are a less expensive option than having a range of specialized aircraft types.
Some of the most expensive fighters such as the US Grumman F-14 Tomcat, McDonnell Douglas F-15 Eagle, Lockheed Martin F-22 Raptor and Russian Sukhoi Su-27 were employed as all-weather interceptors as well as air superiority fighter aircraft, while commonly developing air-to-ground roles late in their careers. An interceptor is generally an aircraft intended to target (or intercept) bombers and so often trades maneuverability for climb rate.
As a part of military nomenclature, a letter is often assigned to various types of aircraft to indicate their use, along with a number to indicate the specific aircraft. The letters used to designate a fighter differ in various countries – in the English-speaking world, "F" is now used to indicate a fighter (e.g. Lockheed Martin F-35 Lightning II or Supermarine Spitfire F.22), though when the pursuit designation was used in the US, they were "P" types (e.g. Curtiss P-40 Warhawk). In Russia "I" was used (Polikarpov I-16), while the French continue to use "C" (Nieuport 17 C.1).
As fighter types have proliferated, the air superiority fighter emerged as a specific role at the pinnacle of speed, maneuverability, and air-to-air weapon systems – able to hold its own against all other fighters and establish its dominance in the skies above the battlefield.
The interceptor is a fighter designed specifically to intercept and engage approaching enemy aircraft. There are two general classes of interceptor: relatively lightweight aircraft in the point-defence role, built for fast reaction, high performance and with a short range, and heavier aircraft with more comprehensive avionics and designed to fly at night or in all weathers and to operate over longer ranges. Originating during World War I, by 1929 this class of fighters had become known as the interceptor.
The equipment necessary for daytime flight is inadequate when flying at night or in poor visibility. The night fighter was developed during World War I with additional equipment to aid the pilot in flying straight, navigating and finding the target. From modified variants of the Royal Aircraft Factory B.E.2c in 1915, the night fighter has evolved into the highly capable all-weather fighter.
The strategic fighter is a fast, heavily armed and long-range type, able to act as an escort fighter protecting bombers, to carry out offensive sorties of its own as a penetration fighter and maintain standing patrols at significant distance from its home base.
Bombers are vulnerable due to their low speed and poor maneuvrability. The escort fighter was developed during World War II to come between the bombers and enemy attackers as a protective shield. The primary requirement was for long range, with several heavy fighters given the role. However they too proved unwieldy and vulnerable, so as the war progressed techniques such as drop tanks were developed to extend the range of more nimble conventional fighters.
The penetration fighter is typically also fitted for the ground-attack role, and so is able to defend itself while conducting attack sorties.
Since World War I, achieving and maintaining air superiority has been considered essential for victory in conventional warfare.
Fighters continued to be developed throughout World War I, to deny enemy aircraft and dirigibles the ability to gather information by reconnaissance over the battlefield. Early fighters were very small and lightly armed by later standards, and most were biplanes built with a wooden frame covered with fabric, and a maximum airspeed of about . As control of the airspace over armies became increasingly important, all of the major powers developed fighters to support their military operations. Between the wars, wood was largely replaced in part or whole by metal tubing, and finally aluminum stressed skin structures (monocoque) began to predominate.
By World War II, most fighters were all-metal monoplanes armed with batteries of machine guns or cannons and some were capable of speeds approaching . Most fighters up to this point had one engine, but a number of twin-engine fighters were built; however they were found to be outmatched against single-engine fighters and were relegated to other tasks, such as night fighters equipped with primitive radar sets.
By the end of the war, turbojet engines were replacing piston engines as the means of propulsion, further increasing aircraft speed. Since the weight of the turbojet engine was far less than a piston engine, having two engines was no longer a handicap and one or two were used, depending on requirements. This in turn required the development of ejection seats so the pilot could escape, and G-suits to counter the much greater forces being applied to the pilot during maneuvers.
In the 1950s, radar was fitted to day fighters, since due to ever increasing air-to-air weapon ranges, pilots could no longer see far enough ahead to prepare for the opposition. Subsequently, radar capabilities grew enormously and are now the primary method of target acquisition. Wings were made thinner and swept back to reduce transonic drag, which required new manufacturing methods to obtain sufficient strength. Skins were no longer sheet metal riveted to a structure, but milled from large slabs of alloy. The sound barrier was broken, and after a few false starts due to required changes in controls, speeds quickly reached Mach 2, past which aircraft cannot maneuver sufficiently to avoid attack.
Air-to-air missiles largely replaced guns and rockets in the early 1960s since both were believed unusable at the speeds being attained, however the Vietnam War showed that guns still had a role to play, and most fighters built since then are fitted with cannon (typically between 20 and 30 mm in caliber) in addition to missiles. Most modern combat aircraft can carry at least a pair of air-to-air missiles.
In the 1970s, turbofans replaced turbojets, improving fuel economy enough that the last piston engined support aircraft could be replaced with jets, making multi-role combat aircraft possible. Honeycomb structures began to replace milled structures, and the first composite components began to appear on components subjected to little stress.
With the steady improvements in computers, defensive systems have become increasingly efficient. To counter this, stealth technologies have been pursued by the United States, Russia, India and China. The first step was to find ways to reduce the aircraft's reflectivity to radar waves by burying the engines, eliminating sharp corners and diverting any reflections away from the radar sets of opposing forces. Various materials were found to absorb the energy from radar waves, and were incorporated into special finishes that have since found widespread application. Composite structures have become widespread, including major structural components, and have helped to counterbalance the steady increases in aircraft weight—most modern fighters are larger and heavier than World War II medium bombers.
Because of the importance of air superiority, since the early days of aerial combat armed forces have constantly competed to develop technologically superior fighters and to deploy these fighters in greater numbers, and fielding a viable fighter fleet consumes a substantial proportion of the defense budgets of modern armed forces.
The global combat aircraft market was worth $45.75 billion in 2017 and is projected by Frost & Sullivan at $47.2 billion in 2026: 35% modernization programs and 65% aircraft purchases, dominated by the Lockheed Martin F-35 with 3,000 deliveries over 20 years.
The word "fighter" was first used to describe a two-seater aircraft with sufficient lift to carry a machine gun and its operator as well as the pilot. Some of the first such "fighters" belonged to the "gunbus" series of experimental gun carriers of the British Vickers company that culminated in the Vickers F.B.5 Gunbus of 1914. The main drawback of this type of aircraft was its lack of speed. Planners quickly realized that an aircraft intended to destroy its kind in the air had to be fast enough to catch its quarry.
One of the first companies to develop an armed aircraft was Vickers. Their Type 18 Destroyer of 1913 was a two-seat pusher type, with the pilot behind and an observer/gunner in front and a machine gun fitted in the nose on a pivoting mount. It would be developed as the F.B.5 "Gunbus" and introduced into service in 1915.
However at the outbreak of World War I, front-line aircraft were unarmed and used almost entirely for reconnaissance. On 15 August 1914, Miodrag Tomić encountered an enemy plane while conducting a reconnaissance flight over Austria-Hungary. The enemy pilot shot at Tomić's plane with a revolver. Tomić produced a pistol of his own and fired back. It was considered the first exchange of fire between aircraft in history. Within weeks, all Serbian and Austro-Hungarian aircraft were armed. Machine guns were soon fitted to existing reconnaissance types for use by the observer, but none of these were true fighter planes.
Another type of military aircraft was to form the basis for an effective "fighter" in the modern sense of the word. It was based on the small fast aircraft developed before the war for such air races as the Gordon Bennett Cup and Schneider Trophy. The military scout airplane was not expected to carry serious armament, but rather to rely on its speed to reach the scout or reconnoiter location and return quickly to report, making it essentially an aerial horse. British scout aircraft, in this sense, included the Sopwith Tabloid and Bristol Scout. French equivalents included the Morane-Saulnier N.
The next advance came with the fixed forward-firing machine gun, so that the pilot pointed the whole plane at the target and fired the gun, instead of relying on a second gunner. Roland Garros (aviator) bolted metal deflector plates to the propeller so that it would not shoot itself out of the sky and a number of Morane-Saulnier Ns were modified. The technique proved effective, however the deflected bullets were still highly dangerous.
The next fighter manufactured in any quantity was the Fokker E.I "Eindecker" and its derivatives, whose introduction in 1915, only a few months after the appearance of the slower Gunbus, ushered in what the Allies came to call the "Fokker scourge" and a period of air superiority for the German forces. Although it still had mediocre flying qualities, the Fokker's unique innovation was an interrupter gear which allowed the gun to fire through the propeller arc without hitting the blades.
Soon after the commencement of the war, pilots armed themselves with pistols, carbines, grenades, and an assortment of improvised weapons. Many of these proved ineffective as the pilot had to fly his airplane while attempting to aim a handheld weapon and make a difficult deflection shot. The first step in finding a real solution was to mount the weapon on the aircraft, but the propeller remained a problem since the best direction to shoot is straight ahead. Numerous solutions were tried. A second crew member behind the pilot could aim and fire a swivel-mounted machine gun at enemy airplanes; however, this limited the area of coverage chiefly to the rear hemisphere, and effective coordination of the pilot's maneuvering with the gunner's aiming was difficult. This option was chiefly employed as a defensive measure on two-seater reconnaissance aircraft from 1915 on. Both the SPAD S.A and the Royal Aircraft Factory B.E.9 added a second crewman ahead of the engine in a pod but this was both hazardous to the second crewman and limited performance. The Sopwith L.R.T.Tr. similarly added a pod on the top wing with no better luck.
An alternative was to build a "pusher" scout such as the Airco DH.2, with the propeller mounted behind the pilot. The main drawback was that the high drag of a pusher type's tail structure made it slower than a similar "tractor" aircraft.
A better solution for a single seat scout was to mount the machine gun (rifles and pistols having been dispensed with) to fire forwards but outside the propeller arc. Wing guns were tried but the unreliable weapons available required frequent clearing of jammed rounds and misfires and remained impractical until after the war. Mounting the machine gun over the top wing worked well and was used long after the ideal solution was found. The Nieuport 11 of 1916 and Royal Aircraft Factory S.E.5 of 1918 both used this system with considerable success; however, this placement made aiming difficult and the location made it difficult for a pilot to both maneuver and have access to the gun's breech. The British Foster mounting was specifically designed for this kind of application, fitted with the Lewis Machine gun, which due to its design was unsuitable for synchronizing.
The need to arm a tractor scout with a forward-firing gun whose bullets passed through the propeller arc was evident even before the outbreak of war and inventors in both France and Germany devised mechanisms that could time the firing of the individual rounds to avoid hitting the propeller blades. Franz Schneider, a Swiss engineer, had patented such a device in Germany in 1913, but his original work was not followed up. French aircraft designer Raymond Saulnier patented a practical device in April 1914, but trials were unsuccessful because of the propensity of the machine gun employed to hang fire due to unreliable ammunition.
In December 1914, French aviator Roland Garros asked Saulnier to install his synchronization gear on Garros' Morane-Saulnier Type L. Unfortunately the gas-operated Hotchkiss machine gun he was provided had an erratic rate of fire and it was impossible to synchronize it with a spinning propeller. As an interim measure, the propeller blades were armored and fitted with metal wedges to protect the pilot from ricochets. Garros' modified monoplane was first flown in March 1915 and he began combat operations soon thereafter. Garros scored three victories in three weeks before he himself was downed on 18 April and his airplane, along with its synchronization gear and propeller was captured by the Germans.
Meanwhile, the synchronization gear (called the "Stangensteuerung" in German, for "pushrod control system") devised by the engineers of Anthony Fokker's firm was the first system to see production contracts, and would make the Fokker "Eindecker" monoplane a feared name over the Western Front, despite its being an adaptation of an obsolete pre-war French Morane-Saulnier racing airplane, with a mediocre performance and poor flight characteristics. The first victory for the "Eindecker" came on 1 July 1915, when "Leutnant" Kurt Wintgens, flying with the "Feldflieger Abteilung 6" unit on the Western Front, forced down a Morane-Saulnier Type L two-seat "parasol" monoplane just east of Luneville. Wintgens' aircraft, one of the five Fokker M.5K/MG production prototype examples of the "Eindecker", was armed with a synchronized, air-cooled aviation version of the Parabellum MG14 machine gun.
The success of the "Eindecker" kicked off a competitive cycle of improvement among the combatants, both sides striving to build ever more capable single-seat fighters. The Albatros D.I and Sopwith Pup of 1916 set the classic pattern followed by fighters for about twenty years. Most were biplanes and only rarely monoplanes or triplanes. The strong box structure of the biplane provided a rigid wing that allowed the accurate lateral control essential for dogfighting. They had a single operator, who flew the aircraft and also controlled its armament. They were armed with one or two Maxim or Vickers machine guns, which were easier to synchronize than other types, firing through the propeller arc. Gun breeches were directly in front of the pilot, with obvious implications in case of accidents, but jams could be cleared in flight, while aiming was simplified.
The use of metal aircraft structures was pioneered before World War I by Breguet but would find its biggest proponent in Anthony Fokker, who used chrome-molybdenum steel tubing for the fuselage structure of all his fighter designs, while the innovative German engineer Hugo Junkers developed two all-metal, single-seat fighter monoplane designs with cantilever wings: the strictly experimental Junkers J 2 private-venture aircraft, made with steel, and some forty examples of the Junkers D.I, made with corrugated duralumin, all based on his experience in creating the pioneering Junkers J 1 all-metal airframe technology demonstration aircraft of late 1915. While Fokker would pursue steel tube fuselages with wooden wings until the late 1930s, and Junkers would focus on corrugated sheet metal, Dornier was the first to build a fighter (The Dornier-Zeppelin D.I) made with pre-stressed sheet aluminum and having cantilevered wings, a form that would replace all others in the 1930s.
As collective combat experience grew, the more successful pilots such as Oswald Boelcke, Max Immelmann, and Edward Mannock developed innovative tactical formations and maneuvers to enhance their air units' combat effectiveness.
Allied and – before 1918 – German pilots of World War I were not equipped with parachutes, so in-flight fires or structural failure were often fatal. Parachutes were well-developed by 1918 having previously been used by balloonists, and were adopted by the German flying services during the course of that year. The well known and feared Manfred von Richthofen, the "Red Baron", was wearing one when he was killed, but the allied command continued to oppose their use on various grounds.
In April 1917, during a brief period of German aerial supremacy a British pilot's average life expectancy was 93 flying hours, or about three weeks of active service. More than 50,000 airmen from both sides died during the war.
Fighter development stagnated between the wars, especially in the United States and the United Kingdom, where budgets were small. In France, Italy and Russia, where large budgets continued to allow major development, both monoplanes and all metal structures were common. By the end of the 1920s, however, those countries overspent themselves and were overtaken in the 1930s by those powers that hadn't been spending heavily, namely the British, the Americans and the Germans.
Given limited defense budgets, air forces tended to be conservative in their aircraft purchases, and biplanes remained popular with pilots because of their agility, and remained in service long after they had ceased to be competitive. Designs such as the Gloster Gladiator, Fiat CR.42, and Polikarpov I-15 were common even in the late 1930s, and many were still in service as late as 1942. Up until the mid-1930s, the majority of fighters in the US, the UK, Italy and Russia remained fabric-covered biplanes.
Fighter armament eventually began to be mounted inside the wings, outside the arc of the propeller, though most designs retained two synchronized machine guns directly ahead of the pilot, where they were more accurate (that being the strongest part of the structure, reducing the vibration to which the guns were subjected to). Shooting with this traditional arrangement was also easier for the further reason that the guns shot directly ahead in the direction of the aircraft's flight, up to the limit of the guns range; unlike wing-mounted guns which to be effective required to be harmonised, that is, preset to shoot at an angle by ground crews so that their bullets would converge on a target area a set distance ahead of the fighter. Rifle-caliber .30 and .303 in (7.62 mm) caliber guns remained the norm, with larger weapons either being too heavy and cumbersome or deemed unnecessary against such lightly built aircraft. It was not considered unreasonable to use World War I-style armament to counter enemy fighters as there was insufficient air-to-air combat during most of the period to disprove this notion.
The rotary engine, popular during World War I, quickly disappeared, its development having reached the point where rotational forces prevented more fuel and air from being delivered to the cylinders, which limited horsepower. They were replaced chiefly by the stationary radial engine though major advances led to inline engines, which gained ground with several exceptional engines—including the V-12 Curtiss D-12. Aircraft engines increased in power several-fold over the period, going from a typical in the 900-kg Fokker D.VII of 1918 to in the 2,500-kg Curtiss P-36 of 1936. The debate between the sleek in-line engines versus the more reliable radial models continued, with naval air forces preferring the radial engines, and land-based forces often choosing in-line units. Radial designs did not require a separate (and vulnerable) cooling system, but had increased drag. In-line engines often had a better power-to-weight ratio, but there were radial engines that kept working even after having suffered significant battle damage.
Some air forces experimented with "heavy fighters" (called "destroyers" by the Germans). These were larger, usually twin-engined aircraft, sometimes adaptations of light or medium bomber types. Such designs typically had greater internal fuel capacity (thus longer range) and heavier armament than their single-engine counterparts. In combat, they proved vulnerable to more agile single-engine fighters.
The primary driver of fighter innovation, right up to the period of rapid re-armament in the late 1930s, were not military budgets, but civilian aircraft racing. Aircraft designed for these races introduced innovations like streamlining and more powerful engines that would find their way into the fighters of World War II. The most significant of these was the Schneider Trophy races, where competition grew so fierce, only national governments could afford to enter.
At the very end of the inter-war period in Europe came the Spanish Civil War. This was just the opportunity the German "Luftwaffe", Italian "Regia Aeronautica", and the Soviet Union's Red Air Force needed to test their latest aircraft. Each party sent numerous aircraft types to support their sides in the conflict. In the dogfights over Spain, the latest Messerschmitt Bf 109 fighters did well, as did the Soviet Polikarpov I-16. The German design had considerably more room for development however and the lessons learned led to greatly improved models in World War II. The Russians, whose side lost, failed to keep up and despite newer models coming into service, I-16s were outfought by the improved Bf 109s in World War II, while remaining the most common Soviet front-line fighter into 1942. For their part, the Italians developed several monoplanes such as the Fiat G.50, but being short on funds, were forced to continue operating obsolete Fiat CR.42 biplanes.
From the early 1930s the Japanese had been at war against both the Chinese Nationalists and the Russians in China, and used the experience to improve both training and aircraft, replacing biplanes with modern cantilever monoplanes and creating a cadre of exceptional pilots for use in the Pacific War. In the United Kingdom, at the behest of Neville Chamberlain, (more famous for his 'peace in our time' speech) the entire British aviation industry was retooled, allowing it to change quickly from fabric covered metal framed biplanes to cantilever stressed skin monoplanes in time for the war with Germany.
The period of improving the same biplane design over and over was now coming to an end, and the Hawker Hurricane and Supermarine Spitfire finally started to supplant the Gloster Gladiator and Hawker Fury biplanes but many of the former remained in front-line service well past the start of World War II. While not a combatant themselves in Spain, they absorbed many of the lessons learned in time to use them.
The Spanish Civil War also provided an opportunity for updating fighter tactics. One of the innovations to result from the aerial warfare experience this conflict provided was the development of the "finger-four" formation by the German pilot Werner Mölders. Each fighter squadron (German: "Staffel") was divided into several flights ("Schwärme") of four aircraft. Each "Schwarm" was divided into two "Rotten", which was a pair of aircraft. Each "Rotte" was composed of a leader and a wingman. This flexible formation allowed the pilots to maintain greater situational awareness, and the two "Rotten" could split up at any time and attack on their own. The finger-four would become widely adopted as the fundamental tactical formation over the course of World War.
World War II featured fighter combat on a larger scale than any other conflict to date. German Field Marshal Erwin Rommel noted the effect of airpower: "Anyone who has to fight, even with the most modern weapons, against an enemy in complete command of the air, fights like a savage against modern European troops, under the same handicaps and with the same chances of success." Throughout the war, fighters performed their conventional role in establishing air superiority through combat with other fighters and through bomber interception, and also often performed roles such as tactical air support and reconnaissance.
Fighter design varied widely among combatants. The Japanese and Italians favored lightly armed and armored but highly maneuverable designs such as the Japanese Nakajima Ki-27, Nakajima Ki-43 and Mitsubishi A6M Zero and the Italian Fiat G.50 and Macchi MC.200. In contrast, designers in the United Kingdom, Germany, the Soviet Union, and the United States believed that the increased speed of fighter aircraft would create "g"-forces unbearable to pilots who attempted maneuvering dogfights typical of the First World War, and their fighters were instead optimized for speed and firepower. In practice, while light, highly maneuverable aircraft did possess some advantages in fighter-versus-fighter combat, those could usually be overcome by sound tactical doctrine, and the design approach of the Italians and Japanese made their fighters ill-suited as interceptors or attack aircraft.
During the invasion of Poland and the Battle of France, Luftwaffe fighters—primarily the Messerschmitt Bf 109—held air superiority, and the Luftwaffe played a major role in German victories in these campaigns. During the Battle of Britain, however, British Hurricanes and Spitfires proved roughly equal to Luftwaffe fighters. Additionally Britain's radar-based Dowding system directing fighters onto German attacks and the advantages of fighting above Britain's home territory allowed the RAF to deny Germany air superiority, saving the UK from possible German invasion and dealing the Axis a major defeat early in the Second World War.
On the Eastern Front, Soviet fighter forces were overwhelmed during the opening phases of Operation Barbarossa. This was a result of the tactical surprise at the outset of the campaign, the leadership vacuum within the Soviet military left by the Great Purge, and the general inferiority of Soviet designs at the time, such as the obsolescent I-15 biplane and the I-16. More modern Soviet designs, including the MiG-3, LaGG-3 and Yak-1, had not yet arrived in numbers and in any case were still inferior to the Messerschmitt Bf 109. As a result, during the early months of these campaigns, Axis air forces destroyed large numbers of Red Air Force aircraft on the ground and in one-sided dogfights.
In the later stages on the Eastern Front, Soviet training and leadership improved, as did their equipment. Since 1942 Soviet designs such as the Yakovlev Yak-9 and Lavochkin La-5 had performance comparable to the German Bf 109 and Focke-Wulf Fw 190. Also, significant numbers of British, and later U.S., fighter aircraft were supplied to aid the Soviet war effort as part of Lend-Lease, with the Bell P-39 Airacobra proving particularly effective in the lower-altitude combat typical of the Eastern Front. The Soviets were also helped indirectly by the American and British bombing campaigns, which forced the Luftwaffe to shift many of its fighters away from the Eastern Front in defense against these raids. The Soviets increasingly were able to challenge the Luftwaffe, and while the Luftwaffe maintained a qualitative edge over the Red Air Force for much of the war, the increasing numbers and efficacy of the Soviet Air Force were critical to the Red Army's efforts at turning back and eventually annihilating the Wehrmacht.
Meanwhile, air combat on the Western Front had a much different character. Much of this combat focused on the strategic bombing campaigns of the RAF and the USAAF against German industry intended to wear down the Luftwaffe. Axis fighter aircraft focused on defending against Allied bombers while Allied fighters' main role was as bomber escorts. The RAF raided German cities at night, and both sides developed radar-equipped night fighters for these battles. The Americans, in contrast, flew daylight bombing raids into Germany. Unescorted Consolidated B-24 Liberators and Boeing B-17 Flying Fortress bombers, however, proved unable to fend off German interceptors (primarily Bf 109s and Fw 190s). With the later arrival of long range fighters, particularly the North American P-51 Mustang, American fighters were able to escort far into Germany on daylight raids and established control of the skies over Western Europe.
By the time of Operation Overlord in June 1944, the Allies had gained near complete air superiority over the Western Front. This cleared the way both for intensified strategic bombing of German cities and industries, and for the tactical bombing of battlefield targets. With the Luftwaffe largely cleared from the skies, Allied fighters increasingly served as attack aircraft.
Allied fighters, by gaining air superiority over the European battlefield, played a crucial role in the eventual defeat of the Axis, which Reichmarshal Hermann Göring, commander of the German "Luftwaffe" summed up when he said: "When I saw Mustangs over Berlin, I knew the jig was up."
Major air combat during the war in the Pacific began with the entry of the Western Allies following Japan's attack against Pearl Harbor. The Imperial Japanese Navy Air Service primarily operated the Mitsubishi A6M Zero, and the Imperial Japanese Army Air Service flew the Nakajima Ki-27 and the Nakajima Ki-43, initially enjoying great success, as these fighters generally had better range, maneuverability, speed and climb rates than their Allied counterparts. Additionally, Japanese pilots had received excellent training and many were combat veterans from Japan's campaigns in China. They quickly gained air superiority over the Allies, who at this stage of the war were often disorganized, under-trained and poorly equipped, and Japanese air power contributed significantly to their successes in the Philippines, Malaysia and Singapore, the Dutch East Indies and Burma.
By mid-1942, the Allies began to regroup and while some Allied aircraft such as the Brewster Buffalo and the P-39 were hopelessly outclassed by fighters like Japan's Zero, others such as the Army's P-40 and the Navy's Wildcat possessed attributes such as superior firepower, ruggedness and dive speed, and the Allies soon developed tactics (such as the Thach Weave) to take advantage of these strengths. These changes soon paid dividends, as the Allied ability to deny Japan air superiority was critical to their victories at Coral Sea, Midway, Guadalcanal and New Guinea. In China, the Flying Tigers also used the same tactics with some success, although they were unable to stem the tide of Japanese advances there.
By 1943, the Allies began to gain the upper hand in the Pacific Campaign's air campaigns. Several factors contributed to this shift. First, the P-38 and second-generation Allied fighters such as the Hellcat and later the Corsair, the P-47 and the P-51, began arriving in numbers. These fighters outperformed Japanese fighters in all respects except maneuverability. Other problems with Japan's fighter aircraft also became apparent as the war progressed, such as their lack of armor and light armament, which made them inadequate as bomber interceptors or ground-attack planes – roles Allied fighters excelled at. Most importantly, Japan's training program failed to provide enough well-trained pilots to replace losses. In contrast, the Allies improved both the quantity and quality of pilots graduating from their training programs.
By mid-1944, Allied fighters had gained air superiority throughout the theater, which would not be contested again during the war. The extent of Allied quantitative and qualitative superiority by this point in the war was demonstrated during the Battle of the Philippine Sea, a lopsided Allied victory in which Japanese fliers were downed in such numbers and with such ease that American fighter pilots likened it to a great turkey shoot.
Late in the war, Japan did begin to produce new fighters such as the Nakajima Ki-84 and the Kawanishi N1K to replace the venerable Zero, but these were produced only in small numbers, and in any case by that time Japan lacked trained pilots or sufficient fuel to mount a sustained challenge to Allied fighters. During the closing stages of the war, Japan's fighter arm could not seriously challenge raids over Japan by American B-29s, and was largely relegated to Kamikaze tactics.
Fighter technology advanced rapidly during the Second World War. Piston-engines, which powered the vast majority of World War II fighters, grew more powerful: at the beginning of the war fighters typically had engines producing between and , while by the end of the war many could produce over . For example, the Spitfire, one of the few fighters in continuous production throughout the war, was in 1939 powered by a Merlin II, while variants produced in 1945 were equipped with the Griffon 61. Nevertheless, these fighters could only achieve modest increases in top speed due to problems of compressibility created as aircraft and their propellers approached the sound barrier, and it was apparent that propeller-driven aircraft were approaching the limits of their performance. German jet and rocket-powered fighters entered combat in 1944, too late to impact the war's outcome. The same year the Allies' only operational jet fighter, the Gloster Meteor, also entered service.
World War II fighters also increasingly featured monocoque construction, which improved their aerodynamic efficiency while adding structural strength. Laminar flow wings, which improved high speed performance, also came into use on fighters such as the P-51, while the Messerschmitt Me 262 and the Messerschmitt Me 163 featured swept wings that dramatically reduced drag at high subsonic speeds.
Armament also advanced during the war. The rifle-caliber machine guns that were common on prewar fighters could not easily down the more rugged warplanes of the era. Air forces began to replace or supplement them with cannons, which fired explosive shells that could blast a hole in an enemy aircraft – rather than relying on kinetic energy from a solid bullet striking a critical component of the aircraft, such as a fuel line or control cable, or the pilot. Cannons could bring down even heavy bombers with just a few hits, but their slower rate of fire made it difficult to hit fast-moving fighters in a dogfight. Eventually, most fighters mounted cannons, sometimes in combination with machine guns.
The British epitomized this shift. Their standard early war fighters mounted eight caliber machine guns, but by mid-war they often featured a combination of machine guns and 20 mm cannons, and late in the war often only cannons. The Americans, in contrast, had problems producing a native cannon design, so instead placed multiple .50 caliber (12.7 mm) heavy machine guns on their fighters. Fighters were also increasingly fitted with bomb racks and air-to-surface ordnance such as bombs or rockets beneath their wings, and pressed into close air support roles as fighter-bombers. Although they carried less ordnance than light and medium bombers, and generally had a shorter range, they were cheaper to produce and maintain and their maneuverability made it easier for them to hit moving targets such as motorized vehicles. Moreover, if they encountered enemy fighters, their ordnance (which reduced lift and increased drag and therefore decreased performance) could be jettisoned and they could engage the enemy fighters, which eliminated the need for the fighter escorts that bombers required. Heavily armed and sturdily constructed fighters such as Germany's Focke-Wulf Fw 190, Britain's Hawker Typhoon and Hawker Tempest, and America's P-40, Corsair, P-47 and P-38 all excelled as fighter-bombers, and since the Second World War ground attack has been an important secondary capability of many fighters.
World War II also saw the first use of airborne radar on fighters. The primary purpose of these radars was to help night fighters locate enemy bombers and fighters. Because of the bulkiness of these radar sets, they could not be carried on conventional single-engined fighters and instead were typically retrofitted to larger heavy fighters or light bombers such as Germany's Messerschmitt Bf 110 and Junkers Ju 88, Britain's Mosquito and Beaufighter, and America's A-20, which then served as night fighters. The Northrop P-61 Black Widow, a purpose-built night fighter, was the only fighter of the war that incorporated radar into its original design. Britain and America cooperated closely in the development of airborne radar, and Germany's radar technology generally lagged slightly behind Anglo-American efforts, while other combatants developed few radar-equipped fighters.
Several prototype fighter programs begun early in 1945 continued on after the war and led to advanced piston-engine fighters that entered production and operational service in 1946. A typical example is the Lavochkin La-9 'Fritz', which was an evolution of the successful wartime Lavochkin La-7 'Fin'. Working through a series of prototypes, the La-120, La-126 and La-130, the Lavochkin design bureau sought to replace the La-7's wooden airframe with a metal one, as well as fit a laminar-flow wing to improve maneuver performance, and increased armament. The La-9 entered service in August 1946 and was produced until 1948; it also served as the basis for the development of a long-range escort fighter, the La-11 'Fang', of which nearly 1200 were produced 1947–1951. Over the course of the Korean War, however, it became obvious that the day of the piston-engined fighter was coming to a close and that the future would lie with the jet fighter.
This period also witnessed experimentation with jet-assisted piston engine aircraft. La-9 derivatives included examples fitted with two underwing auxiliary pulsejet engines (the La-9RD) and a similarly mounted pair of auxiliary ramjet engines (the La-138); however, neither of these entered service. One that did enter service – with the U.S. Navy in March 1945 – was the Ryan FR-1 Fireball; production was halted with the war's end on VJ-Day, with only 66 having been delivered, and the type was withdrawn from service in 1947. The USAAF had ordered its first 13 mixed turboprop-turbojet-powered pre-production prototypes of the Consolidated Vultee XP-81 fighter, but this program was also canceled by VJ Day, with 80% of the engineering work completed.
The first rocket-powered aircraft was the Lippisch Ente, which made a successful maiden flight in March 1928. The only pure rocket aircraft ever mass-produced was the Messerschmitt Me 163B "Komet" in 1944, one of several German World War II projects aimed at developing high speed, point-defense aircraft. Later variants of the Me 262 (C-1a and C-2b) were also fitted with "mixed-power" jet/rocket powerplants, while earlier models were fitted with rocket boosters, but were not mass-produced with these modifications.
The USSR experimented with a rocket-powered interceptor in the years immediately following World War II, the Mikoyan-Gurevich I-270. Only two were built.
In the 1950s, the British developed mixed-power jet designs employing both rocket and jet engines to cover the performance gap that existed in turbojet designs. The rocket was the main engine for delivering the speed and height required for high-speed interception of high-level bombers and the turbojet gave increased fuel economy in other parts of flight, most notably to ensure the aircraft was able to make a powered landing rather than risking an unpredictable gliding return.
The Saunders-Roe SR.53 was a successful design, and was planned for production when economics forced the British to curtail most aircraft programs in the late 1950s. Furthermore, rapid advancements in jet engine technology rendered mixed-power aircraft designs like Saunders-Roe's SR.53 (and the following SR.177) obsolete. The American Republic XF-91 Thunderceptor –the first U.S. fighter to exceed Mach 1 in level flight– met a similar fate for the same reason, and no hybrid rocket-and-jet-engine fighter design has ever been placed into service.
The only operational implementation of mixed propulsion was Rocket-Assisted Take Off (RATO), a system rarely used in fighters, such as with the zero-length launch, RATO-based takeoff scheme from special launch platforms, tested out by both the United States and the Soviet Union, and made obsolete with advancements in surface-to-air missile technology.
It has become common in the aviation community to classify jet fighters by "generations" for historical purposes. No official definitions of these generations exist; rather, they represent the notion of stages in the development of fighter-design approaches, performance capabilities, and technological evolution. Different authors have packed jet fighters into different generations. For example, Richard P. Hallion of the Secretary of the Air Force's Action Group classified the F-16 as a sixth-generation jet fighter.
The timeframes associated with each generation remain inexact and are only indicative of the period during which their design philosophies and technology employment enjoyed a prevailing influence on fighter design and development. These timeframes also encompass the peak period of service entry for such aircraft.
The first generation of jet fighters comprised the initial, subsonic jet-fighter designs introduced late in World War II (1939–1945) and in the early post-war period. They differed little from their piston-engined counterparts in appearance, and many employed unswept wings. Guns and cannon remained the principal armament. The need to obtain a decisive advantage in maximum speed pushed the development of turbojet-powered aircraft forward. Top speeds for fighters rose steadily throughout World War II as more powerful piston engines developed, and they approached transonic flight-speeds where the efficiency of propellers drops off, making further speed increases nearly impossible.
The first jets developed during World War II and saw combat in the last two years of the war. Messerschmitt developed the first operational jet fighter, the Me 262A, primarily serving with the Luftwaffe's JG 7, the world's first jet-fighter wing. It was considerably faster than contemporary piston-driven aircraft, and in the hands of a competent pilot, proved quite difficult for Allied pilots to defeat. The Luftwaffe never deployed the design in numbers sufficient to stop the Allied air campaign, and a combination of fuel shortages, pilot losses, and technical difficulties with the engines kept the number of sorties low. Nevertheless, the Me 262 indicated the obsolescence of piston-driven aircraft. Spurred by reports of the German jets, Britain's Gloster Meteor entered production soon after, and the two entered service around the same time in 1944. Meteors commonly served to intercept the V-1 flying bomb, as they were faster than available piston-engined fighters at the low altitudes used by the flying bombs. Nearer the end of World War II, the first military jet-powered light-fighter design, the Luftwaffe intended the Heinkel He 162A "Spatz" (sparrow) to serve as a simple jet fighter for German home defense, with a few examples seeing squadron service with JG 1 by April 1945. By the end of the war almost all work on piston-powered fighters had ended. A few designs combining piston- and jet-engines for propulsion – such as the Ryan FR Fireball – saw brief use, but by the end of the 1940s virtually all new fighters were jet-powered.
Despite their advantages, the early jet-fighters were far from perfect. The operational lifespan of turbines were very short and engines were temperamental, while power could be adjusted only slowly and acceleration was poor (even if top speed was higher) compared to the final generation of piston fighters. Many squadrons of piston-engined fighters remained in service until the early to mid-1950s, even in the air forces of the major powers (though the types retained were the best of the World War II designs). Innovations including ejection seats, air brakes and all-moving tailplanes became widespread in this period.
The Americans began using jet fighters operationally after World War II, the wartime Bell P-59 having proven a failure. The Lockheed P-80 Shooting Star (soon re-designated F-80) was less elegant than the swept-wing Me 262, but had a cruise speed () as high as the maximum speed attainable by many piston-engined fighters. The British designed several new jets, including the distinctive single-engined twin boom de Havilland Vampire which Britain sold to the air forces of many nations.
The British transferred the technology of the Rolls-Royce Nene jet-engine to the Soviets, who soon put it to use in their advanced Mikoyan-Gurevich MiG-15 fighter, which used fully swept wings that allowed flying closer to the speed of sound than straight-winged designs such as the F-80. The MiG-15s' top speed of proved quite a shock to the American F-80 pilots who encountered them in the Korean War, along with their armament of two 23 mm cannons and a single 37 mm cannon. Nevertheless, in the first jet-versus-jet dogfight, which occurred during the Korean War on 8 November 1950, an F-80 shot down two North Korean MiG-15s
The Americans responded by rushing their own swept-wing fighter – the North American F-86 Sabre – into battle against the MiGs, which had similar transsonic performance. The two aircraft had different strengths and weaknesses, but were similar enough that victory could go either way. While the Sabres focused primarily on downing MiGs and scored favorably against those flown by the poorly-trained North Koreans, the MiGs in turn decimated US bomber formations and forced the withdrawal of numerous American types from operational service.
The world's navies also transitioned to jets during this period, despite the need for catapult-launching of the new aircraft. The U.S. Navy adopted the Grumman F9F Panther as their primary jet fighter in the Korean War period, and it was one of the first jet fighters to employ an afterburner. The de Havilland Sea Vampire became the Royal Navy's first jet fighter. Radar was used on specialized night-fighters such as the Douglas F3D Skyknight, which also downed MiGs over Korea, and later fitted to the McDonnell F2H Banshee and swept-wing Vought F7U Cutlass and McDonnell F3H Demon as all-weather / night fighters. Early versions of Infra-red (IR) air-to-air missiles (AAMs) such as the AIM-9 Sidewinder and radar-guided missiles such as the AIM-7 Sparrow whose descendants remain in use , were first introduced on swept-wing subsonic Demon and Cutlass naval fighters.
Technological breakthroughs, lessons learned from the aerial battles of the Korean War, and a focus on conducting operations in a nuclear warfare environment shaped the development of second-generation fighters. Technological advances in aerodynamics, propulsion and aerospace building-materials (primarily aluminum alloys) permitted designers to experiment with aeronautical innovations such as swept wings, delta wings, and area-ruled fuselages. Widespread use of afterburning turbojet engines made these the first production aircraft to break the sound barrier, and the ability to sustain supersonic speeds in level flight became a common capability amongst fighters of this generation.
Fighter designs also took advantage of new electronics technologies that made effective radars small enough to carry aboard smaller aircraft. Onboard radars permitted detection of enemy aircraft beyond visual range, thereby improving the handoff of targets by longer-ranged ground-based warning- and tracking-radars. Similarly, advances in guided-missile development allowed air-to-air missiles to begin supplementing the gun as the primary offensive weapon for the first time in fighter history. During this period, passive-homing infrared-guided (IR) missiles became commonplace, but early IR missile sensors had poor sensitivity and a very narrow field of view (typically no more than 30°), which limited their effective use to only close-range, tail-chase engagements. Radar-guided (RF) missiles were introduced as well, but early examples proved unreliable. These semi-active radar homing (SARH) missiles could track and intercept an enemy aircraft "painted" by the launching aircraft's onboard radar. Medium- and long-range RF air-to-air missiles promised to open up a new dimension of "beyond-visual-range" (BVR) combat, and much effort concentrated on further development of this technology.
The prospect of a potential third world war featuring large mechanized armies and nuclear-weapon strikes led to a degree of specialization along two design approaches: interceptors, such as the English Electric Lightning and Mikoyan-Gurevich MiG-21F; and fighter-bombers, such as the Republic F-105 Thunderchief and the Sukhoi Su-7B. Dogfighting, "per se", became de-emphasized in both cases. The interceptor was an outgrowth of the vision that guided missiles would completely replace guns and combat would take place at beyond-visual ranges. As a result, strategists designed interceptors with a large missile-payload and a powerful radar, sacrificing agility in favor of high speed, altitude ceiling and rate of climb. With a primary air-defense role, emphasis was placed on the ability to intercept strategic bombers flying at high altitudes. Specialized point-defense interceptors often had limited range and little, if any, ground-attack capabilities. Fighter-bombers could swing between air-superiority and ground-attack roles, and were often designed for a high-speed, low-altitude dash to deliver their ordnance. Television- and IR-guided air-to-surface missiles were introduced to augment traditional gravity bombs, and some were also equipped to deliver a nuclear bomb.
The third generation witnessed continued maturation of second-generation innovations, but it is most marked by renewed emphases on maneuverability and on traditional ground-attack capabilities. Over the course of the 1960s, increasing combat experience with guided missiles demonstrated that combat would devolve into close-in dogfights. Analog avionics began to appear, replacing older "steam-gauge" cockpit instrumentation. Enhancements to the aerodynamic performance of third-generation fighters included flight control surfaces such as canards, powered slats, and blown flaps. A number of technologies would be tried for Vertical/Short Takeoff and Landing, but thrust vectoring would be successful on the Harrier.
Growth in air-combat capability focused on the introduction of improved air-to-air missiles, radar systems, and other avionics. While guns remained standard equipment (early models of F-4 being a notable exception), air-to-air missiles became the primary weapons for air-superiority fighters, which employed more sophisticated radars and medium-range RF AAMs to achieve greater "stand-off" ranges, however, kill probabilities proved unexpectedly low for RF missiles due to poor reliability and improved electronic countermeasures (ECM) for spoofing radar seekers. Infrared-homing AAMs saw their fields of view expand to 45°, which strengthened their tactical usability. Nevertheless, the low dogfight loss-exchange ratios experienced by American fighters in the skies over Vietnam led the U.S. Navy to establish its famous "TOPGUN" fighter-weapons school, which provided a graduate-level curriculum to train fleet fighter-pilots in advanced Air Combat Maneuvering (ACM) and Dissimilar air combat training (DACT) tactics and techniques.
This era also saw an expansion in ground-attack capabilities, principally in guided missiles, and witnessed the introduction of the first truly effective avionics for enhanced ground attack, including terrain-avoidance systems. Air-to-surface missiles (ASM) equipped with electro-optical (E-O) contrast seekers – such as the initial model of the widely used AGM-65 Maverick – became standard weapons, and laser-guided bombs (LGBs) became widespread in an effort to improve precision-attack capabilities. Guidance for such precision-guided munitions (PGM) was provided by externally-mounted targeting pods, which were introduced in the mid-1960s.
The third generation also led to the development of new automatic-fire weapons, primarily chain-guns that use an electric motor to drive the mechanism of a cannon. This allowed a plane to carry a single multi-barrel weapon (such as the 20 mm Vulcan), and provided greater accuracy and rates of fire. Powerplant reliability increased, and jet engines became "smokeless" to make it harder to sight aircraft at long distances.
Dedicated ground-attack aircraft (like the Grumman A-6 Intruder, SEPECAT Jaguar and LTV A-7 Corsair II) offered longer range, more sophisticated night-attack systems or lower cost than supersonic fighters. With variable-geometry wings, the supersonic F-111 introduced the Pratt & Whitney TF30, the first turbofan equipped with afterburner. The ambitious project sought to create a versatile common fighter for many roles and services. It would serve well as an all-weather bomber, but lacked the performance to defeat other fighters. The McDonnell F-4 Phantom was designed to capitalize on radar and missile technology as an all-weather interceptor, but emerged as a versatile strike-bomber nimble enough to prevail in air combat, adopted by the U.S. Navy, Air Force and Marine Corps. Despite numerous shortcomings that would be not be fully addressed until newer fighters, the Phantom claimed 280 aerial kills (more than any other U.S. fighter) over Vietnam. With range and payload capabilities that rivaled that of World War II bombers such as B-24 Liberator, the Phantom would become a highly successful multirole aircraft.
 Fourth-generation fighters continued the trend towards multirole configurations, and were equipped with increasingly sophisticated avionics- and weapon-systems. Fighter designs were significantly influenced by the Energy-Maneuverability (E-M) theory developed by Colonel John Boyd and mathematician Thomas Christie, based upon Boyd's combat experience in the Korean War and as a fighter-tactics instructor during the 1960s. E-M theory emphasized the value of aircraft-specific energy maintenance as an advantage in fighter combat. Boyd perceived maneuverability as the primary means of getting "inside" an adversary's decision-making cycle, a process Boyd called the "OODA loop" (for "Observation-Orientation-Decision-Action"). This approach emphasized aircraft designs capable of performing "fast transients" – quick changes in speed, altitude, and direction – as opposed to relying chiefly on high speed alone.
E-M characteristics were first applied to the McDonnell Douglas F-15 Eagle, but Boyd and his supporters believed these performance parameters called for a small, lightweight aircraft with a larger, higher-lift wing. The small size would minimize drag and increase the thrust-to-weight ratio, while the larger wing would minimize wing loading; while the reduced wing loading tends to lower top speed and can cut range, it increases payload capacity and the range reduction can be compensated for by increased fuel in the larger wing. The efforts of Boyd's "Fighter mafia" would result in the General Dynamics F-16 Fighting Falcon (now Lockheed Martin's).
The F-16's maneuverability was further enhanced by its slight aerodynamic instability. This technique, called "relaxed static stability" (RSS), was made possible by introduction of the "fly-by-wire" (FBW) flight-control system (FLCS), which in turn was enabled by advances in computers and in system-integration techniques. Analog avionics, required to enable FBW operations, became a fundamental requirement, but began to be replaced by digital flight-control systems in the latter half of the 1980s. Likewise, Full Authority Digital Engine Controls (FADEC) to electronically manage powerplant performance was introduced with the Pratt & Whitney F100 turbofan. The F-16's sole reliance on electronics and wires to relay flight commands, instead of the usual cables and mechanical linkage controls, earned it the sobriquet of "the electric jet". Electronic FLCS and FADEC quickly became essential components of all subsequent fighter designs.
Other innovative technologies introduced in fourth-generation fighters included pulse-Doppler fire-control radars (providing a "look-down/shoot-down" capability), head-up displays (HUD), "hands on throttle-and-stick" (HOTAS) controls, and multi-function displays (MFD), all essential equipment . Aircraft designers began to incorporate composite materials in the form of bonded-aluminum honeycomb structural elements and graphite epoxy laminate skins to reduce weight. Infrared search-and-track (IRST) sensors became widespread for air-to-ground weapons delivery, and appeared for air-to-air combat as well. "All-aspect" IR AAM became standard air superiority weapons, which permitted engagement of enemy aircraft from any angle (although the field of view remained relatively limited). The first long-range active-radar-homing RF AAM entered service with the AIM-54 Phoenix, which solely equipped the Grumman F-14 Tomcat, one of the few variable-sweep-wing fighter designs to enter production. Even with the tremendous advancement of air-to-air missiles in this era, internal guns were standard equipment.
Another revolution came in the form of a stronger reliance on ease of maintenance, which led to standardization of parts, reductions in the numbers of access panels and lubrication points, and overall parts reduction in more complicated equipment like the engines. Some early jet fighters required 50 man-hours of work by a ground crew for every hour the aircraft was in the air; later models substantially reduced this to allow faster turn-around times and more sorties in a day. Some modern military aircraft only require 10-man-hours of work per hour of flight time, and others are even more efficient.
Aerodynamic innovations included variable-camber wings and exploitation of the vortex lift effect to achieve higher angles of attack through the addition of leading-edge extension devices such as strakes.
Unlike interceptors of the previous eras, most fourth-generation air-superiority fighters were designed to be agile dogfighters (although the Mikoyan MiG-31 and Panavia Tornado ADV are notable exceptions). The continually rising cost of fighters, however, continued to emphasize the value of multirole fighters. The need for both types of fighters led to the "high/low mix" concept, which envisioned a high-capability and high-cost core of dedicated air-superiority fighters (like the F-15 and Su-27) supplemented by a larger contingent of lower-cost multi-role fighters (such as the F-16 and MiG-29).
Most fourth-generation fighters, such as the McDonnell Douglas F/A-18 Hornet, HAL Tejas, JF-17 and Dassault Mirage 2000, are true multirole warplanes, designed as such from the start. This was facilitated by multimode avionics that could switch seamlessly between air and ground modes. The earlier approaches of adding on strike capabilities or designing separate models specialized for different roles generally became "passé" (with the Panavia Tornado being an exception in this regard). Attack roles were generally assigned to dedicated ground-attack aircraft such as the Sukhoi Su-25 and the A-10 Thunderbolt II.
A typical US Air Force fighter wing of the period might contain a mix of one air superiority squadron (F-15C), one strike fighter squadron (F-15E), and two multirole fighter squadrons (F-16C).
Perhaps the most novel technology introduced for combat aircraft was "stealth", which involves the use of special "low-observable" (L-O) materials and design techniques to reduce the susceptibility of an aircraft to detection by the enemy's sensor systems, particularly radars. The first stealth aircraft introduced were the Lockheed F-117 Nighthawk attack aircraft (introduced in 1983) and the Northrop Grumman B-2 Spirit bomber (first flew in 1989). Although no stealthy fighters per se appeared among the fourth generation, some radar-absorbent coatings and other L-O treatments developed for these programs are reported to have been subsequently applied to fourth-generation fighters.
The end of the Cold War in 1991 led many governments to significantly decrease military spending as a "peace dividend". Air force inventories were cut. Research and development programs working on "fifth-generation" fighters took serious hits. Many programs were canceled during the first half of the 1990s, and those that survived were "stretched out". While the practice of slowing the pace of development reduces annual investment expenses, it comes at the penalty of increased overall program and unit costs over the long-term. In this instance, however, it also permitted designers to make use of the tremendous achievements being made in the fields of computers, avionics and other flight electronics, which had become possible largely due to the advances made in microchip and semiconductor technologies in the 1980s and 1990s. This opportunity enabled designers to develop fourth-generation designs – or redesigns – with significantly enhanced capabilities. These improved designs have become known as "Generation 4.5" fighters, recognizing their intermediate nature between the 4th and 5th generations, and their contribution in furthering development of individual fifth-generation technologies.
The primary characteristics of this sub-generation are the application of advanced digital avionics and aerospace materials, modest signature reduction (primarily RF "stealth"), and highly integrated systems and weapons. These fighters have been designed to operate in a "network-centric" battlefield environment and are principally multirole aircraft. Key weapons technologies introduced include beyond-visual-range (BVR) AAMs; Global Positioning System (GPS)-guided weapons, solid-state phased-array radars; helmet-mounted sights; and improved secure, jamming-resistant datalinks. Thrust vectoring to further improve transient maneuvering capabilities has also been adopted by many 4.5th generation fighters, and uprated powerplants have enabled some designs to achieve a degree of "supercruise" ability. Stealth characteristics are focused primarily on frontal-aspect radar cross section (RCS) signature-reduction techniques including radar-absorbent materials (RAM), L-O coatings and limited shaping techniques.
"Half-generation" designs are either based on existing airframes or are based on new airframes following similar design theory to previous iterations; however, these modifications have introduced the structural use of composite materials to reduce weight, greater fuel fractions to increase range, and signature reduction treatments to achieve lower RCS compared to their predecessors. Prime examples of such aircraft, which are based on new airframe designs making extensive use of carbon-fiber composites, include the Eurofighter Typhoon, Dassault Rafale, Saab JAS 39 Gripen, and HAL Tejas Mark 1A.
Apart from these fighter jets, most of the 4.5 generation aircraft are actually modified variants of existing airframes from the earlier fourth generation fighter jets. Such fighter jets are generally heavier and examples include the Boeing F/A-18E/F Super Hornet, which is an evolution of the F/A-18 Hornet, the F-15E Strike Eagle, which is a ground-attack/multi-role variant of the F-15 Eagle, the Su-30SM and Su-35S modified variants of the Sukhoi Su-27, and the MiG-35 upgraded version of the Mikoyan MiG-29. The Su-30SM/Su-35S and MiG-35 feature thrust vectoring engine nozzles to enhance maneuvering. The upgraded version of F-16 is also considered a member of the 4.5 generation aircraft.
4.5 generation fighters first entered service in the early 1990s, and most of them are still being produced and evolved. It is quite possible that they may continue in production alongside fifth-generation fighters due to the expense of developing the advanced level of stealth technology needed to achieve aircraft designs featuring very low observables (VLO), which is one of the defining features of fifth-generation fighters. Of the 4.5th generation designs, the Strike Eagle, Super Hornet, Typhoon, Gripen, and Rafale have been used in combat.
The U.S. government has defined 4.5 generation fighter aircraft as those that "(1) have advanced capabilities, including— (A) AESA radar; (B) high capacity data-link; and (C) enhanced avionics; and (2) have the ability to deploy current and reasonably foreseeable advanced armaments."
Currently the cutting edge of fighter design, fifth-generation fighters are characterized by being designed from the start to operate in a network-centric combat environment, and to feature extremely low, all-aspect, multi-spectral signatures employing advanced materials and shaping techniques. They have multifunction AESA radars with high-bandwidth, low-probability of intercept (LPI) data transmission capabilities. The Infra-red search and track sensors incorporated for air-to-air combat as well as for air-to-ground weapons delivery in the 4.5th generation fighters are now fused in with other sensors for Situational Awareness IRST or SAIRST, which constantly tracks all targets of interest around the aircraft so the pilot need not guess when he glances. These sensors, along with advanced avionics, glass cockpits, helmet-mounted sights (not currently on F-22), and improved secure, jamming-resistant LPI datalinks are highly integrated to provide multi-platform, multi-sensor data fusion for vastly improved situational awareness while easing the pilot's workload. Avionics suites rely on extensive use of very high-speed integrated circuit (VHSIC) technology, common modules, and high-speed data buses. Overall, the integration of all these elements is claimed to provide fifth-generation fighters with a "first-look, first-shot, first-kill capability".
A key attribute of fifth-generation fighters is a small radar cross-section. Great care has been taken in designing its layout and internal structure to minimize RCS over a broad bandwidth of detection and tracking radar frequencies; furthermore, to maintain its VLO signature during combat operations, primary weapons are carried in internal weapon bays that are only briefly opened to permit weapon launch. Furthermore, stealth technology has advanced to the point where it can be employed without a tradeoff with aerodynamics performance, in contrast to previous stealth efforts. Some attention has also been paid to reducing IR signatures, especially on the F-22. Detailed information on these signature-reduction techniques is classified, but in general includes special shaping approaches, thermoset and thermoplastic materials, extensive structural use of advanced composites, conformal sensors, heat-resistant coatings, low-observable wire meshes to cover intake and cooling vents, heat ablating tiles on the exhaust troughs (seen on the Northrop YF-23), and coating internal and external metal areas with radar-absorbent materials and paint (RAM/RAP).
The AESA radar offers unique capabilities for fighters (and it is also quickly becoming essential for Generation 4.5 aircraft designs, as well as being retrofitted onto some fourth-generation aircraft). In addition to its high resistance to ECM and LPI features, it enables the fighter to function as a sort of "mini-AWACS", providing high-gain electronic support measures (ESM) and electronic warfare (EW) jamming functions. Other technologies common to this latest generation of fighters includes integrated electronic warfare system (INEWS) technology, integrated communications, navigation, and identification (CNI) avionics technology, centralized "vehicle health monitoring" systems for ease of maintenance, fiber optics data transmission, stealth technology and even hovering capabilities. Maneuver performance remains important and is enhanced by thrust-vectoring, which also helps reduce takeoff and landing distances. Supercruise may or may not be featured; it permits flight at supersonic speeds without the use of the afterburner – a device that significantly increases IR signature when used in full military power.
Such aircraft are sophisticated and expensive. The fifth generation was ushered in by the Lockheed Martin/Boeing F-22 Raptor in late 2005. The U.S. Air Force originally planned to acquire 650 F-22s, but now only 187 will be built. As a result, its unit flyaway cost (FAC) is around US$150 million. To spread the development costs – and production base – more broadly, the Joint Strike Fighter (JSF) program enrolls eight other countries as cost- and risk-sharing partners. Altogether, the nine partner nations anticipate procuring over 3,000 Lockheed Martin F-35 Lightning II fighters at an anticipated average FAC of $80–85 million. The F-35, however, is designed to be a family of three aircraft, a conventional take-off and landing (CTOL) fighter, a short take-off and vertical landing (STOVL) fighter, and a Catapult Assisted Take Off But Arrested Recovery (CATOBAR) fighter, each of which has a different unit price and slightly varying specifications in terms of fuel capacity (and therefore range), size and payload.
Other countries have initiated fifth-generation fighter development projects, with Russia's Sukhoi Su-57 and Mikoyan LMFS. In December 2010, it was discovered that China is developing the 5th generation fighter Chengdu J-20. The J-20 took its maiden flight in January 2011. The Shenyang J-31 took its maiden flight on 31 October 2012. Japan is exploring its technical feasibility to produce fifth-generation fighters. India is developing the Advanced Medium Combat Aircraft (AMCA), a medium weight stealth fighter jet designated to enter into serial production by late 2030s. India also had initiated a joint fifth generation heavy fighter with Russia called the FGFA. May, the project is suspected to have not yielded desired progress or results for India and has been put on hold or dropped altogether. Other countries considering fielding an indigenous or semi-indigenous advanced fifth generation aircraft include Korea, Sweden, Turkey and Pakistan.
As of November 2018, France, Germany, Japan, Russia, the United Kingdom and the United States have announced the development of a sixth-generation aircraft program.
France and Germany will develop a joint sixth-generation fighter to replace their current fleet of Dassault Rafales, Eurofighter Typhoons, and Panavia Tornados by 2035. The overall development will be led by a collaboration of Dassault and Airbus, while the engines will reportedly be jointly developed by Safran and MTU Aero Engines. Thales and MBDA are also seeking a stake in the project. Spain is reportedly planning to join the program in the later stages and is expected to sign a letter of intent in early 2019.
Currently at the concept stage, the first sixth-generation jet fighter is expected to enter service in the United States Navy in 2025–30 period. The USAF seeks a new fighter for the 2030–50 period named the "Next Generation Tactical Aircraft" ("Next Gen TACAIR"). The US Navy looks to replace its F/A-18E/F Super Hornets beginning in 2025 with the Next Generation Air Dominance air superiority fighter.
The United Kingdom's proposed stealth fighter is being developed by a European consortium called "Team Tempest", consisting of BAE Systems, Rolls-Royce, Leonardo S.p.A. and MBDA. The aircraft is intended to enter service in 2035.
Fighters were typically armed with guns only for air to air combat up through the late 1950s, though unguided rockets for mostly air to ground use and limited air to air use were deployed in WWII. From the late 1950s forward guided missiles came into use for air to air combat. Throughout this history fighters which by surprise or maneuver attain a good firing position have achieved the kill about one third to one half the time, no matter what weapons were carried. The only major historic exception to this has been the low effectiveness shown by guided missiles in the first one to two decades of their existence.
From WWI to the present fighter aircraft have featured machine guns and automatic cannons as weapons, and they are still considered as essential back-up weapons today. The power of air-to-air guns has increased greatly over time, and has kept them relevant in the guided missile era. In WWI two rifle caliber machine guns was the typical armament producing a weight of fire of about per second. The standard WWII American fighter armament of six 0.50-cal (12.7mm) machine guns fired a bullet weight of approximately 3.7 kg/sec (8.1 lbs/sec), at a muzzle velocity of 856 m/s (2,810 ft/s). British and German aircraft tended to use a mix of machine guns and autocannon, the latter firing explosive projectiles. The modern M61 Vulcan 20 mm rotating barrel Gatling gun that is standard on current American fighters fires a projectile weight of about 10 kg/s (22 lb/s), nearly three times that of six 0.50-cal machine guns, with higher velocity of 1,052 m/s (3450 ft/s) supporting a flatter trajectory, and with exploding projectiles. Modern fighter gun systems also feature ranging radar and lead computing electronic gun sights to ease the problem of aim point to compensate for projectile drop and time of flight (target lead) in the complex three dimensional maneuvering of air-to-air combat. However, getting in position to use the guns is still a challenge. The range of guns is longer than in the past but still quite limited compared to missiles, with modern gun systems having a maximum effective range of approximately 1,000 meters. High probability of kill also requires firing to usually occur from the rear hemisphere of the target. Despite these limits, when pilots are well trained in air-to-air gunnery and these conditions are satisfied, gun systems are tactically effective and highly cost efficient. The cost of a gun firing pass is far less than firing a missile, and the projectiles are not subject to the thermal and electronic countermeasures than can sometimes defeat missiles. When the enemy can be approached to within gun range, the lethality of guns is approximately a 25% to 50% chance of "kill per firing pass".
The range limitations of guns, and the desire to overcome large variations in fighter pilot skill and thus achieve higher force effectiveness, led to the development of the guided air-to-air missile. There are two main variations, heat-seeking (infrared homing), and radar guided. Radar missiles are typically several times heavier and more expensive than heat-seekers, but with longer range, greater destructive power, and ability to track through clouds.
The highly successful AIM-9 Sidewinder heat-seeking (infrared homing) short-range missile was developed by the United States Navy in the 1950s. These small missiles are easily carried by lighter fighters, and provide effective ranges of approximately 10 to 35 km (~6 to 22 miles). Beginning with the AIM-9L in 1977, subsequent versions of Sidewinder have added all-aspect capability, the ability to use the lower heat of air to skin friction on the target aircraft to track from the front and sides. The latest (2003 service entry) AIM-9X also features "off-boresight" and "lock on after launch" capabilities, which allow the pilot to make a quick launch of a missile to track a target anywhere within the pilot's vision. The AIM-9X development cost was U.S. $3 billion in mid to late 1990s dollars, and 2015 per unit procurement cost is $0.6 million each. The missile weighs 85.3 kg (188 lbs), and has a maximum range of 35 km (22 miles) at higher altitudes. Like most air-to-air missiles, lower altitude range can be as limited as only about one third of maximum due to higher drag and less ability to coast downward.
The effectiveness of heat-seeking missiles was only 7% early in the Vietnam War, but improved to approximately 15%–40% over the course of the war. The AIM-4 Falcon used by the USAF had kill rates of approximately 7% and was considered a failure. The AIM-9B Sidewinder introduced later achieved 15% kill rates, and the further improved AIM-9D and J models reached 19%. The AIM-9G used in the last year of the Vietnam air war achieved 40%. Israel used almost totally guns in the 1967 Six-Day War, achieving 60 kills and 10 losses. However, Israel made much more use of steadily improving heat-seeking missiles in the 1973 Yom Kippur War. In this extensive conflict Israel scored 171 of out of 261 total kills with heat-seeking missiles (65.5%), 5 kills with radar guided missiles (1.9%), and 85 kills with guns (32.6%). The AIM-9L Sidewinder scored 19 kills out of 26 fired missiles (73%) in the 1982 Falklands War. But, in a conflict against opponents using thermal countermeasures, the United States only scored 11 kills out of 48 fired (Pk = 23%) with the follow-on AIM-9M in the 1991 Gulf War.
Radar guided missiles fall into two main missile guidance types. In the historically more common semi-active radar homing case the missile homes in on radar signals transmitted from launching aircraft and reflected from the target. This has the disadvantage that the firing aircraft must maintain radar lock on the target and is thus less free to maneuver and more vulnerable to attack. A widely deployed missile of this type was the AIM-7 Sparrow, which entered service in 1954 and was produced in improving versions until 1997. In more advanced active radar homing the missile is guided to the vicinity of the target by internal data on its projected position, and then "goes active" with an internally carried small radar system to conduct terminal guidance to the target. This eliminates the requirement for the firing aircraft to maintain radar lock, and thus greatly reduces risk. A prominent example is the AIM-120 AMRAAM, which was first fielded in 1991 as the AIM-7 replacement, and which has no firm retirement date . The current AIM-120D version has a maximum high altitude range of greater than 160 km (>99 miles), and cost approximately $2.4 million each (2016). As is typical with most other missiles, range at lower altitude may be as little as one third that of high altitude.
In the Vietnam air war radar missile kill reliability was approximately 10% at shorter ranges, and even worse at longer ranges due to reduced radar return and greater time for the target aircraft to detect the incoming missile and take evasive action. At one point in the Vietnam war, the U.S. Navy fired 50 AIM-7 Sparrow radar guided missiles in a row without a hit. Between 1958 and 1982 in five wars there were 2,014 combined heat-seeking and radar guided missile firings by fighter pilots engaged in air-to-air combat, achieving 528 kills, of which 76 were radar missile kills, for a combined effectiveness of 26%. However, only four of the 76 radar missile kills were in the beyond-visual-range mode intended to be the strength of radar guided missiles. The United States invested over $10 billion in air-to-air radar missile technology from the 1950s to the early 1970s. Amortized over actual kills achieved by the U.S. and its allies, each radar guided missile kill thus cost over $130 million. The defeated enemy aircraft were for the most part older MiG-17s, −19s, and −21s, with new cost of $0.3 million to $3 million each. Thus, the radar missile investment over that period far exceeded the value of enemy aircraft destroyed, and furthermore had very little of the intended BVR effectiveness.
However, continuing heavy development investment and rapidly advancing electronic technology led to significant improvement in radar missile reliabilities from the late 1970s onward. Radar guided missiles achieved 75% Pk (9 kills out of 12 shots) in operations in the Gulf War in 1991. The percentage of kills achieved by radar guided missiles also surpassed 50% of total kills for the first time by 1991. Since 1991, 20 of 61 kills worldwide have been beyond-visual-range using radar missiles. Discounting an accidental friendly fire kill, in operational use the AIM-120D (the current main American radar guided missile) has achieved 9 kills out of 16 shots for a 56% Pk. Six of these kills were BVR, out of 13 shots, for a 46% BVR Pk. Though all these kills were against less capable opponents who were not equipped with operating radar, electronic countermeasures, or a comparable weapon themselves, the BVR Pk was a significant improvement from earlier eras. However, a current concern is electronic countermeasures to radar missiles, which are thought to be reducing the effectiveness of the AIM-120D. Some experts believe that the European Meteor missile, the Russian R-37M, and the Chinese PL-15 are more resistant to countermeasures and more effective than the AIM-120D.
Now that higher reliabilities have been achieved, both types of missiles allow the fighter pilot to often avoid the risk of the short-range dogfight, where only the more experienced and skilled fighter pilots tend to prevail, and where even the finest fighter pilot can simply get unlucky. Taking maximum advantage of complicated missile parameters in both attack and defense against competent opponents does take considerable experience and skill, but against surprised opponents lacking comparable capability and countermeasures, air-to-air missile warfare is relatively simple. By partially automating air-to-air combat and reducing reliance on gun kills mostly achieved by only a small expert fraction of fighter pilots, air-to-air missiles now serve as highly effective force multipliers.

</doc>
<doc id="10930" url="https://en.wikipedia.org/wiki?curid=10930" title="February 25">
February 25

It is the feast day of Saint Walpurga and is a day of national significance in Hungary, Kuwait, the Philippines and Suriname. Among famous events on the day have been the patent for Samuel Colt's revolver (1836); the first African American sworn into the United States Senate (1870); Hitler obtaining German citizenship so he could run for public office (1932); the construction in an Islington garden of the first Anderson shelter (1939); Nikita Khrushchev's denunciation of Stalin (1956); and the disbandment of the Warsaw Pact (1991).
People born on 25 February include José de San Martín (1788), the first President of Peru; Impressionist painter Pierre-Auguste Renoir (1841); operatic tenor Enrico Caruso (1873); actor Tom Courtenay (1937); musician George Harrison (1943); and eight-time Olympic champion Birgit Fischer (1962). Among those who have died on the day are Albrecht von Wallenstein, Austrian general in the Thirty Years War (1634); Christopher Wren, English architect who designed St Paul's Cathedral (1723); Paul Reuter, founder of the Reuters news agency (1899); Prohibition Era gangster Bugs Moran (1957); acclaimed playwright Tennessee Williams (1983); and record-breaking Australian cricketer Don Bradman (2001).

</doc>
<doc id="10931" url="https://en.wikipedia.org/wiki?curid=10931" title="Finite-state machine">
Finite-state machine

A finite-state machine (FSM) or finite-state automaton (FSA, plural: "automata"), finite automaton, or simply a state machine, is a mathematical model of computation. It is an abstract machine that can be in exactly one of a finite number of "states" at any given time. The FSM can change from one state to another in response to some inputs; the change from one state to another is called a "transition". An FSM is defined by a list of its states, its initial state, and the inputs that trigger each transition. Finite-state machines are of two types—deterministic finite-state machines and non-deterministic finite-state machines. A deterministic finite-state machine can be constructed equivalent to any non-deterministic one.
The behavior of state machines can be observed in many devices in modern society that perform a predetermined sequence of actions depending on a sequence of events with which they are presented. Simple examples are vending machines, which dispense products when the proper combination of coins is deposited, elevators, whose sequence of stops is determined by the floors requested by riders, traffic lights, which change sequence when cars are waiting, and combination locks, which require the input of a sequence of numbers in the proper order.
The finite-state machine has less computational power than some other models of computation such as the Turing machine. The computational power distinction means there are computational tasks that a Turing machine can do but an FSM cannot. This is because an FSM's memory is limited by the number of states it has. FSMs are studied in the more general field of automata theory.
An example of a simple mechanism that can be modeled by a state machine is a turnstile. A turnstile, used to control access to subways and amusement park rides, is a gate with three rotating arms at waist height, one across the entryway. Initially the arms are locked, blocking the entry, preventing patrons from passing through. Depositing a coin or token in a slot on the turnstile unlocks the arms, allowing a single customer to push through. After the customer passes through, the arms are locked again until another coin is inserted.
Considered as a state machine, the turnstile has two possible states: Locked and Unlocked. There are two possible inputs that affect its state: putting a coin in the slot (coin) and pushing the arm (push). In the locked state, pushing on the arm has no effect; no matter how many times the input push is given, it stays in the locked state. Putting a coin in – that is, giving the machine a coin input – shifts the state from Locked to Unlocked. In the unlocked state, putting additional coins in has no effect; that is, giving additional coin inputs does not change the state. However, a customer pushing through the arms, giving a push input, shifts the state back to Locked.
The turnstile state machine can be represented by a state-transition table, showing for each possible state, the transitions between them (based upon the inputs given to the machine) and the outputs resulting from each input:
The turnstile state machine can also be represented by a directed graph called a state diagram "(above)". Each state is represented by a node ("circle"). Edges ("arrows") show the transitions from one state to another. Each arrow is labeled with the input that triggers that transition. An input that doesn't cause a change of state (such as a coin input in the Unlocked state) is represented by a circular arrow returning to the original state. The arrow into the Locked node from the black dot indicates it is the initial state.
A "state" is a description of the status of a system that is waiting to execute a "transition". A transition is a set of actions to be executed when a condition is fulfilled or when an event is received.
For example, when using an audio system to listen to the radio (the system is in the "radio" state), receiving a "next" stimulus results in moving to the next station. When the system is in the "CD" state, the "next" stimulus results in moving to the next track. Identical stimuli trigger different actions depending on the current state.
In some finite-state machine representations, it is also possible to associate actions with a state:
Several state-transition table types are used. The most common representation is shown below: the combination of current state (e.g. B) and input (e.g. Y) shows the next state (e.g. C). The complete action's information is not directly described in the table and can only be added using footnotes. An FSM definition including the full actions information is possible using state tables (see also virtual finite-state machine).
The Unified Modeling Language has a notation for describing state machines. UML state machines overcome the limitations of traditional finite-state machines while retaining their main benefits. UML state machines introduce the new concepts of hierarchically nested states and orthogonal regions, while extending the notion of actions. UML state machines have the characteristics of both Mealy machines and Moore machines. They support actions that depend on both the state of the system and the triggering event, as in Mealy machines, as well as entry and exit actions, which are associated with states rather than transitions, as in Moore machines.
The Specification and Description Language is a standard from ITU that includes graphical symbols to describe actions in the transition:
SDL embeds basic data types called "Abstract Data Types", an action language, and an execution semantic in order to make the finite-state machine executable.
There are a large number of variants to represent an FSM such as the one in figure 3.
In addition to their use in modeling reactive systems presented here, finite-state machines are significant in many different areas, including electrical engineering, linguistics, computer science, philosophy, biology, mathematics, video game programming, and logic. Finite-state machines are a class of automata studied in automata theory and the theory of computation.
In computer science, finite-state machines are widely used in modeling of application behavior, design of hardware digital systems, software engineering, compilers, network protocols, and the study of computation and languages.
Finite-state machines can be subdivided into acceptors, classifiers, transducers and sequencers.
Acceptors (also called detectors or recognizers) produce binary output, indicating whether or not the received input is accepted. Each state of an acceptor is either "accepting" or "non accepting". Once all input has been received, if the current state is an accepting state, the input is accepted; otherwise it is rejected. As a rule, input is a sequence of symbols (characters); actions are not used. The start state can also be an accepting state, in which case the acceptor accepts the empty string. The example in figure 4 shows an acceptor that accepts the string "nice". In this acceptor, the only accepting state is state 7.
A (possibly infinite) set of symbol sequences, called a formal language, is a regular language if there is some acceptor that accepts "exactly" that set. For example, the set of binary strings with an even number of zeroes is a regular language (cf. Fig. 5), while the set of all strings whose length is a prime number is not.
An acceptor could also be described as defining a language that would contain every string accepted by the acceptor but none of the rejected ones; that language is "accepted" by the acceptor. By definition, the languages accepted by acceptors are the regular languages.
The problem of determining the language accepted by a given acceptor is an instance of the algebraic path problem—itself a generalization of the shortest path problem to graphs with edges weighted by the elements of an (arbitrary) semiring.
An example of an accepting state appears in Fig. 5: a deterministic finite automaton (DFA) that detects whether the binary input string contains an even number of 0s.
"S" (which is also the start state) indicates the state at which an even number of 0s has been input. S is therefore an accepting state. This acceptor will finish in an accept state, if the binary string contains an even number of 0s (including any binary string containing no 0s). Examples of strings accepted by this acceptor are ε (the empty string), 1, 11, 11..., 00, 010, 1010, 10110, etc.
Classifiers are a generalization of acceptors that produce "n"-ary output where "n" is strictly greater than two.
Transducers produce output based on a given input and/or a state using actions. They are used for control applications and in the field of computational linguistics.
In control applications, two types are distinguished:
Sequencers (also called generators) are a subclass of acceptors and transducers that have a single-letter input alphabet. They produce only one sequence which can be seen as an output sequence of acceptor or transducer outputs.
A further distinction is between deterministic (DFA) and non-deterministic (NFA, GNFA) automata. In a deterministic automaton, every state has exactly one transition for each possible input. In a non-deterministic automaton, an input can lead to one, more than one, or no transition for a given state. The powerset construction algorithm can transform any nondeterministic automaton into a (usually more complex) deterministic automaton with identical functionality.
A finite-state machine with only one state is called a "combinatorial FSM". It only allows actions upon transition "into" a state. This concept is useful in cases where a number of finite-state machines are required to work together, and when it is convenient to consider a purely combinatorial part as a form of FSM to suit the design tools.
There are other sets of semantics available to represent state machines. For example, there are tools for modeling and designing logic for embedded controllers. They combine hierarchical state machines (which usually have more than one current state), flow graphs, and truth tables into one language, resulting in a different formalism and set of semantics. These charts, like Harel's original state machines, support hierarchically nested states, orthogonal regions, state actions, and transition actions.
In accordance with the general classification, the following formal definitions are found.
A "deterministic finite-state machine" or "deterministic finite-state acceptor" is a quintuple formula_1, where:
For both deterministic and non-deterministic FSMs, it is conventional to allow formula_6 to be a partial function, i.e. formula_13 does not have to be defined for every combination of formula_14 and formula_15. If an FSM formula_16 is in a state formula_17, the next symbol is formula_18 and formula_13 is not defined, then formula_16 can announce an error (i.e. reject the input). This is useful in definitions of general state machines, but less useful when transforming the machine. Some algorithms in their default form may require total functions.
A finite-state machine has the same computational power as a Turing machine that is restricted such that its head may only perform "read" operations, and always has to move from left to right. That is, each formal language accepted by a finite-state machine is accepted by such a kind of restricted Turing machine, and vice versa.
A "finite-state transducer" is a sextuple formula_21, where:
If the output function depends on the state and input symbol (formula_30) that definition corresponds to the "Mealy model", and can be modelled as a Mealy machine. If the output function depends only on the state (formula_31) that definition corresponds to the "Moore model", and can be modelled as a Moore machine. A finite-state machine with no output function at all is known as a semiautomaton or transition system.
If we disregard the first output symbol of a Moore machine, formula_32, then it can be readily converted to an output-equivalent Mealy machine by setting the output function of every Mealy transition (i.e. labeling every edge) with the output symbol given of the destination Moore state. The converse transformation is less straightforward because a Mealy machine state may have different output labels on its incoming transitions (edges). Every such state needs to be split in multiple Moore machine states, one for every incident output symbol.
Optimizing an FSM means finding a machine with the minimum number of states that performs the same function. The fastest known algorithm doing this is the Hopcroft minimization algorithm. Other techniques include using an implication table, or the Moore reduction procedure. Additionally, acyclic FSAs can be minimized in linear time.
In a digital circuit, an FSM may be built using a programmable logic device, a programmable logic controller, logic gates and flip flops or relays. More specifically, a hardware implementation requires a register to store state variables, a block of combinational logic that determines the state transition, and a second block of combinational logic that determines the output of an FSM. One of the classic hardware implementations is the Richards controller.
In a "Medvedev machine", the output is directly connected to the state flip-flops minimizing the time delay between flip-flops and output.
Through state encoding for low power state machines may be optimized to minimize power consumption.
The following concepts are commonly used to build software applications with finite-state machines:
Finite automata are often used in the frontend of programming language compilers. Such a frontend may comprise several finite-state machines that implement a lexical analyzer and a parser.
Starting from a sequence of characters, the lexical analyzer builds a sequence of language tokens (such as reserved words, literals, and identifiers) from which the parser builds a syntax tree. The lexical analyzer and the parser handle the regular and context-free parts of the programming language's grammar.
Finite Markov-chain processes are also known as subshifts of finite type.

</doc>
<doc id="10933" url="https://en.wikipedia.org/wiki?curid=10933" title="Functional programming">
Functional programming

In computer science, functional programming is a programming paradigm where programs are constructed by applying and composing functions. It is a declarative programming paradigm in which function definitions are trees of expressions that each return a value, rather than a sequence of imperative statements which change the state of the program.
In functional programming, functions are treated as first-class citizens, meaning that they can be bound to names (including local identifiers), passed as arguments, and returned from other functions, just as any other data type can. This allows programs to be written in a declarative and composable style, where small functions are combined in a modular manner.
Functional programming is sometimes treated as synonymous with purely functional programming, a subset of functional programming which treats all functions as deterministic mathematical functions, or pure functions. When a pure function is called with some given arguments, it will always return the same result, and cannot be affected by any mutable state or other side effects. This is in contrast with impure procedures, common in imperative programming, which can have side effects (such as modifying the program's state or taking input from a user). Proponents of purely functional programming claim that by restricting side effects, programs can have fewer bugs, be easier to debug and test, and be more suited to formal verification.
Functional programming has its roots in academia, evolving from the lambda calculus, a formal system of computation based only on functions. Functional programming has historically been less popular than imperative programming, but many functional languages are seeing use today in industry and education, including Common Lisp, Scheme, Clojure, Wolfram Language, Racket, Erlang, OCaml, Haskell, and F#. Functional programming is also key to some languages that have found success in specific domains, like R in statistics, J, K and Q in financial analysis, and XQuery/XSLT for XML. Domain-specific declarative languages like SQL and Lex/Yacc use some elements of functional programming, such as not allowing mutable values. In addition, many other programming languages support programming in a functional style or have implemented features from functional programming, such as C++11, Kotlin, Perl, PHP, Python, and Scala.
The lambda calculus, developed in the 1930s by Alonzo Church, is a formal system of computation built from function application. In 1937 Alan Turing proved that the lambda calculus and Turing machines are equivalent models of computation, showing that the lambda calculus is Turing complete. Lambda calculus forms the basis of all functional programming languages. An equivalent theoretical formulation, combinatory logic, was developed by Moses Schönfinkel and Haskell Curry in the 1920s and 1930s.
Church later developed a weaker system, the simply-typed lambda calculus, which extended the lambda calculus by assigning a type to all terms. This forms the basis for statically-typed functional programming.
The first functional programming language, LISP, was developed in the late 1950s for the IBM 700/7000 series of scientific computers by John McCarthy while at Massachusetts Institute of Technology (MIT). LISP functions were defined using Church's lambda notation, extended with a label construct to allow recursive functions. Lisp first introduced many paradigmatic features of functional programming, though early Lisps were multi-paradigm languages, and incorporated support for numerous programming styles as new paradigms evolved. Later dialects, such as Scheme and Clojure, and offshoots such as Dylan and Julia, sought to simplify and rationalise Lisp around a cleanly functional core, while Common Lisp was designed to preserve and update the paradigmatic features of the numerous older dialects it replaced.
Information Processing Language (IPL), 1956, is sometimes cited as the first computer-based functional programming language. It is an assembly-style language for manipulating lists of symbols. It does have a notion of "generator", which amounts to a function that accepts a function as an argument, and, since it is an assembly-level language, code can be data, so IPL can be regarded as having higher-order functions. However, it relies heavily on mutating list structure and similar imperative features.
Kenneth E. Iverson developed APL in the early 1960s, described in his 1962 book "A Programming Language" (). APL was the primary influence on John Backus's FP. In the early 1990s, Iverson and Roger Hui created J. In the mid-1990s, Arthur Whitney, who had previously worked with Iverson, created K, which is used commercially in financial industries along with its descendant Q.
John Backus presented FP in his 1977 Turing Award lecture "Can Programming Be Liberated From the von Neumann Style? A Functional Style and its Algebra of Programs". He defines functional programs as being built up in a hierarchical way by means of "combining forms" that allow an "algebra of programs"; in modern language, this means that functional programs follow the principle of compositionality. Backus's paper popularized research into functional programming, though it emphasized function-level programming rather than the lambda-calculus style now associated with functional programming.
The 1973 language ML was created by Robin Milner at the University of Edinburgh, and David Turner developed the language SASL at the University of St Andrews. Also in Edinburgh in the 1970s, Burstall and Darlington developed the functional language NPL. NPL was based on Kleene Recursion Equations and was first introduced in their work on program transformation. Burstall, MacQueen and Sannella then incorporated the polymorphic type checking from ML to produce the language Hope. ML eventually developed into several dialects, the most common of which are now OCaml and Standard ML.
In the 1970s, Guy L. Steele and Gerald Jay Sussman developed Scheme, as described in the Lambda Papers and the 1985 textbook "Structure and Interpretation of Computer Programs". Scheme was the first dialect of lisp to use lexical scoping and to require tail-call optimization, features that encourage functional programming.
In the 1980s, Per Martin-Löf developed intuitionistic type theory (also called "constructive" type theory), which associated functional programs with constructive proofs expressed as dependent types. This led to new approaches to interactive theorem proving and has influenced the development of subsequent functional programming languages.
The lazy functional language, Miranda, developed by David Turner, initially appeared in 1985 and had a strong influence on Haskell. With Miranda being proprietary, Haskell began with a consensus in 1987 to form an open standard for functional programming research; implementation releases have been ongoing since 1990.
More recently it has found use in niches such as parametric CAD courtesy of the OpenSCAD language built on the CSG geometry framework, although its restriction on reassigning values (all values are treated as constants) has led to confusion among users who are unfamiliar with functional programming as a concept.
Functional programming continues to be used in commercial settings.
A number of concepts and paradigms are specific to functional programming, and generally foreign to imperative programming (including object-oriented programming). However, programming languages often cater to several programming paradigms, so programmers using "mostly imperative" languages may have utilized some of these concepts.
Higher-order functions are functions that can either take other functions as arguments or return them as results. In calculus, an example of a higher-order function is the differential operator formula_1, which returns the derivative of a function formula_2.
Higher-order functions are closely related to first-class functions in that higher-order functions and first-class functions both allow functions as arguments and results of other functions. The distinction between the two is subtle: "higher-order" describes a mathematical concept of functions that operate on other functions, while "first-class" is a computer science term for programming language entities that have no restriction on their use (thus first-class functions can appear anywhere in the program that other first-class entities like numbers can, including as arguments to other functions and as their return values).
Higher-order functions enable partial application or currying, a technique that applies a function to its arguments one at a time, with each application returning a new function that accepts the next argument. This lets a programmer succinctly express, for example, the successor function as the addition operator partially applied to the natural number one.
Pure functions (or expressions) have no side effects (memory or I/O). This means that pure functions have several useful properties, many of which can be used to optimize the code:
While most compilers for imperative programming languages detect pure functions and perform common-subexpression elimination for pure function calls, they cannot always do this for pre-compiled libraries, which generally do not expose this information, thus preventing optimizations that involve those external functions. Some compilers, such as gcc, add extra keywords for a programmer to explicitly mark external functions as pure, to enable such optimizations. Fortran 95 also lets functions be designated "pure". C++11 added codice_1 keyword with similar semantics.
Iteration (looping) in functional languages is usually accomplished via recursion. Recursive functions invoke themselves, letting an operation be repeated until it reaches the base case. In general, recursion requires maintaining a stack, which consumes space in a linear amount to the depth of recursion. This could make recursion prohibitively expensive to use instead of imperative loops. However, a special form of recursion known as tail recursion can be recognized and optimized by a compiler into the same code used to implement iteration in imperative languages. Tail recursion optimization can be implemented by transforming the program into continuation passing style during compiling, among other approaches.
The Scheme language standard requires implementations to support proper tail recursion, meaning they must allow an unbounded number of active tail calls. Proper tail recursion is not simply an optimization; it is a language feature that assures users that they can use recursion to express a loop and doing so would be safe-for-space. Moreover, on contrary to its name, it accounts for all tail calls, not just tail recursion. While proper tail recursion is usually implemented by turning code into imperative loops, implementations might implement it in other ways. For example, CHICKEN intentionally maintains a stack and lets the stack overflow. However, when this happens, its garbage collector will claim space back, allowing an unbounded number of active tail calls even though it does not turn tail recursion into a loop.
Common patterns of recursion can be abstracted away using higher-order functions, with catamorphisms and anamorphisms (or "folds" and "unfolds") being the most obvious examples. Such recursion schemes play a role analogous to built-in control structures such as loops in imperative languages.
Most general purpose functional programming languages allow unrestricted recursion and are Turing complete, which makes the halting problem undecidable, can cause unsoundness of equational reasoning, and generally requires the introduction of inconsistency into the logic expressed by the language's type system. Some special purpose languages such as Coq allow only well-founded recursion and are strongly normalizing (nonterminating computations can be expressed only with infinite streams of values called codata). As a consequence, these languages fail to be Turing complete and expressing certain functions in them is impossible, but they can still express a wide class of interesting computations while avoiding the problems introduced by unrestricted recursion. Functional programming limited to well-founded recursion with a few other constraints is called total functional programming.
Functional languages can be categorized by whether they use "strict (eager)" or "non-strict (lazy)" evaluation, concepts that refer to how function arguments are processed when an expression is being evaluated. The technical difference is in the denotational semantics of expressions containing failing or divergent computations. Under strict evaluation, the evaluation of any term containing a failing subterm fails. For example, the expression:
fails under strict evaluation because of the division by zero in the third element of the list. Under lazy evaluation, the length function returns the value 4 (i.e., the number of items in the list), since evaluating it does not attempt to evaluate the terms making up the list. In brief, strict evaluation always fully evaluates function arguments before invoking the function. Lazy evaluation does not evaluate function arguments unless their values are required to evaluate the function call itself.
The usual implementation strategy for lazy evaluation in functional languages is graph reduction. Lazy evaluation is used by default in several pure functional languages, including Miranda, Clean, and Haskell.
Especially since the development of Hindley–Milner type inference in the 1970s, functional programming languages have tended to use typed lambda calculus, rejecting all invalid programs at compilation time and risking false positive errors, as opposed to the untyped lambda calculus, that accepts all valid programs at compilation time and risks false negative errors, used in Lisp and its variants (such as Scheme), though they reject all invalid programs at runtime, when the information is enough to not reject valid programs. The use of algebraic datatypes makes manipulation of complex data structures convenient; the presence of strong compile-time type checking makes programs more reliable in absence of other reliability techniques like test-driven development, while type inference frees the programmer from the need to manually declare types to the compiler in most cases.
Some research-oriented functional languages such as Coq, Agda, Cayenne, and Epigram are based on intuitionistic type theory, which lets types depend on terms. Such types are called dependent types. These type systems do not have decidable type inference and are difficult to understand and program with. But dependent types can express arbitrary propositions in predicate logic. Through the Curry–Howard isomorphism, then, well-typed programs in these languages become a means of writing formal mathematical proofs from which a compiler can generate certified code. While these languages are mainly of interest in academic research (including in formalized mathematics), they have begun to be used in engineering as well. Compcert is a compiler for a subset of the C programming language that is written in Coq and formally verified.
A limited form of dependent types called generalized algebraic data types (GADT's) can be implemented in a way that provides some of the benefits of dependently typed programming while avoiding most of its inconvenience. GADT's are available in the Glasgow Haskell Compiler, in OCaml (since version 4.00) and in Scala (as "case classes"), and have been proposed as additions to other languages including Java and C#.
Functional programs do not have assignment statements, that is, the value of a variable in a functional program never changes once defined. This eliminates any chances of side effects because any variable can be replaced with its actual value at any point of execution. So, functional programs are referentially transparent.
Consider C assignment statement codice_2, this changes the value assigned to the variable codice_3. Let us say that the initial value of codice_3 was codice_5, then two consecutive evaluations of the variable codice_3 yields codice_7 and codice_8 respectively. Clearly, replacing codice_2 with either codice_7 or codice_8 gives a program with different meaning, and so the expression "is not" referentially transparent. In fact, assignment statements are never referentially transparent.
Now, consider another function such as int plusone(int x) {return x+1;} "is" transparent, as it does not implicitly change the input x and thus has no such side effects.
Functional programs exclusively use this type of function and are therefore referentially transparent.
Purely functional data structures are often represented in a different way than their imperative counterparts. For example, the array with constant access and update times is a basic component of most imperative languages, and many imperative data-structures, such as the hash table and binary heap, are based on arrays. Arrays can be replaced by maps or random access lists, which admit purely functional implementation, but have logarithmic access and update times. Purely functional data structures have persistence, a property of keeping previous versions of the data structure unmodified. In Clojure, persistent data structures are used as functional alternatives to their imperative counterparts. Persistent vectors, for example, use trees for partial updating. Calling the insert method will result in some but not all nodes being created.
Functional programming is very different from imperative programming. The most significant differences stem from the fact that functional programming avoids side effects, which are used in imperative programming to implement state and I/O. Pure functional programming completely prevents side-effects and provides referential transparency.
Higher-order functions are rarely used in older imperative programming. A traditional imperative program might use a loop to traverse and modify a list. A functional program, on the other hand, would probably use a higher-order “map” function that takes a function and a list, generating and returning a new list by applying the function to each list item.
The following two examples (written in JavaScript) achieve the same effect - they multiply all even numbers in an array by 10 and add them all, storing the final sum in the variable "result".
Traditional Imperative Loop:
const numList = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10];
let result = 0;
for (let i = 0; i < numList.length; i++) {
Functional Programming with higher-order functions:
const result = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
There are tasks (for example, maintaining a bank account balance) that often seem most naturally implemented with state. Pure functional programming performs these tasks, and I/O tasks such as accepting user input and printing to the screen, in a different way.
The pure functional programming language Haskell implements them using monads, derived from category theory. Monads offer a way to abstract certain types of computational patterns, including (but not limited to) modeling of computations with mutable state (and other side effects such as I/O) in an imperative manner without losing purity. While existing monads may be easy to apply in a program, given appropriate templates and examples, many students find them difficult to understand conceptually, e.g., when asked to define new monads (which is sometimes needed for certain types of libraries).
Functional languages also simulate states by passing around immutable states. This can be done by making a function accept the state as one of its parameters, and return a new state together with the result, leaving the old state unchanged.
Impure functional languages usually include a more direct method of managing mutable state. Clojure, for example, uses managed references that can be updated by applying pure functions to the current state. This kind of approach enables mutability while still promoting the use of pure functions as the preferred way to express computations.
Alternative methods such as Hoare logic and uniqueness have been developed to track side effects in programs. Some modern research languages use effect systems to make the presence of side effects explicit.
Functional programming languages are typically less efficient in their use of CPU and memory than imperative languages such as C and Pascal. This is related to the fact that some mutable data structures like arrays have a very straightforward implementation using present hardware. Flat arrays may be accessed very efficiently with deeply pipelined CPUs, prefetched efficiently through caches (with no complex pointer chasing), or handled with SIMD instructions. It is also not easy to create their equally efficient general-purpose immutable counterparts. For purely functional languages, the worst-case slowdown is logarithmic in the number of memory cells used, because mutable memory can be represented by a purely functional data structure with logarithmic access time (such as a balanced tree). However, such slowdowns are not universal. For programs that perform intensive numerical computations, functional languages such as OCaml and Clean are only slightly slower than C according to The Computer Language Benchmarks Game. For programs that handle large matrices and multidimensional databases, array functional languages (such as J and K) were designed with speed optimizations.
Immutability of data can in many cases lead to execution efficiency by allowing the compiler to make assumptions that are unsafe in an imperative language, thus increasing opportunities for inline expansion.
Lazy evaluation may also speed up the program, even asymptotically, whereas it may slow it down at most by a constant factor (however, it may introduce memory leaks if used improperly). Launchbury 1993 discusses theoretical issues related to memory leaks from lazy evaluation, and O'Sullivan "et al." 2008 give some practical advice for analyzing and fixing them.
However, the most general implementations of lazy evaluation making extensive use of dereferenced code and data perform poorly on modern processors with deep pipelines and multi-level caches (where a cache miss may cost hundreds of cycles) .
It is possible to use a functional style of programming in languages that are not traditionally considered functional languages. For example, both D and Fortran 95 explicitly support pure functions.
JavaScript, Lua and Python had first class functions from their inception. Python had support for "lambda", "map", "reduce", and "filter" in 1994, as well as closures in Python 2.2, though Python 3 relegated "reduce" to the codice_12 standard library module. First-class functions have been introduced into other mainstream languages such as PHP 5.3, Visual Basic 9, C# 3.0, C++11, and Kotlin.
In PHP, anonymous classes, closures and lambdas are fully supported. Libraries and language extensions for immutable data structures are being developed to aid programming in the functional style.
In Java, anonymous classes can sometimes be used to simulate closures; however, anonymous classes are not always proper replacements to closures because they have more limited capabilities. Java 8 supports lambda expressions as a replacement for some anonymous classes.
In C#, anonymous classes are not necessary, because closures and lambdas are fully supported. Libraries and language extensions for immutable data structures are being developed to aid programming in the functional style in C#.
Many object-oriented design patterns are expressible in functional programming terms: for example, the strategy pattern simply dictates use of a higher-order function, and the visitor pattern roughly corresponds to a catamorphism, or fold.
Similarly, the idea of immutable data from functional programming is often included in imperative programming languages, for example the tuple in Python, which is an immutable array.
Functional programming is an active area of research in the field of programming language theory. There are several peer-reviewed publication venues focusing on functional programming, including the International Conference on Functional Programming, the Journal of Functional Programming, and the Symposium on Trends in Functional Programming.
Functional programming has seen use in a wide variety of industrial applications. For example, Erlang, which was developed by the Swedish company Ericsson in the late 1980s, was originally used to implement fault-tolerant telecommunications systems, but has since become popular for building a range of applications at companies such as Nortel, Facebook, Électricité de France and WhatsApp. Scheme, a dialect of Lisp, was used as the basis for several applications on early Apple Macintosh computers, and has been applied to problems such as training simulation software and telescope control. OCaml, which was introduced in the mid-1990s, has seen commercial use in areas such as financial analysis, driver verification, industrial robot programming, and static analysis of embedded software. Haskell, though initially intended as a research language, has also been applied by a range of companies, in areas such as aerospace systems, hardware design, and web programming.
Other functional programming languages that have seen use in industry include Scala, F#, Wolfram Language, Lisp, Standard ML, and Clojure.
Functional "platforms" have been popular in finance for risk analytics (particularly with the larger investment banks). Risk factors are coded as functions that form interdependent graphs (categories) to measure correlations in market shifts not unlike Gröbner basis optimizations but also for regulatory compliance such as Comprehensive Capital Analysis and Review. Given the use of OCAML or CAML variations in finance, these systems are sometimes considered related to a categorical abstract machine or CAM. Indeed, functional programming is heavily influenced by category theory.
Many universities teach or have taught functional programming as part of their undergraduate Computer Science degrees. Some use it as their introduction to programming, while others teach it after teaching imperative programming. 
Outside of computer science, functional programming is being used as a method to teach problem solving, algebra and geometric concepts.
It has also been used as a tool to teach classical mechanics in "Structure and Interpretation of Classical Mechanics".

</doc>
<doc id="10936" url="https://en.wikipedia.org/wiki?curid=10936" title="February 29">
February 29

February 29, also known as leap day or leap year day, is a date added to most years that are divisible by 4, such as 2016, 2020, and 2024. A leap day is added in various solar calendars (calendars based on the Earth's revolution around the Sun), including the Gregorian calendar standard in most of the world. Lunisolar calendars (whose months are based on the phases of the Moon) instead add a leap or intercalary month.
In the Gregorian calendar, years that are divisible by 100, but not by 400, do not contain a leap day. Thus, 1700, 1800, and 1900 did not contain a leap day; neither will 2100, 2200, and 2300. Conversely, 1600 and 2000 did and 2400 will. Years containing a leap day are called leap years. Years not containing a leap day are called common years. In a leap year, February 29 is the 60th day of the Gregorian calendar, with 306 days remaining until the end of the year. In the Chinese calendar, this day will only occur in years of the monkey, dragon, and rat.
A leap day is observed because the Earth's period of orbital revolution around the Sun takes approximately six hours longer than 365 whole days. A leap day compensates for this lag, realigning the calendar with the Earth's position in the Solar System; otherwise, seasons would occur later than intended in the calendar year. The Julian calendar used in Christendom until the 16th century added a leap day every four years; but this rule adds too many days (roughly three every 400 years), making the equinoxes and solstices shift gradually to earlier dates. By the 16th century the vernal equinox had drifted to March 11, so the Gregorian calendar was introduced both to shift it back by omitting several days, and to reduce the number of leap years via the aforementioned century rule to keep the equinoxes more or less fixed and the date of Easter consistently close to the vernal equinox.
Leap days can present a particular problem in computing known as the leap year bug when February 29 is not handled correctly in logic that accepts or manipulates dates. For example, this has happened with ATMs and Microsoft's cloud system Azure.
Although most modern calendar years have 365 days, a complete revolution around the Sun (one solar year) takes approximately 365 days, 5 hours, 48 minutes, and 46 seconds (or, for simplicity's sake, approximately 365 days and 6 hours, or 365.25 days). An extra 23 hours, 15 minutes, and 4 seconds thus accumulates every four years (again, for simplicity's sake, approximately an extra 24 hours, or 1 day, every four years), requiring that an extra calendar day be added to align the calendar with the Sun's apparent position. Without the added day, in future years the seasons would occur later in the calendar, eventually leading to confusion about when to undertake activities dependent on weather, ecology, or hours of daylight.
Solar years are actually slightly shorter than 365 days and 6 hours (365.25 days), which had been known since the 2nd century BC when Hipparchus stated that it lasted 365 + − days, but this was ignored by Julius Caesar and his astronomical adviser Sosigenes. The Gregorian calendar corrected this by adopting the length of the tropical year stated in three medieval sources, the Alfonsine tables, De Revolutionibus, and the Prutenic Tables, truncated to two sexagesimal places, 365 days or 365 + − days or 365.2425 days. The length of the tropical year in 2000 was 365.24217 mean solar days, Adding a calendar day every four years, therefore, results in an excess of around 44 minutes every four years, or about 3 days every 400 years. To compensate for this, three days are removed every 400 years. The Gregorian calendar reform implements this adjustment by making an exception to the general rule that there is a leap year every four years. Instead, a year divisible by 100 is not a leap year unless that year is also divisible by 400. This means that the years 1600, 2000, and 2400 are leap years, while the years 1700, 1800, 1900, 2100, 2200, 2300, and 2500 are not leap years.
The Gregorian calendar repeats itself every 400 years, which is exactly 20,871 weeks including 97 leap days (146,097 days). Over this period, February 29 falls on Sunday, Tuesday, and Thursday 13 times; Friday and Saturday 14 times; and Monday and Wednesday 15 times. Except for a century mark that is not a multiple of 400, consecutive leap days fall in order Sunday, Friday, Wednesday, Monday, Saturday, Thursday, Tuesday, and repeats again.
The calendar of the Roman king Numa Pompilius had only 355 days (even though it was not a lunar calendar) which meant that it would quickly become unsynchronized with the solar year. An earlier Roman solution to this problem was to lengthen the calendar periodically by adding extra days to February, the last month of the year. February consisted of two parts, each with an odd number of days. The first part ended with the "Terminalia" on the 23rd, which was considered the end of the religious year, and the five remaining days formed the second part. To keep the calendar year roughly aligned with the solar year, a leap month, called "Mensis Intercalaris" ("intercalary month"), was added from time to time between these two parts of February. The (usual) second part of February was incorporated in the intercalary month as its last five days, with no change either in their dates or the festivals observed on them. This followed naturally, because the days after the Ides (13th) of February (in an ordinary year) or the Ides of Intercalaris (in an intercalary year) both counted down to the Kalends of March (i.e. they were known as "the "n"th day before the Kalends of March"). The Nones (5th) and Ides of Intercalaris occupied their normal positions.
The third-century writer Censorinus says:
The set leap day was introduced in Rome as a part of the Julian reform in the 1st century BCE. As before, the intercalation was made after February 23. The day following the Terminalia (February 23) was doubled, forming the "bis sextum"—literally 'twice sixth', since February 24 was 'the sixth day before the Kalends of March' using Roman inclusive counting (March 1 was the Kalends of March and was also the first day of the calendar year). Inclusive counting initially caused the Roman priests to add the extra day every three years instead of four; Augustus was compelled to omit leap years for a few decades to return the calendar to its proper position. Although there were exceptions, the first day of the "bis sextum" (February 24) was usually regarded as the intercalated or "bissextile" day since the 3rd century CE. February 29 came to be regarded as the leap day when the Roman system of numbering days was replaced by sequential numbering in the late Middle Ages, although this has only been formally enacted in Sweden and Finland. In Britain, the extra day added to leap years remains notionally the 24th, although the 29th remains more visible on the calendar.
A person born on February 29 may be called a "leapling", a "leaper", or a "leap-year baby". Some leaplings celebrate their birthday in non-leap years on either February 28 or March 1, while others only observe birthdays on the authentic intercalary date, February 29.
The effective legal date of a leapling's birthday in non-leap years varies between jurisdictions.
In the United Kingdom and its former colony Hong Kong, when a person born on February 29 turns 18, they are considered to have their birthday on March 1 in the relevant year.
In New Zealand, a person born on February 29 is deemed to have their birthday on February 28 in non-leap years, for the purposes of Driver Licensing under §2(2) of the Land Transport (Driver Licensing) Rule 1999. The net result is that for drivers aged 75, or over 80, their driver licence expires at the end of the last day of February, even though their birthday would otherwise fall on the first day in March in non-leap years. Otherwise, New Zealand legislation is silent on when a person born on February 29 has their birthday, although case law would suggest that age is computed based on the number of years elapsed, from the day after the date of birth, and that the person's birth day then occurs on the last day of the year period. This differs from English common law where a birthday is considered to be the start of the next year, the preceding year ending at midnight on the day preceding the birthday. While a person attains the same age on the same day, it also means that, in New Zealand, if something must be done by the time a person attains a certain age, that thing can be done on the birthday that they attain that age and still be lawful.
In Taiwan, the legal birthday of a leapling is February 28 in common years:
Thus, in England and Wales or in Hong Kong, a person born on February 29 will have legally reached 18 years old on March 1. If they were born in Taiwan they legally become 18 on February 28, a day earlier. 
In the United States, according to John Reitz, a professor of law at the University of Iowa, there is no "... statute or general rule that has anything to do with leap day." Reitz speculates that "March 1 would likely be considered the legal birthday in non-leap years of someone born on leap day," using the same reasoning as described for the United Kingdom and Hong Kong. However, for the purposes of Social Security, a person attains the next age the day before the anniversary of birth. Therefore, Social Security would recognize February 28 as the change in age for leap year births, not March 1.
There are many instances in children's literature where a person's claim to be only a quarter of their actual age turns out to be based on counting only their leap-year birthdays.
A similar device is used in the plot of Gilbert and Sullivan's 1879 comic opera "The Pirates of Penzance": as a child, Frederic was apprenticed to a band of pirates until his 21st birthday. Having passed his 21st year, he leaves the pirate band and falls in love. However, since he was born on February 29, his 21st "birthday" will not arrive until he is eighty-eight (since 1900 was not a leap year), so he must leave his fiancée and return to the pirates.
Since 1967, February 29 has been the official birthday of Superman, but not Clark Kent.
There is a popular tradition known as Bachelor's Day in some countries allowing a woman to propose marriage to a man on February 29. If the man refuses, he then is obliged to give the woman money or buy her a dress. In upper-class societies in Europe, if the man refuses marriage, he then must purchase 12 pairs of gloves for the woman, suggesting that the gloves are to hide the woman's embarrassment of not having an engagement ring. In Ireland, the tradition is supposed to originate from a deal that Saint Bridget struck with Saint Patrick.
In the town of Aurora, Illinois, single women are deputized and may arrest single men, subject to a four-dollar fine, every February 29.
In Greece, it is considered unlucky to marry on a leap day.

</doc>
<doc id="10937" url="https://en.wikipedia.org/wiki?curid=10937" title="Francis Scott Key">
Francis Scott Key

Francis Scott Key (August 1, 1779January 11, 1843) was an American lawyer, author, and amateur poet from Frederick, Maryland, who is best known for writing the lyrics for the American national anthem "The Star-Spangled Banner".
Key observed the British bombardment of Fort McHenry in 1814 during the War of 1812. He was inspired upon seeing the American flag still flying over the fort at dawn and wrote the poem "Defence of Fort M'Henry"; it was published within a week with the suggested tune of the popular song "To Anacreon in Heaven". The song with Key's lyrics became known as "The Star-Spangled Banner" and slowly gained in popularity as an unofficial anthem, finally achieving official status more than a century later under President Herbert Hoover as the national anthem. The national motto "In God We Trust" derives from a line in "The Star-Spangled Banner".
Key was a lawyer in Maryland and Washington D.C. for four decades and worked on important cases, including the Burr conspiracy trial, and he argued numerous times before the Supreme Court. He was nominated for District Attorney for the District of Columbia by President Andrew Jackson, where he served from 1833 to 1841. Key was a devout Episcopalian.
Key owned slaves from 1800, during which time abolitionists ridiculed his words, claiming that America was more like the "Land of the Free and Home of the Oppressed". As District Attorney, he suppressed abolitionists and did not support an immediate end to slavery. He was also a leader of the American Colonization Society which sent freed slaves to Africa. He freed some of his slaves in the 1830s, paying one ex-slave as his farm foreman. He publicly criticized slavery and gave free legal representation to some slaves seeking freedom, but he also represented owners of runaway slaves.
Key's father John Ross Key was a lawyer, a commissioned officer in the Continental Army, and a judge of English descent. His mother Ann Phoebe Dagworthy Charlton was born (February 6, 1756-1830), to Arthur Charlton, a tavern keeper, and his wife, Eleanor Harrison of Frederick in the colony of Maryland.
Key grew up on the family plantation Terra Rubra in Frederick County, Maryland (now Carroll County). He graduated from St.John's College, Annapolis, Maryland, in 1796 and read law under his uncle Philip Barton Key who was loyal to the British Crown during the War of Independence. He married Mary Tayloe Lloyd on January 1, 1802.
During the War of 1812, Key and British Prisoner Exchange Agent Colonel John Stuart Skinner dined aboard as the guests of Vice Admiral Alexander Cochrane, Rear Admiral George Cockburn, and Major General Robert Ross. Skinner and Key were there to negotiate the release of prisoners, one of whom was William Beanes, a resident of Upper Marlboro, Maryland, who had been arrested after jailing British troops who were taking food from local farms. Skinner, Key, and Beanes were not allowed to return to their own sloop because they had become familiar with the strength and position of the British units and their intention to launch an attack upon Baltimore, and Key was unable to do anything but watch the bombarding of the American forces at Fort McHenry during the Battle of Baltimore on the night of September 1314,1814.
At dawn, Key was able to see an American flag waving, and he later wrote a poem about his experience entitled "Defence of Fort M'Henry" which was published in William Pechin's "American and Commercial Daily Advertiser" on September 21, 1814. He took it to music publisher Thomas Carr, who adapted it to the rhythms of composer John Stafford Smith's "To Anacreon in Heaven", a popular tune that Key had already used as a setting for his 1805 song "When the Warrior Returns", celebrating American heroes of the First Barbary War. It was somewhat difficult to sing, yet it became increasingly popular, competing with "Hail, Columbia" (1796) as the de facto national anthem by the time of the Mexican–American War and the American Civil War. The song was finally adopted as the American national anthem more than a century after its first publication, first by an Executive Order from President Woodrow Wilson in1916, and then by a Congressional resolution in1931 signed by President Herbert Hoover.
The third stanza of the Star-Spangled Banner makes disparaging mention of blacks and demonstrates Key's opinion of their seeking freedom at the time by escaping to the British, who promised them freedom from American enslavement.
Key was a leading attorney in Frederick, Maryland, and Washington, D.C., for many years, with an extensive real estate and trial practice. He and his family settled in Georgetown in 1805 or 1806, near the new national capital. He assisted his uncle Philip Barton Key in the sensational conspiracy trial of Aaron Burr and in the expulsion of Senator John Smith of Ohio. He made the first of his many arguments before the United States Supreme Court in 1807. In 1808, he assisted President Thomas Jefferson's attorney general in "United Statesv.Peters".
In 1829, Key assisted in the prosecution of Tobias Watkins, former U.S. Treasury auditor under President John Quincy Adams, for misappropriating public funds. He also handled the Petticoat affair concerning Secretary of War John Eaton, and he served as the attorney for Sam Houston in 1832 during his trial for assaulting Representative William Stanbery of Ohio. After years as an adviser to President Jackson, Key was nominated by the President to District Attorney for the District of Columbia in 1833. He served from 1833 to 1841 while also handling his own private legal cases. In 1835, he prosecuted Richard Lawrence for his attempt to assassinate President Jackson at the top steps of the Capitol, the first attempt to kill an American president.
Key purchased his first slave in 1800 or 1801 and owned six slaves in 1820. He freed seven slaves in the 1830s, one of whom continued to work for him for wages as his farm's foreman, supervising several slaves. Key also represented several slaves seeking their freedom, as well as several slave-owners seeking return of their runaway slaves. Key was one of the executors of John Randolph of Roanoke's will, which freed his 400 slaves, and Key fought to enforce the will for the next decade and to provide the freed slaves with land to support themselves.
Key is known to have publicly criticized slavery's cruelties, and a newspaper editorial stated that "he often volunteered to defend the downtrodden sons and daughters of Africa." The editor said that Key "convinced me that slavery was wrong—radically wrong". 
A quote increasingly credited to Key stating that free blacks are "a distinct and inferior race of people, which all experience proves to be the greatest evil that afflicts a community" is erroneous. The quote is taken from an 1838 letter that Key wrote to Reverend Benjamin Tappan of Maine who had sent Key a questionnaire about the attitudes of Southern religious institutions about slavery. Rather than representing a statement by Key identifying his personal thoughts, the words quoted are offered by Key to describe the attitudes of others who assert that formerly enslaved blacks could not remain in the U.S. as paid laborers. This was the official policy of the American Colonization Society. Key was an ACS leader and fundraiser for the organization, but he himself did not send the men and women he freed to Africa upon their emancipation. The original confusion around this quote arises from ambiguities in the 1937 biography of Key by Edward S. Delaplaine.
Key was a founding member and active leader of the American Colonization Society (ACS), whose primary goal was to send free blacks to Africa. Though many free blacks were born in the United States by this time, historians argue that upper-class American society, of which Key was a part, could never "envision a multiracial society". The ACS was not supported by most abolitionists or free blacks of the time, but the organization's work would eventually lead to the creation of Liberia in 1847.
In the early 1830s American thinking on slavery changed quite abruptly. Considerable opposition to the American Colonization Society's project emerged. Led by newspaper editor and publisher Wm. Lloyd Garrison, a growing portion of the population noted that only a very small number of free blacks were actually moved, and they faced brutal conditions in West Africa, with very high mortality. Free blacks made it clear that few of them wanted to move, and if they did, it would be to Canada, Mexico, or Central America, not Africa. The leaders of the American Colonization Society, including Key, were predominantly slaveowners. The Society was intended to preserve slavery, rather than eliminate it. In the words of philanthropist Gerrit Smith, it was "quite as much an Anti-Abolition, as Colonization Society". "This Colonization Society had, by an invisible process, half conscious, half unconscious, been transformed into a serviceable organ and member of the Slave Power."
The alternative to the colonization of Africa, project of the American Colonization Society, was the total and immediate abolition of slavery in the United States. This Key was firmly against, with or without slaveowner compensation, and he used his position as District Attorney to attack abolitionists. In 1833, he secured a grand jury indictment against Benjamin Lundy, editor of the anti-slavery publication "Genius of Universal Emancipation", and his printer William Greer, for libel after Lundy published an article that declared, "There is neither mercy nor justice for colored people in this district [of Columbia]". Lundy's article, Key said in the indictment, "was intended to injure, oppress, aggrieve, and vilify the good name, fame, credit & reputation of the Magistrates and constables" of Washington. Lundy left town rather than face trial; Greer was acquitted.
In a larger unsuccessful prosecution, in August 1836 Key obtained an indictment against Reuben Crandall, brother of controversial Connecticut teacher Prudence Crandall, who had recently moved to Washington, D.C. It accused Crandall of "seditious libel" after two marshals (who operated as slave catchers in their off hours) found Crandall had a trunk full of anti-slavery publications in his Georgetown residence/office, five days after the Snow riot, caused by rumors that a mentally ill slave had attempted to kill an elderly white woman. In an April 1837 trial that attracted nationwide attention and that congressmen attended, Key charged that Crandall's publications instigated slaves to rebel. Crandall's attorneys acknowledged he opposed slavery, but denied any intent or actions to encourage rebellion. Evidence was introduced that the anti-slavery publications were packing materials used by his landlady in shipping his possessions to him. He had not "published" anything; he had given one copy to one man who had asked for it.
Key, in his final address to the jury said:
The jury acquitted Crandall. This public and humiliating defeat, as well as family tragedies in 1835, diminished Key's political ambition. He resigned as District Attorney in 1840. He remained a staunch proponent of African colonization and a strong critic of the abolition movement until his death.
Crandall died shortly after his acquittal of pneumonia contracted in the Washington jail.
Key was a devout and prominent Episcopalian. In his youth, he almost became an Episcopal priest rather than a lawyer. Throughout his life he sprinkled biblical references in his correspondence. He was active in All Saints Parish in Frederick, Maryland, near his family's home. He also helped found or financially support several parishes in the new national capital, including St. John's Episcopal Church in Georgetown and Christ Church in Alexandria (at the time, in the District of Columbia).
From 1818 until his death in 1843, Key was associated with the American Bible Society. He successfully opposed an abolitionist resolution presented to that group around 1838.
Key also helped found two Episcopal seminaries, one in Baltimore and the other across the Potomac River in Alexandria (the Virginia Theological Seminary). Key also published a prose work called "The Power of Literature, and Its Connection with Religion" in 1834.
The US national motto "In God We Trust" was adapted from a phrase in Key's "Star-Spangled Banner", the fourth stanza of which includes the phrase, "And this be our motto: 'In God is our Trust'", leading some to speculate that the phrase was derived from the song.
On January 11, 1843, Key died at the home of his daughter Elizabeth Howard in Baltimore from pleurisy at age 63. He was initially interred in Old Saint Paul's Cemetery in the vault of John Eager Howard but in 1866, his body was moved to his family plot in Frederick at Mount Olivet Cemetery.
The Key Monument Association erected a memorial in 1898 and the remains of both Francis Scott Key and his wife, Mary Tayloe Lloyd, were placed in a crypt in the base of the monument.
Despite several efforts to preserve it, the Francis Scott Key residence was ultimately dismantled in1947. The residence had been located at 351618MStreet in Georgetown.
Though Key had written poetry from time to time, often with heavily religious themes, these works were not collected and published until 14years after his death. Two of his religious poems used as Christian hymns include "Before the Lord We Bow" and "Lord, with Glowing Heart I'd Praise Thee".
In1806, Key's sister, Anne Phoebe Charlton Key, married Roger B. Taney, who would later become Chief Justice of the United States. In 1846 one daughter, Alice, married U.S. Senator George H. Pendleton and another, Ellen Lloyd, married Simon F. Blunt. In1859, Key's son Philip Barton Key II, who also served as United States Attorney for the District of Columbia, was shot and killed by Daniel Sicklesa U.S.Representative from New York who would serve as a general in the American Civil Warafter he discovered that Philip Barton Key was having an affair with his wife. Sickles was acquitted in the first use of the temporary insanity defense. In1861, Key's grandson Francis Key Howard was imprisoned in Fort McHenry with the Mayor of Baltimore George William Brown and other locals deemed to be Confederate sympathizers.
Key was a distant cousin and the namesake of F. Scott Fitzgerald, whose full name was Francis Scott Key Fitzgerald. His direct descendants include geneticist Thomas Hunt Morgan, guitarist Dana Key, and American fashion designer and socialite Pauline de Rothschild.

</doc>
<doc id="10938" url="https://en.wikipedia.org/wiki?curid=10938" title="FSU">
FSU

FSU may refer to:

</doc>
<doc id="10939" url="https://en.wikipedia.org/wiki?curid=10939" title="Formal language">
Formal language

In mathematics, computer science, and linguistics, a formal language consists of words whose letters are taken from an alphabet and are well-formed according to a specific set of rules.
The alphabet of a formal language consist of symbols, letters, or tokens that concatenate into strings of the language. Each string concatenated from symbols of this alphabet is called a word, and the words that belong to a particular formal language are sometimes called "well-formed words" or "well-formed formulas". A formal language is often defined by means of a formal grammar such as a regular grammar or context-free grammar, which consists of its formation rules.
The field of formal language theory studies primarily the purely syntactical aspects of such languages—that is, their internal structural patterns. Formal language theory sprang out of linguistics, as a way of understanding the syntactic regularities of natural languages.
In computer science, formal languages are used among others as the basis for defining the grammar of programming languages and formalized versions of subsets of natural languages in which the words of the language represent concepts that are associated with particular meanings or semantics. In computational complexity theory, decision problems are typically defined as formal languages, and complexity classes are defined as the sets of the formal languages that can be parsed by machines with limited computational power. In logic and the foundations of mathematics, formal languages are used to represent the syntax of axiomatic systems, and mathematical formalism is the philosophy that all of mathematics can be reduced to the syntactic manipulation of formal languages in this way.
The first formal language is thought to be the one used by Gottlob Frege in his "Begriffsschrift" (1879), literally meaning "concept writing", and which Frege described as a "formal language of pure thought."
Axel Thue's early semi-Thue system, which can be used for rewriting strings, was influential on formal grammars.
An alphabet, in the context of formal languages, can be any set, although it often makes sense to use an alphabet in the usual sense of the word, or more generally a character set such as ASCII or Unicode. The elements of an alphabet are called its letters. An alphabet may contain an infinite number of elements; however, most definitions in formal language theory specify alphabets with a finite number of elements, and most results apply only to them.
A word over an alphabet can be any finite sequence (i.e., string) of letters. The set of all words over an alphabet Σ is usually denoted by Σ (using the Kleene star). The length of a word is the number of letters it is composed of. For any alphabet, there is only one word of length 0, the "empty word", which is often denoted by e, ε, λ or even Λ. By concatenation one can combine two words to form a new word, whose length is the sum of the lengths of the original words. The result of concatenating a word with the empty word is the original word.
In some applications, especially in logic, the alphabet is also known as the "vocabulary" and words are known as "formulas" or "sentences"; this breaks the letter/word metaphor and replaces it by a word/sentence metaphor.
A formal language "L" over an alphabet Σ is a subset of Σ, that is, a set of words over that alphabet. Sometimes the sets of words are grouped into expressions, whereas rules and constraints may be formulated for the creation of 'well-formed expressions'.
In computer science and mathematics, which do not usually deal with natural languages, the adjective "formal" is often omitted as redundant.
While formal language theory usually concerns itself with formal languages that are described by some syntactical rules, the actual definition of the concept "formal language" is only as above: a (possibly infinite) set of finite-length strings composed from a given alphabet, no more and no less. In practice, there are many languages that can be described by rules, such as regular languages or context-free languages. The notion of a formal grammar may be closer to the intuitive concept of a "language," one described by syntactic rules. By an abuse of the definition, a particular formal language is often thought of as being equipped with a formal grammar that describes it.
The following rules describe a formal language  over the alphabet Σ = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, +, =}:
Under these rules, the string "23+4=555" is in , but the string "=234=+" is not. This formal language expresses natural numbers, well-formed additions, and well-formed addition equalities, but it expresses only what they look like (their syntax), not what they mean (semantics). For instance, nowhere in these rules is there any indication that "0" means the number zero, "+" means addition, "23+4=555" is false, etc.
For finite languages, one can explicitly enumerate all well-formed words. For example, we can describe a language  as just  = {a, b, ab, cba}. The degenerate case of this construction is the empty language, which contains no words at all ( = ∅).
However, even over a finite (non-empty) alphabet such as Σ = {a, b} there are an infinite number of finite-length words that can potentially be expressed: "a", "abb", "ababba", "aaababbbbaab", ... Therefore, formal languages are typically infinite, and describing an infinite formal language is not as simple as writing "L" = {a, b, ab, cba}. Here are some examples of formal languages:
Formal languages are used as tools in multiple disciplines. However, formal language theory rarely concerns itself with particular languages (except as examples), but is mainly concerned with the study of various types of formalisms to describe languages. For instance, a language can be given as
Typical questions asked about such formalisms include:
Surprisingly often, the answer to these decision problems is "it cannot be done at all", or "it is extremely expensive" (with a characterization of how expensive). Therefore, formal language theory is a major application area of computability theory and complexity theory. Formal languages may be classified in the Chomsky hierarchy based on the expressive power of their generative grammar as well as the complexity of their recognizing automaton. Context-free grammars and regular grammars provide a good compromise between expressivity and ease of parsing, and are widely used in practical applications.
Certain operations on languages are common. This includes the standard set operations, such as union, intersection, and complement. Another class of operation is the element-wise application of string operations.
Examples: suppose formula_1 and formula_2 are languages over some common alphabet formula_3.
Such string operations are used to investigate closure properties of classes of languages. A class of languages is closed under a particular operation when the operation, applied to languages in the class, always produces a language in the same class again. For instance, the context-free languages are known to be closed under union, concatenation, and intersection with regular languages, but not closed under intersection or complement. The theory of trios and abstract families of languages studies the most common closure properties of language families in their own right.
A compiler usually has two distinct components. A lexical analyzer, sometimes generated by a tool like codice_1, identifies the tokens of the programming language grammar, e.g. identifiers or keywords, numeric and string literals, punctuation and operator symbols, which are themselves specified by a simpler formal language, usually by means of regular expressions. At the most basic conceptual level, a parser, sometimes generated by a parser generator like codice_2, attempts to decide if the source program is syntactically valid, that is if it is well formed with respect to the programming language grammar for which the compiler was built.
Of course, compilers do more than just parse the source code – they usually translate it into some executable format. Because of this, a parser usually outputs more than a yes/no answer, typically an abstract syntax tree. This is used by subsequent stages of the compiler to eventually generate an executable containing machine code that runs directly on the hardware, or some intermediate code that requires a virtual machine to execute.
In mathematical logic, a "formal theory" is a set of sentences expressed in a formal language.
A "formal system" (also called a "logical calculus", or a "logical system") consists of a formal language together with a deductive apparatus (also called a "deductive system"). The deductive apparatus may consist of a set of transformation rules, which may be interpreted as valid rules of inference, or a set of axioms, or have both. A formal system is used to derive one expression from one or more other expressions. Although a formal language can be identified with its formulas, a formal system cannot be likewise identified by its theorems. Two formal systems formula_24 and formula_25 may have all the same theorems and yet differ in some significant proof-theoretic way (a formula A may be a syntactic consequence of a formula B in one but not another for instance).
A "formal proof" or "derivation" is a finite sequence of well-formed formulas (which may be interpreted as sentences, or propositions) each of which is an axiom or follows from the preceding formulas in the sequence by a rule of inference. The last sentence in the sequence is a theorem of a formal system. Formal proofs are useful because their theorems can be interpreted as true propositions.
Formal languages are entirely syntactic in nature but may be given semantics that give meaning to the elements of the language. For instance, in mathematical logic, the set of possible formulas of a particular logic is a formal language, and an interpretation assigns a meaning to each of the formulas—usually, a truth value.
The study of interpretations of formal languages is called formal semantics. In mathematical logic, this is often done in terms of model theory. In model theory, the terms that occur in a formula are interpreted as objects within mathematical structures, and fixed compositional interpretation rules determine how the truth value of the formula can be derived from the interpretation of its terms; a "model" for a formula is an interpretation of terms such that the formula becomes true.

</doc>
<doc id="10940" url="https://en.wikipedia.org/wiki?curid=10940" title="Free to Choose">
Free to Choose

Free to Choose: A Personal Statement (1980) is a book by economists Milton and Rose D. Friedman, accompanied by a ten-part series broadcast on public television, that advocates free market principles. It was primarily a response to an earlier landmark book and television series "The Age of Uncertainty", by the noted economist John Kenneth Galbraith. Milton Friedman won the Nobel Memorial Prize in Economics in 1976.
"Free to Choose: A Personal Statement" maintains that the free market works best for all members of a society, provides examples of how the free market engenders prosperity, and maintains that it can solve problems where other approaches have failed. Published in January 1980, the 297 page book contains 10 chapters. The book was on the United States best sellers list for 5 weeks.
PBS broadcast the programs, beginning in January 1980. It was filmed at the invitation of Robert Chitester, the owner of WQLN-TV. It was based on a 15-part series of taped public lectures and question-and-answer sessions. The general format was that of Milton Friedman visiting and narrating a number of success and failure stories in history, which he attributes to capitalism or the lack thereof (e.g. Hong Kong is commended for its free markets, while India is excoriated for relying on centralized planning especially for its protection of its traditional textile industry). Following the primary show, Friedman would engage in discussion moderated by Robert McKenzie with a number of selected debaters drawn from trade unions, academy and the business community, such as Donald Rumsfeld (then of G.D. Searle & Company) and Frances Fox Piven of City University of New York. The interlocutors would offer objections to or support for the proposals put forward by Friedman, who would in turn respond. After the final episode, Friedman sat down for an interview with Lawrence Spivak.
The series was rebroadcast in 1990 with Linda Chavez moderating the episodes. Arnold Schwarzenegger, Ronald Reagan, Steve Allen and George Shultz give personal introductions for each episode. This time, after the documentary part, Friedman sits down with a single opponent to debate the issues raised in the episode.
Guest debaters included:
The Friedmans advocate "laissez-faire" economic policies, often criticizing interventionist government policies and their cost in personal freedoms and economic efficiency in the United States and abroad. They argue that international free trade has been restricted through tariffs and protectionism while domestic free trade and freedom have been limited through high taxation and regulation. They cite the 19th-century United Kingdom, the United States before the Great Depression, and modern Hong Kong as ideal examples of a minimalist economic policy. They contrast the economic growth of Japan after the Meiji Restoration and the economic stagnation of India after its independence from the British Empire, and argue that India has performed worse despite its superior economic potential due to its centralized planning. They argue that even countries with command economies, including the Soviet Union and Yugoslavia, have been forced to adopt limited market mechanisms in order to operate. The authors argue against government taxation on gas and tobacco and government regulation of the public school systems. The Friedmans argue that the Federal Reserve exacerbated the Great Depression by neglecting to prevent the decline of the money supply in the years leading up to it. They further argue that the American public falsely perceived the Depression to be a result of a failure of capitalism rather than the government, and that the Depression allowed the Federal Reserve Board to centralize its control of the monetary system despite its responsibility for it. 
On the subject of welfare, the Friedmans argue that the United States has maintained a higher degree of freedom and productivity by avoiding the nationalizations and extensive welfare systems of Western European countries such as the United Kingdom and Sweden. However, they also argue that welfare practices since the New Deal under "the HEW empire" have been harmful. They argue that public assistance programs have become larger than originally envisioned and are creating "wards of the state" as opposed to "self-reliant individuals." They also argue that the Social Security System is fundamentally flawed, that urban renewal and public housing programs have contributed to racial inequality and diminished quality of low-income housing, and that Medicare and Medicaid are responsible for rising healthcare prices in the United States. They suggest completely replacing the welfare state with a negative income tax as a less harmful alternative. 
The Friedmans also argue that declining academic performance in the United States is the result of increasing government control of the American education system tracing back to the 1840s, but suggest a voucher system as a politically feasible solution. They blame the 1970s recession and lower quality of consumer goods on extensive business regulations since the 1960s, and advocate abolishing the Food and Drug Administration, the Interstate Commerce Commission, the Consumer Product Safety Commission, Amtrak, and Conrail. They argue that the energy crisis would be resolved by abolishing the Department of Energy and price floors on crude oil. They recommend replacing the Environmental Protection Agency and environmental regulation with an effluent charge. They criticize labor unions for raising prices and lowering demand by enforcing high wage levels, and for contributing to unemployment by limiting jobs. They argue that inflation is caused by excessive government spending, the Federal Reserve's attempts to control interest rates, and full employment policy. They call for tighter control of Fed money supply despite the fact that it will result in a temporary period of high unemployment and low growth due to the interruption of the wage-price spiral. In the final chapter, they take note of recent current events that seem to suggest a return to free-market principles in academic thought and public opinion, and argue in favor of an "economic Bill of Rights" to cement the changes. 

</doc>
<doc id="10945" url="https://en.wikipedia.org/wiki?curid=10945" title="Albert Park Circuit">
Albert Park Circuit

The Albert Park Circuit is a motorsport street circuit around Albert Park Lake, three kilometres south of central Melbourne. It is used annually as a circuit for the traditional Formula One season opening Australian Grand Prix, the supporting Supercars Championship Melbourne 400 and other associated support races. The circuit has an FIA Grade 1 licence. Although the entire track consists of normally public roads, each sector includes medium to high speed characteristics more commonly associated with dedicated racetracks facilitated by grass and gravel run-off safety zones that are reconstructed annually. However, the circuit also has characteristics of a street circuit's enclosed nature due to concrete barriers annually built along the Lakeside Drive curve, in particular, where run-off is not available due to the proximity of the lake shore.
The circuit uses everyday sections of road that circle Albert Park Lake, a small man-altered lake (originally a large lagoon formed as part of the ancient Yarra River course) just south of the Central Business District of Melbourne. The road sections that are used were rebuilt prior to the inaugural event in 1996 to ensure consistency and smoothness. As a result, compared to other circuits that are held on public roads, the Albert Park track has quite a smooth surface. Before 2007 there existed only a few other places on the Formula 1 calendar with a body of water close to the track. Many of the new tracks, such as Valencia, Singapore and Abu Dhabi are close to a body of water.
The course is considered to be quite fast and relatively easy to drive, drivers having commented that the consistent placement of corners allows them to easily learn the circuit and achieve competitive times. However, the flat terrain around the lake, coupled with a track design that features few true straights, means that the track is not conducive to overtaking or easy spectating unless in possession of a grandstand seat.
Each year, most of the trackside fencing, pedestrian overpasses, grandstands and other motorsport infrastructure are erected approximately two months prior to the Grand Prix weekend and removed within 6 weeks after the event. Land around the circuit (including a large aquatic centre, a golf course, a Lakeside Stadium, some restaurants and rowing boathouses) has restricted access during that entire period. Dissent is still prevalent among nearby local residents and users of those others facilities, and some still maintain a silent protest against the event. Nevertheless, the event is reasonably popular in Melbourne and Australia (with a large European population and a general interest in motorsport). Middle Park, the home of South Melbourne FC was demolished in 1994 due to expansion at Albert Park. 
On 4 July 2008, F1 announced that more than 300,000 people attended the four-day Melbourne Grand Prix, though actual ticket sales were later disputed by the local media. There has never been a night race at Albert Park, however, the 2009 and 2010 events both started at 5:00 p.m. local time. The current contract for the Grand Prix at the circuit concludes in 2025.
Starting on the Walker Straight, Turn 1 is a tight right-hander, followed by a quick flick to the left in Turn 2. A short straight follows, in which the cars accelerate up to 300 km/h, before Turn 3 is taken at a third of that speed. Two successive short straights lead to the left and right-handers of Turns 4 and 5, and the end of Sector 1. The next straight lasts for half a kilometre, before a 90 degree right in Turn 6. Turn 7, a small leftwards kink, leads onto a sweeping right hander. Turns 9 and 10, a car park when not in use, form the Clark Chicane, before the long lakeside sweep right up to Turn 11, on which the cars reach 300 km/h. The next two corners are the fastest on the circuit, with drivers taking this chicane at up to 225 km/h, sustaining g-forces up to 3.5g. The straight that follows, with a small right kink in the middle, is a DRS zone (the detection point was shortly before Turn 11), before another 90 degree right in the form of Turn 13, called Ascari. After another short straight, the right hander of Turn 14 leads to the slowest corner on the track, a tight left. The pit lane entry is located halfway before this corner and the next, a faster right hander, which together form an extended chicane, and lead back onto the Walker Straight. The track is known for being bumpy, and in even slightly wet weather, is notoriously slippery.
During the nine months of the year when the track is not required for Grand Prix preparation or the race weekend, most of the track can be driven by ordinary street-registered vehicles either clockwise or anti-clockwise.
Only the sections between turns 3, 4 and 5, then 5 and 6, differ significantly from the race track configuration. Turn 4 is replaced by a car park access road running directly from turns 3 to 5. Between turns 5 and 6, the road is blocked. It is possible to drive from turn 5 on to Albert Road and back on to the track at turn 7 though three sets of lights control the flow of this option. The only set of lights on the actual track is halfway between turns 12 and 13, where drivers using Queens Road are catered for. The chicanes at turns 11 and 12 are considerably more open than that used in the Grand Prix, using the escape roads. Turn 9 is also a car park and traffic is directed down another escape road.
The speed limit is generally , while some short sections have a speed limit of , which is still slower than an F1 car under pit lane speed restrictions. The back of the track, turns 7 to 13 inclusive, is known as Lakeside Drive. Double lines separate the two-way traffic along most of Lakeside Drive with short road islands approximately every 50 metres which means overtaking is illegal here. Black Swans live and breed in Albert Park, and frequently cross the road causing traffic delays, sometimes with up to five cygnets (young swans).
Approximately 80% of the track edge is lined with short parkland-style chain-linked fencing leaving normal drivers less room for error than F1 drivers have during race weekend. There is however substantial shoulder room between the outside of each lane and the fencing, which is used as parking along Aughtie Drive during the other nine months.
Albert Park has the distinction of being the only venue to host the Australian Grand Prix in both World Championship and non-World Championship formats with an earlier configuration of the current circuit used for the race on two occasions during the 1950s. During this time racing was conducted in an anti-clockwise direction as opposed to the current circuit which runs clockwise.
Known as the Albert Park Circuit, the original 3.125 mile (5.03 kilometre) course hosted a total of six race meetings:
As of 16 March 2019.

</doc>
<doc id="10946" url="https://en.wikipedia.org/wiki?curid=10946" title="Monaco Grand Prix">
Monaco Grand Prix

The Monaco Grand Prix () is a Formula One motor race held annually on the Circuit de Monaco on the last weekend in May. Run since 1929, it is widely considered to be one of the most important and prestigious automobile races in the world, and is one of the races—along with the Indianapolis 500 and the 24 Hours of Le Mans—that form the Triple Crown of Motorsport. The circuit has been called "an exceptional location of glamour and prestige".
The race is held on a narrow course laid out in the streets of Monaco, with many elevation changes and tight corners as well as a tunnel, making it one of the most demanding tracks in Formula One. In spite of the relatively low average speeds, the Monaco circuit is a dangerous place to race and often involves the intervention of a safety car. It is the only Grand Prix that does not adhere to the FIA's mandated minimum race distance for F1 races.
The Monaco Grand Prix was part of the pre-Second World War European Championship and was included in the first World Championship of Drivers in 1950. It was designated the European Grand Prix two times, 1955 and 1963, when this title was an honorary designation given each year to one Grand Prix race in Europe. Graham Hill was known as "Mr. Monaco" due to his five Monaco wins in the 1960s. Ayrton Senna won the race more times than any other driver, with six victories, winning five races consecutively between 1989 and 1993.
Like many European races, the Monaco Grand Prix predates the current World Championship. The principality's first Grand Prix was organised in 1929 by Antony Noghès, under the auspices of Prince Louis II, through the Automobile Club de Monaco (ACM), of which he was president. The ACM organised the Rallye Automobile Monte Carlo, and in 1928 applied to the "Association Internationale des Automobiles Clubs Reconnus" (AIACR), the international governing body of motorsport, to be upgraded from a regional French club to full national status. Their application was refused due to the lack of a major motorsport event held wholly within Monaco's boundaries. The rally could not be considered as it mostly used the roads of other European countries.
To attain full national status, Noghès proposed the creation of an automobile Grand Prix in the streets of Monte Carlo. He obtained the official sanction of Prince Louis II, and the support of Monégasque "Grand Prix" driver Louis Chiron. Chiron thought Monaco's topography well-suited to setting up a race track.
The first race, held on 14 April 1929, was won by William Grover-Williams (using the pseudonym "Williams"), driving a works Bugatti Type 35B. It was an invitation-only event, but not all of those invited decided to attend. The leading Maserati and Alfa Romeo drivers decided not to compete, but Bugatti was well represented. Mercedes sent their leading driver, Rudolf Caracciola. Starting fifteenth, Caracciola drove a fighting race, taking his SSK into the lead before wasting 4½ minutes on refuelling and a tyre change to finish second. Another driver who competed using a pseudonym was "Georges Philippe", the Baron Philippe de Rothschild. Chiron was unable to compete, having a prior commitment to compete in the Indianapolis 500 on the same day.
Caracciola's SSK was refused permission to race the following year, but Chiron did compete (in the works Bugatti Type 35C), when he was beaten by privateer René Dreyfus and his Bugatti Type 35B, and finished second. Chiron took victory in the 1931 race driving a Bugatti. , he remains the only native of Monaco to have won the event.
The race quickly grew in importance after its inception. Because of the high number of races which were being termed 'Grands Prix', the AIACR formally recognised the most important race of each of its affiliated national automobile clubs as International Grands Prix, or "Grandes Épreuves", and in 1933 Monaco was ranked as such alongside the French, Belgian, Italian, and Spanish Grands Prix. That year's race was the first Grand Prix in which grid positions were decided, as they are now, by practice time rather than the established method of balloting. The race saw Achille Varzi and Tazio Nuvolari exchange the lead many times before the race being settled in Varzi's favour on the final lap when Nuvolari's car caught fire.
The race became a round of the new European Championship in 1936, when stormy weather and a broken oil line led to a series of crashes, eliminating the Mercedes-Benzes of Chiron, Fagioli, and von Brauchitsch, as well as Bernd Rosemeyer's "Typ C" for newcomer Auto Union; Rudolf Caracciola, proving the truth of his nickname, "Regenmeister" (Rainmaster), went on to win. In 1937, von Brauchitsch duelled Caracciola before coming out on top. It was the last prewar "Grand Prix" at Monaco, for in 1938, the demand for £500 (about US$2450) in appearance money per top entrant led AIACR to cancel the event, while looming war overtook it in 1939, and the Second World War ended organised racing in Europe until 1945.
Racing in Europe started again on 9 September 1945 at the Bois de Boulogne Park in the city of Paris, four months and one day after the end of the war in Europe. However, the Monaco Grand Prix was not run between 1945 and 1947 due to financial reasons. In 1946 a new premier racing category, Grand Prix, was defined by the Fédération Internationale de l'Automobile (FIA), the successor of the AIACR, based on the pre-war voiturette class. A Monaco Grand Prix was run to this formula in 1948, won by the future world champion Nino Farina in a Maserati 4CLT.
The 1949 event was cancelled due to the death of Prince Louis II; it was included in the new Formula One World Drivers' Championship the following year. The race provided future five-time world champion Juan Manuel Fangio with his first win in a World Championship race, as well as third place for the 51-year-old Louis Chiron, his best result in the World Championship era. However, there was no race in 1951. In 1952, the first of the two years in which the World Drivers' Championship was run to less powerful Formula Two regulations, the race was run to sports car rules instead, and it did not form part of the World Championship.
No races were held in 1953 or 1954.
Since 1955, the Monaco Grand Prix has continuously been part of the Formula One World Championship. That year, Maurice Trintignant won in Monte Carlo for the first time and Chiron again scored points and at 56 became the oldest driver to compete in a Formula One Grand Prix. It was not until 1957, when Fangio won again, that the Grand Prix saw a double winner. Between 1954 and 1961 Fangio's former Mercedes colleague, Stirling Moss, went one better, as did Trintignant, who won the race again in 1958 driving a Cooper. The 1961 race saw Moss fend off three works Ferrari 156s in a year-old privateer Rob Walker Racing Team Lotus 18, to take his third Monaco victory.
Britain's Graham Hill won the race five times in the 1960s and became known as "King of Monaco" and "Mr. Monaco". He first won in 1963, and then won the next two years. In the 1965 race he took pole position and led from the start, but went up an escape road on lap 25 to avoid hitting a slow backmarker. Re-joining in fifth place, Hill set several new lap records on the way to winning. The race was also notable for Jim Clark's absence (he was doing the Indianapolis 500), and for Paul Hawkins's Lotus ending up in the harbour. Hill's teammate, Briton Jackie Stewart, won in 1966 and New Zealander Denny Hulme won in 1967, but Hill won the next two years, the 1969 event being his final Formula One championship victory, by which time he was a double Formula One world champion.
By the start of the 1970s, efforts by Jackie Stewart saw several Formula One events cancelled because of safety concerns. For the 1969 event, Armco barriers were placed at specific points for the first time in the circuit's history. Before that, the circuit's conditions were (aside from the removal of people's production cars parked on the side of the road) virtually identical to everyday road use. If a driver went off, he had a chance to crash into whatever was next to the track (buildings, trees, lamp posts, glass windows, and even a train station), and in Alberto Ascari's and Paul Hawkins's cases, the harbour water, because the concrete road the course used had no Armco to protect the drivers from going off the track and into the Mediterranean. The circuit gained more Armco in specific points for the next two races, and by 1972, the circuit was almost completely Armco-lined. For the first time in its history, the Monaco circuit was altered in 1972 as the pits were moved next to the waterfront straight between the chicane and Tabac and the chicane was moved further forward right before Tabac becoming the junction point between the pits and the course. The course was changed again for the 1973 race. The Rainier III Nautical Stadium was constructed where the straight that went behind the pits was and the circuit introduced a double chicane that went around the new swimming pool (this chicane complex is known today as "Swimming Pool"). This created space for a whole new pit facility and in 1976 the course was altered yet again; the Sainte Devote corner was made slower and a chicane was placed right before the pit straight.
By the early 1970s, as Brabham team owner Bernie Ecclestone started to marshal the collective bargaining power of the Formula One Constructors Association (FOCA), Monaco was prestigious enough to become an early bone of contention. Historically the number of cars permitted in a race was decided by the race organiser, in this case the ACM, which had always set a low number of around 16. In 1972 Ecclestone started to negotiate deals which relied on FOCA guaranteeing at least 18 entrants for every race. A stand-off over this issue left the 1972 race in jeopardy until the ACM gave in and agreed that 26 cars could participate – the same number permitted at most other circuits. Two years later, in 1974, the ACM got the numbers back down to 18.
Because of its tight confines, slow average speeds and punishing nature, Monaco has often thrown up unexpected results. In the 1982 race René Arnoux led the first 15 laps, before retiring. Alain Prost then led until four laps from the end, when he spun off on the wet track, hit the barriers and lost a wheel, giving Riccardo Patrese the lead. Patrese himself spun with only a lap and a half to go, letting Didier Pironi through to the front, followed by Andrea de Cesaris. On the last lap, Pironi ran out of fuel in the tunnel, but De Cesaris also ran out of fuel before he could overtake. In the meantime, Patrese had bump-started his car and went through to score his first Grand Prix win.
In 1983 the ACM became entangled in the disagreements between Fédération Internationale du Sport Automobile (FISA) and FOCA. The ACM, with the agreement of Bernie Ecclestone, negotiated an individual television rights deal with ABC in the United States. This broke an agreement enforced by FISA for a single central negotiation of television rights. Jean-Marie Balestre, president of FISA, announced that the Monaco Grand Prix would not form part of the Formula One world championship in 1985. The ACM fought their case in the French courts. They won the case and the race was eventually reinstated.
For the decade from 1984 to 1993 the race was won by only two drivers, arguably the two best drivers in Formula One at the time – Frenchman Alain Prost and Brazilian Ayrton Senna. Prost, already a winner of the support race for Formula Three cars in 1979, took his first Monaco win at the 1984 race. The race started 45 minutes late after heavy rain. Prost led briefly before Nigel Mansell overtook him on lap 11. Mansell crashed out five laps later, letting Prost back into the lead. On lap 27, Prost led from Ayrton Senna's Toleman and Stefan Bellof's Tyrrell. Senna was catching Prost and Bellof was catching both of them in the only naturally aspirated car in the race. However, on lap 31, the race was controversially stopped with conditions deemed to be undriveable. Later, FISA fined the clerk of the course, Jacky Ickx, $6,000 and suspended his licence for not consulting the stewards before stopping the race. The drivers received only half of the points that would usually be awarded, as the race had been stopped before two-thirds of the intended race distance had been completed.
Prost won 1985 after polesitter Senna retired with a blown Renault engine in his Lotus after over-revving it at the start, and Michele Alboreto in the Ferrari retook the lead twice, but he went off the track at Sainte-Devote, where Brazilian Nelson Piquet and Italian Riccardo Patrese had a huge accident only a few laps previously and oil and debris littered the track. Prost passed Alboreto, who retook the Frenchman, and then he punctured a tyre after running over bodywork debris from the Piquet/Patrese accident, which dropped him to 4th. He was able to pass his Roman countrymen Andrea De Cesaris and Elio de Angelis, but finished 2nd behind Prost. The French Prost dominated 1986 after starting from pole position, a race where the Nouvelle Chicane had been changed on the grounds of safety.
Senna holds the record for the most victories in Monaco, with six, including five consecutive wins between 1989 and 1993, as well as eight podium finishes in ten starts. His 1987 win was the first time a car with an active suspension had won a Grand Prix. He won this race after Briton Nigel Mansell in a Williams-Honda went out with a broken exhaust. His win was very popular with the people of Monaco, and when he was arrested on the Monday following the race, for riding a motorcycle without wearing a helmet, he was released by the officers after they realised who he was. Senna dominated 1988, and was able to get ahead of his teammate Prost while the Frenchman was held up for most of the race by Austrian Gerhard Berger in a Ferrari. By the time Prost got past Berger, he pushed as hard as he could and set a lap some 6 seconds faster than Senna's; Senna then set 2 fastest laps, and while pushing as hard as possible, he touched the barrier at the Portier corner and crashed into the Armco separating the road from the Mediterranean. Senna was so upset that he went back to his Monaco flat and was not heard from until the evening. Prost went on to win for the fourth time.
Senna dominated 1989 while Prost was stuck behind backmarker Rene Arnoux and others; the Brazilian also dominated 1990 and 1991. At the 1992 event Nigel Mansell, who had won all five races held to that point in the season, took pole and dominated the race in his Williams FW14B-Renault. However, with seven laps remaining, Mansell suffered a loose wheel nut and was forced into the pits, emerging behind Ayrton Senna's McLaren-Honda, who was on worn tyres. Mansell, on fresh tyres, set a lap record almost two seconds quicker than Senna's and closed from 5.2 to 1.9 seconds in only two laps. The pair duelled around Monaco for the final four laps but Mansell could find no way past, finishing just two-tenths of a second behind the Brazilian. It was Senna's fifth win at Monaco, equalling Graham Hill's record. Senna had a poor start to the 1993 event, crashing in practice and qualifying 3rd behind pole-sitter Prost and the rising German star Michael Schumacher. Both of them beat Senna to the first corner, but Prost had to serve a time penalty for jumping the start and Schumacher retired after suspension problems, so Senna took his sixth win to break Graham Hill's record for most wins at the Monaco Grand Prix. Runner-up Damon Hill commented, "If my father was around now, he would be the first to congratulate Ayrton."
The 1994 race was an emotional and tragic affair. It came two weeks after the race at Imola in which Austrian Roland Ratzenberger and Ayrton Senna both died from massive head injuries from on-track accidents on successive days. During the Monaco event, Austrian Karl Wendlinger had an accident in his Sauber in the tunnel; he went into a coma and was to miss the rest of the season. The German Michael Schumacher won the 1994 Monaco event. The 1996 race saw Michael Schumacher take pole position before crashing out on the first lap after being overtaken by Damon Hill. Hill led the first 40 laps before his engine expired in the tunnel. Jean Alesi took the lead but suffered suspension failure 20 laps later. Olivier Panis, who started in 14th place, moved into the lead and stayed there until the end of the race, being pushed all the way by David Coulthard. It was Panis's only win, and the last for his Ligier team. Only three cars crossed the finish line, but seven were classified.
Seven-time world champion Schumacher would eventually win the race five times, matching Graham Hill's record. In his appearance at the 2006 event, he attracted criticism when, while provisionally holding pole position and with the qualifying session drawing to a close, he stopped his car at the Rascasse hairpin, blocking the track and obliging competitors to slow down. Although Schumacher claimed it was the unintentional result of a genuine car failure, the FIA disagreed and he was sent to the back of the grid.
In July 2010, Bernie Ecclestone announced that a 10-year deal had been reached with the race organisers, keeping the race on the calendar until at least 2020.
Due to the COVID-19 pandemic, the FIA announced the 2020 Monaco Grand Prix's postponement, along with the two other races scheduled for May 2020, to help prevent the spread of the virus.
However, later the same day the Automobile Club de Monaco confirmed that the Grand Prix was instead cancelled, making 2020 the first time the Grand Prix was not run since 1954.
The Circuit de Monaco consists of the city streets of Monte Carlo and La Condamine, which includes the famous harbour. It is unique in having been held on the same circuit every time it has been run over such a long period – only the Italian Grand Prix, which has been held at Autodromo Nazionale Monza during every Formula One regulated year except 1980, has a similarly lengthy and close relationship with a single circuit.
The race circuit has many elevation changes, tight corners, and a narrow course that makes it one of the most demanding tracks in Formula One racing. , two drivers have crashed and ended up in the harbour, the most famous being Alberto Ascari in 1955. Despite the fact that the course has had minor changes several times during its history, it is still considered the ultimate test of driving skills in Formula One, and if it were not already an existing Grand Prix, it would not be permitted to be added to the schedule for safety reasons. Even in 1929, 'La Vie Automobile' magazine offered the opinion that "Any respectable traffic system would have covered the track with «Danger» sign posts left, right and centre".
Triple Formula One champion Nelson Piquet was fond of saying that racing at Monaco was "like trying to cycle round your living room", but added that "a win here was worth two anywhere else".
Notably, the course includes a tunnel. The contrast of daylight and gloom when entering/exiting the tunnel presents "challenges not faced elsewhere", as the drivers have to "adjust their vision as they emerge from the tunnel at the fastest point of the track and brake for the chicane in the daylight.".
The fastest-ever qualifying lap was set by Lewis Hamilton in qualifying (Q3) for the 2019 Monaco Grand Prix, at a time of 1m 10.166s.
During the Grand Prix weekend, spectators crowd around the Monaco Circuit. There are a number of temporary grandstands built around the circuit, mostly around the harbour area. The rich and famous spectators often arrive on their boats and the yachts through the harbour. Balconies around Monaco become viewing areas for the race as well. Many hotels and residents cash in on the bird's eye views of the race.
The Monaco Grand Prix is organised each year by the "Automobile Club de Monaco" which also runs the Monte Carlo Rally and the Junior Monaco Kart Cup.
The Monaco Grand Prix differs in several ways from other Grands Prix. The practice session for the race is held on the Thursday preceding the race instead of Friday. This allows the streets to be opened to the public again on Friday. Until the late 1990s the race started at 3:30 p.m. local time – an hour and a half later than other European Formula One races. In recent years the race has fallen in line with the other Formula One races for the convenience of television viewers. Also, earlier the event was traditionally held on the week of Ascension Day. It is now always held on the last weekend in May. For many years, the numbers of cars admitted to Grands Prix was at the discretion of the race organisers – Monaco had the smallest grids, ostensibly because of its narrow and twisting track. Only 18 cars were permitted to enter the 1975 Monaco Grand Prix, compared to 23 to 26 cars at all other rounds that year.
The erecting of the circuit takes six weeks, and the removal after the race takes three weeks. There was no podium as such at the race, until 2017. Instead, a section of the track was closed after the race to act as parc fermé, a place where the cars are held for official inspection. The first three drivers in the race left their cars there and walked directly to the royal box where the 'podium' ceremony was held, which was considered a custom for the race. The trophies were handed out before the national anthems for the winning driver and team are played, as opposed to other Grands Prix where the anthems are played first.
The Monaco Grand Prix is widely considered to be one of the most important and prestigious automobile races in the world alongside the Indianapolis 500 and the 24 Hours of Le Mans. These three races are considered to form a "Triple Crown" of the three most famous motor races in the world. As of 2020, Graham Hill is the only driver to have won the Triple Crown, by winning all three races. The practice session for Monaco overlaps with that for the Indianapolis 500, and the races themselves sometimes clash. As the two races take place on opposite sides of the Atlantic Ocean and form part of different championships, it is difficult for one driver to compete effectively in both during his career. Juan Pablo Montoya and Fernando Alonso are the only active drivers to have won two of the three events.
In awarding its first Gold medal for motorsport to Prince Rainier III, the Fédération Internationale de l'Automobile (FIA) characterised the Monaco Grand Prix as contributing "an exceptional location of glamour and prestige" to motorsport. The Grand Prix has been run under the patronage of three generations of Monaco's royal family: Louis II, Rainier III and Albert II, all of whom have taken a close interest in the race. A large part of the principality's income comes from tourists attracted by the warm climate and the famous casino, but it is also a tax haven and is home to many millionaires, including several Formula One drivers.
Monaco has produced four native Formula One drivers - Louis Chiron, André Testut, Olivier Beretta, and Charles Leclerc - but its tax status has made it home to many drivers over the years, including Gilles Villeneuve and Ayrton Senna. Of the Formula One contenders, several have property in the principality, including Jenson Button and David Coulthard, who was part owner of a hotel there. Because of the small size of the town and the location of the circuit, drivers whose races end early can usually get back to their apartments in minutes. Ayrton Senna famously retired to his apartment after crashing out of the lead of the 1988 race.
The Grand Prix attracts big-name celebrities each year who come to experience the glamour and prestige of the event. Big parties are held in the nightclubs on the Grand Prix weekend, and the Port Hercule fills up with party-goers joining in the celebrations.
Unlike other venues, the Monaco Grand Prix has never been sponsored since its first race in 1950. It however, has had multiple names it goes by, namely the following:
"Drivers in bold are competing in the Formula One championship in the current season."
"Teams in bold are competing in the Formula One championship in the current season."<br>
"A pink background indicates an event which was not part of the Formula One World Championship."<br>
"A yellow background indicates an event which was part of the pre-war European Championship."
"Manufacturers in bold are competing in the Formula One championship in the current season."<br>
"A pink background indicates an event which was not part of the Formula One World Championship."<br>
"A yellow background indicates an event which was part of the pre-war European Championship."
"A pink background indicates an event which was not part of the Formula One World Championship."<br>
"A yellow background indicates an event which was part of the pre-war European Championship."

</doc>
<doc id="10947" url="https://en.wikipedia.org/wiki?curid=10947" title="Fission">
Fission

Fission, a splitting of something into two or more parts, may refer to:

</doc>
<doc id="10948" url="https://en.wikipedia.org/wiki?curid=10948" title="Fusion">
Fusion

Fusion, or synthesis, is the process of combining two or more distinct entities into a new whole.
Fusion may also refer to:

</doc>
<doc id="10949" url="https://en.wikipedia.org/wiki?curid=10949" title="Four color theorem">
Four color theorem

In mathematics, the four color theorem, or the four color map theorem, states that, given any separation of a plane into contiguous regions, producing a figure called a "map", no more than four colors are required to color the regions of the map so that no two adjacent regions have the same color. "Adjacent" means that two regions share a common boundary curve segment, not merely a corner where three or more regions meet. It was the first major theorem to be proved using a computer. Initially, this proof was not accepted by all mathematicians because the computer-assisted proof was infeasible for a human to check by hand. Since then the proof has gained wide acceptance, although some doubters remain.
The four color theorem was proved in 1976 by Kenneth Appel and Wolfgang Haken after many false proofs and counterexamples (unlike the five color theorem, proved in the 1800s, which states that five colors are enough to color a map). To dispel any remaining doubts about the Appel–Haken proof, a simpler proof using the same ideas and still relying on computers was published in 1997 by Robertson, Sanders, Seymour, and Thomas. Additionally, in 2005, the theorem was proved by Georges Gonthier with general-purpose theorem-proving software.
In graph-theoretic terms, the theorem states that for loopless planar graph formula_1, the chromatic number of its dual graph is formula_2.
The intuitive statement of the four color theorem – "given any separation of a plane into contiguous regions, the regions can be colored using at most four colors so that no two adjacent regions have the same color" – needs to be interpreted appropriately to be correct.
First, regions are adjacent if they share a boundary segment; two regions that share only isolated boundary points are not considered adjacent. Second, bizarre regions, such as those with finite area but infinitely long perimeter, are not allowed; maps with such regions can require more than four colors. (To be safe, we can restrict to regions whose boundaries consist of finitely many straight line segments. It is allowed that a region entirely surround one or more other regions.) Note that the notion of "contiguous region" (technically: connected open subset of the plane) is not the same as that of a "country" on regular maps, since countries need not be contiguous (e.g., the Cabinda Province as part of Angola, Nakhchivan as part of Azerbaijan, Kaliningrad as part of Russia, and Alaska as part of the United States are not contiguous). If we required the entire territory of a country to receive the same color, then four colors are not always sufficient. For instance, consider a simplified map:
In this map, the two regions labeled "A" belong to the same country. If we wanted those regions to receive the same color, then five colors would be required, since the two "A" regions together are adjacent to four other regions, each of which is adjacent to all the others. A similar construction also applies if a single color is used for all bodies of water, as is usual on real maps. For maps in which more than one country may have multiple disconnected regions, six or more colors might be required.
A simpler statement of the theorem uses graph theory. The set of regions of a map can be represented more abstractly as an undirected graph that has a vertex for each region and an edge for every pair of regions that share a boundary segment. This graph is planar: it can be drawn in the plane without crossings by placing each vertex at an arbitrarily chosen location within the region to which it corresponds, and by drawing the edges as curves without crossings that lead from one region's vertex, across a shared boundary segment, to an adjacent region's vertex. Conversely any planar graph can be formed from a map in this way. In graph-theoretic terminology, the four-color theorem states that the vertices of every planar graph can be colored with at most four colors so that no two adjacent vertices receive the same color, or for short:
As far as is known, the conjecture was first proposed on October 23, 1852, when Francis Guthrie, while trying to color the map of counties of England, noticed that only four different colors were needed. At the time, Guthrie's brother, Frederick, was a student of Augustus De Morgan (the former advisor of Francis) at University College London. Francis inquired with Frederick regarding it, who then took it to De Morgan (Francis Guthrie graduated later in 1852, and later became a professor of mathematics in South Africa). According to De Morgan:
"A student of mine [Guthrie] asked me to day to give him a reason for a fact which I did not know was a fact—and do not yet. He says that if a figure be any how divided and the compartments differently colored so that figures with any portion of common boundary "line" are differently colored—four colors may be wanted but not more—the following is his case in which four colors "are" wanted. Query cannot a necessity for five or more be invented…" 
"F.G.", perhaps one of the two Guthries, published the question in "The Athenaeum" in 1854, and De Morgan posed the question again in the same magazine in 1860. Another early published reference by in turn credits the conjecture to De Morgan.
There were several early failed attempts at proving the theorem. De Morgan believed that it followed from a simple fact about four regions, though he didn't believe that fact could be derived from more elementary facts.
This arises in the following way. We never need four colors in a neighborhood unless there be four counties, each of which has boundary lines in common with each of the other three. Such a thing cannot happen with four areas unless one or more of them be inclosed by the rest; and the color used for the inclosed county is thus set free to go on with. Now this principle, that four areas cannot each have common boundary with all the other three without inclosure, is not, we fully believe, capable of demonstration upon anything more evident and more elementary; it must stand as a postulate.
One alleged proof was given by Alfred Kempe in 1879, which was widely acclaimed; another was given by Peter Guthrie Tait in 1880. It was not until 1890 that Kempe's proof was shown incorrect by Percy Heawood, and in 1891, Tait's proof was shown incorrect by Julius Petersen—each false proof stood unchallenged for 11 years.
In 1890, in addition to exposing the flaw in Kempe's proof, Heawood proved the five color theorem and generalized the four color conjecture to surfaces of arbitrary genus.
Tait, in 1880, showed that the four color theorem is equivalent to the statement that a certain type of graph (called a snark in modern terminology) must be non-planar.
In 1943, Hugo Hadwiger formulated the Hadwiger conjecture, a far-reaching generalization of the four-color problem that still remains unsolved.
During the 1960s and 1970s, German mathematician Heinrich Heesch developed methods of using computers to search for a proof. Notably he was the first to use discharging for proving the theorem, which turned out to be important in the unavoidability portion of the subsequent Appel–Haken proof. He also expanded on the concept of reducibility and, along with Ken Durre, developed a computer test for it. Unfortunately, at this critical juncture, he was unable to procure the necessary supercomputer time to continue his work.
Others took up his methods, including his computer-assisted approach. While other teams of mathematicians were racing to complete proofs, Kenneth Appel and Wolfgang Haken at the University of Illinois announced, on June 21, 1976, that they had proved the theorem. They were assisted in some algorithmic work by John A. Koch.
If the four-color conjecture were false, there would be at least one map with the smallest possible number of regions that requires five colors. The proof showed that such a minimal counterexample cannot exist, through the use of two technical concepts:
Using mathematical rules and procedures based on properties of reducible configurations, Appel and Haken found an unavoidable set of reducible configurations, thus proving that a minimal counterexample to the four-color conjecture could not exist. Their proof reduced the infinitude of possible maps to 1,834 reducible configurations (later reduced to 1,482) which had to be checked one by one by computer and took over a thousand hours. This reducibility part of the work was independently double checked with different programs and computers. However, the unavoidability part of the proof was verified in over 400 pages of microfiche, which had to be checked by hand with the assistance of Haken's daughter Dorothea Blostein .
Appel and Haken's announcement was widely reported by the news media around the world, and the math department at the University of Illinois used a postmark stating "Four colors suffice." At the same time the unusual nature of the proof—it was the first major theorem to be proved with extensive computer assistance—and the complexity of the human-verifiable portion aroused considerable controversy .
In the early 1980s, rumors spread of a flaw in the Appel–Haken proof. Ulrich Schmidt at RWTH Aachen had examined Appel and Haken's proof for his master's thesis that was published in 1981 . He had checked about 40% of the unavoidability portion and found a significant error in the discharging procedure . In 1986, Appel and Haken were asked by the editor of "Mathematical Intelligencer" to write an article addressing the rumors of flaws in their proof. They responded that the rumors were due to a "misinterpretation of [Schmidt's] results" and obliged with a detailed article . Their magnum opus, "Every Planar Map is Four-Colorable", a book claiming a complete and detailed proof (with a microfiche supplement of over 400 pages), appeared in 1989; it explained and corrected the error discovered by Schmidt as well as several further errors found by others .
Since the proving of the theorem, efficient algorithms have been found for 4-coloring maps requiring only O("n") time, where "n" is the number of vertices. In 1996, Neil Robertson, Daniel P. Sanders, Paul Seymour, and Robin Thomas created a quadratic-time algorithm, improving on a quartic-time algorithm based on Appel and Haken's proof. This new proof is similar to Appel and Haken's but more efficient because it reduces the complexity of the problem and requires checking only 633 reducible configurations. Both the unavoidability and reducibility parts of this new proof must be executed by computer and are impractical to check by hand. In 2001, the same authors announced an alternative proof, by proving the snark conjecture. This proof remains unpublished, however.
In 2005, Benjamin Werner and Georges Gonthier formalized a proof of the theorem inside the Coq proof assistant. This removed the need to trust the various computer programs used to verify particular cases; it is only necessary to trust the Coq kernel.
The following discussion is a summary based on the introduction to "Every Planar Map is Four Colorable" . Although flawed, Kempe's original purported proof of the four color theorem provided some of the basic tools later used to prove it. The explanation here is reworded in terms of the modern graph theory formulation above.
Kempe's argument goes as follows. First, if planar regions separated by the graph are not "triangulated", i.e. do not have exactly three edges in their boundaries, we can add edges without introducing new vertices in order to make every region triangular, including the unbounded outer region. If this triangulated graph is colorable using four colors or fewer, so is the original graph since the same coloring is valid if edges are removed. So it suffices to prove the four color theorem for triangulated graphs to prove it for all planar graphs, and without loss of generality we assume the graph is triangulated.
Suppose "v", "e", and "f" are the number of vertices, edges, and regions (faces). Since each region is triangular and each edge is shared by two regions, we have that 2"e" = 3"f". This together with Euler's formula, "v" − "e" + "f" = 2, can be used to show that 6"v" − 2"e" = 12. Now, the "degree" of a vertex is the number of edges abutting it. If "v" is the number of vertices of degree "n" and "D" is the maximum degree of any vertex,
But since 12 > 0 and 6 − "i" ≤ 0 for all "i" ≥ 6, this demonstrates that there is at least one vertex of degree 5 or less.
If there is a graph requiring 5 colors, then there is a "minimal" such graph, where removing any vertex makes it four-colorable. Call this graph "G". Then "G" cannot have a vertex of degree 3 or less, because if "d"("v") ≤ 3, we can remove "v" from "G", four-color the smaller graph, then add back "v" and extend the four-coloring to it by choosing a color different from its neighbors.
Kempe also showed correctly that "G" can have no vertex of degree 4. As before we remove the vertex "v" and four-color the remaining vertices. If all four neighbors of "v" are different colors, say red, green, blue, and yellow in clockwise order, we look for an alternating path of vertices colored red and blue joining the red and blue neighbors. Such a path is called a Kempe chain. There may be a Kempe chain joining the red and blue neighbors, and there may be a Kempe chain joining the green and yellow neighbors, but not both, since these two paths would necessarily intersect, and the vertex where they intersect cannot be colored. Suppose it is the red and blue neighbors that are not chained together. Explore all vertices attached to the red neighbor by red-blue alternating paths, and then reverse the colors red and blue on all these vertices. The result is still a valid four-coloring, and "v" can now be added back and colored red.
This leaves only the case where "G" has a vertex of degree 5; but Kempe's argument was flawed for this case. Heawood noticed Kempe's mistake and also observed that if one was satisfied with proving only five colors are needed, one could run through the above argument (changing only that the minimal counterexample requires 6 colors) and use Kempe chains in the degree 5 situation to prove the five color theorem.
In any case, to deal with this degree 5 vertex case requires a more complicated notion than removing a vertex. Rather the form of the argument is generalized to considering "configurations", which are connected subgraphs of "G" with the degree of each vertex (in G) specified. For example, the case described in degree 4 vertex situation is the configuration consisting of a single vertex labelled as having degree 4 in "G". As above, it suffices to demonstrate that if the configuration is removed and the remaining graph four-colored, then the coloring can be modified in such a way that when the configuration is re-added, the four-coloring can be extended to it as well. A configuration for which this is possible is called a "reducible configuration". If at least one of a set of configurations must occur somewhere in G, that set is called "unavoidable". The argument above began by giving an unavoidable set of five configurations (a single vertex with degree 1, a single vertex with degree 2, ..., a single vertex with degree 5) and then proceeded to show that the first 4 are reducible; to exhibit an unavoidable set of configurations where every configuration in the set is reducible would prove the theorem.
Because "G" is triangular, the degree of each vertex in a configuration is known, and all edges internal to the configuration are known, the number of vertices in "G" adjacent to a given configuration is fixed, and they are joined in a cycle. These vertices form the "ring" of the configuration; a configuration with "k" vertices in its ring is a "k"-ring configuration, and the configuration together with its ring is called the "ringed configuration". As in the simple cases above, one may enumerate all distinct four-colorings of the ring; any coloring that can be extended without modification to a coloring of the configuration is called "initially good". For example, the single-vertex configuration above with 3 or less neighbors were initially good. In general, the surrounding graph must be systematically recolored to turn the ring's coloring into a good one, as was done in the case above where there were 4 neighbors; for a general configuration with a larger ring, this requires more complex techniques. Because of the large number of distinct four-colorings of the ring, this is the primary step requiring computer assistance.
Finally, it remains to identify an unavoidable set of configurations amenable to reduction by this procedure. The primary method used to discover such a set is the method of discharging. The intuitive idea underlying discharging is to consider the planar graph as an electrical network. Initially positive and negative "electrical charge" is distributed amongst the vertices so that the total is positive.
Recall the formula above:
Each vertex is assigned an initial charge of 6-deg("v"). Then one "flows" the charge by systematically redistributing the charge from a vertex to its neighboring vertices according to a set of rules, the "discharging procedure". Since charge is preserved, some vertices still have positive charge. The rules restrict the possibilities for configurations of positively charged vertices, so enumerating all such possible configurations gives an unavoidable set.
As long as some member of the unavoidable set is not reducible, the discharging procedure is modified to eliminate it (while introducing other configurations). Appel and Haken's final discharging procedure was extremely complex and, together with a description of the resulting unavoidable configuration set, filled a 400-page volume, but the configurations it generated could be checked mechanically to be reducible. Verifying the volume describing the unavoidable configuration set itself was done by peer review over a period of several years.
A technical detail not discussed here but required to complete the proof is "immersion reducibility".
The four color theorem has been notorious for attracting a large number of false proofs and disproofs in its long history. At first, "The New York Times" refused as a matter of policy to report on the Appel–Haken proof, fearing that the proof would be shown false like the ones before it . Some alleged proofs, like Kempe's and Tait's mentioned above, stood under public scrutiny for over a decade before they were refuted. But many more, authored by amateurs, were never published at all.
Generally, the simplest, though invalid, counterexamples attempt to create one region which touches all other regions. This forces the remaining regions to be colored with only three colors. Because the four color theorem is true, this is always possible; however, because the person drawing the map is focused on the one large region, they fail to notice that the remaining regions can in fact be colored with three colors.
This trick can be generalized: there are many maps where if the colors of some regions are selected beforehand, it becomes impossible to color the remaining regions without exceeding four colors. A casual verifier of the counterexample may not think to change the colors of these regions, so that the counterexample will appear as though it is valid.
Perhaps one effect underlying this common misconception is the fact that the color restriction is not transitive: a region only has to be colored differently from regions it touches directly, not regions touching regions that it touches. If this were the restriction, planar graphs would require arbitrarily large numbers of colors.
Other false disproofs violate the assumptions of the theorem, such as using a region that consists of multiple disconnected parts, or disallowing regions of the same color from touching at a point.
While every planar map can be colored with four colors, it is NP-complete in complexity to decide whether an arbitrary planar map can be colored with just three colors.
The four-color theorem applies not only to finite planar graphs, but also to infinite graphs that can be drawn without crossings in the plane, and even more generally to infinite graphs (possibly with an uncountable number of vertices) for which every finite subgraph is planar. To prove this, one can combine a proof of the theorem for finite planar graphs with the De Bruijn–Erdős theorem stating that, if every finite subgraph of an infinite graph is "k"-colorable, then the whole graph is also "k"-colorable . This can also be seen as an immediate consequence of Kurt Gödel's compactness theorem for first-order logic, simply by expressing the colorability of an infinite graph with a set of logical formulae.
One can also consider the coloring problem on surfaces other than the plane (Weisstein). The problem on the sphere or cylinder is equivalent to that on the plane. For closed (orientable or non-orientable) surfaces with positive genus, the maximum number "p" of colors needed depends on the surface's Euler characteristic χ according to the formula
where the outermost brackets denote the floor function.
Alternatively, for an orientable surface the formula can be given in terms of the genus of a surface, "g":
This formula, the Heawood conjecture, was conjectured by P. J. Heawood in 1890 and proved by Gerhard Ringel and J. W. T. Youngs in 1968. The only exception to the formula is the Klein bottle, which has Euler characteristic 0 (hence the formula gives p = 7) and requires only 6 colors, as shown by P. Franklin in 1934 (Weisstein).
For example, the torus has Euler characteristic χ = 0 (and genus "g" = 1) and thus "p" = 7, so no more than 7 colors are required to color any map on a torus. This upper bound of 7 is sharp: certain toroidal polyhedra such as the Szilassi polyhedron require seven colors.
A Möbius strip requires six colors as do 1-planar graphs (graphs drawn with at most one simple crossing per edge) . If both the vertices and the faces of a planar graph are colored, in such a way that no two adjacent vertices, faces, or vertex-face pair have the same color, then again at most six colors are needed .
There is no obvious extension of the coloring result to three-dimensional solid regions. By using a set of "n" flexible rods, one can arrange that every rod touches every other rod. The set would then require "n" colors, or "n"+1 if you consider the empty space that also touches every rod. The number "n" can be taken to be any integer, as large as desired. Such examples were known to Fredrick Guthrie in 1880 . Even for axis-parallel cuboids (considered to be adjacent when two cuboids share a two-dimensional boundary area) an unbounded number of colors may be necessary (; ).
Dror Bar-Natan gave a statement concerning Lie algebras and Vassiliev invariants which is equivalent to the four color theorem.
Despite the motivation from coloring political maps of countries, the theorem is not of particular interest to cartographers. According to an article by the math historian Kenneth May, "Maps utilizing only four colors are rare, and those that do usually require only three. Books on cartography and the history of mapmaking do not mention the four-color property" . The theorem also does not guarantee the usual cartographic requirement that non-contiguous regions of the same country (such as the exclave Kaliningrad and the rest of Russia) be colored identically.

</doc>
<doc id="10951" url="https://en.wikipedia.org/wiki?curid=10951" title="Fahrenheit 451">
Fahrenheit 451

Fahrenheit 451 is a dystopian novel by American writer Ray Bradbury, first published in 1953. Often regarded as one of his best works, the novel presents a future American society where books are outlawed and "firemen" burn any that are found. The book's tagline explains the title: "Fahrenheit 451 – the temperature at which book paper catches fire, and burns..." The lead character, Guy Montag, is a fireman who becomes disillusioned with his role of censoring literature and destroying knowledge, eventually quitting his job and committing himself to the preservation of literary and cultural writings.
The novel has been the subject of interpretations focusing on the historical role of book burning in suppressing dissenting ideas for change. In a 1956 radio interview, Bradbury said that he wrote "Fahrenheit 451" because of his concerns at the time include/are about(during the McCarthy era) about the threat of book burning in the United States. In later years, he described the book as a commentary on how mass media reduces interest in reading literature.
In 1954, "Fahrenheit 451" won the American Academy of Arts and Letters Award in Literature and the Commonwealth Club of California Gold Medal. It later won the Prometheus "Hall of Fame" Award in 1984 and a "Retro" Hugo Award, one of a limited number of Best Novel Retro Hugos ever given, in 2004. Bradbury was honored with a Spoken Word Grammy nomination for his 1976 audiobook version.
Adaptations of the novel include François Truffaut's 1966 film adaptation and a 1982 BBC Radio dramatization. Bradbury published a stage play version in 1979 and helped develop a 1984 interactive fiction computer game titled "Fahrenheit 451", as well as a collection of his short stories titled "A Pleasure to Burn". HBO released a television film based on the novel and written and directed by Ramin Bahrani in 2018.
"Fahrenheit 451" is set in an unspecified city (likely in the American Midwest) in the year 1999 (according to Ray Bradbury's Coda), though it is written as if set in a distant future. The earliest editions make clear that it takes place no earlier than the year 1960.
The novel is divided into three parts: "The Hearth and the Salamander," "The Sieve and the Sand," and "Burning Bright."
Guy Montag is a "fireman" employed to burn houses containing outlawed books. He is married but has no children. One fall night while returning from work, he meets his new neighbour, a teenage girl named Clarisse McClellan, whose free-thinking ideals and liberating spirit cause him to question his life and his own perceived happiness. Montag returns home to find that his wife Mildred has overdosed on sleeping pills, and he calls for medical attention. Two uncaring EMTs pump Mildred's stomach, drain her poisoned blood, and fill her with new blood. After the EMTs leave to rescue another overdose victim, Montag goes outside and overhears Clarisse and her family talking about the way life is in this hedonistic, illiterate society. Montag's mind is bombarded with Clarisse's subversive thoughts and the memory of his wife's near-death. Over the next few days, Clarisse faithfully meets Montag each night as he walks home. She tells him about how her simple pleasures and interests make her an outcast among her peers and how she is forced to go to therapy for her behavior and thoughts. Montag looks forward to these meetings, and just as he begins to expect them, Clarisse goes missing. He senses something is wrong.
In the following days, while at work with the other firemen ransacking the book-filled house of an old woman and drenching it in kerosene before the inevitable burning, Montag steals a book before any of his coworkers notice. The woman refuses to leave her house and her books, choosing instead to light a match and burn herself alive. Jarred by the woman's suicide, Montag returns home and hides the stolen book under his pillow. Later, Montag wakes Mildred from her sleep and asks her if she has seen or heard anything about Clarisse McClellan. She reveals that Clarisse's family moved away after Clarisse was hit by a speeding car and died four days ago. Dismayed by her failure to mention this earlier, Montag uneasily tries to fall asleep. Outside he suspects the presence of "The Mechanical Hound", an eight-legged robotic dog-like creature that resides in the firehouse and aids the firemen in hunting book hoarders.
Montag awakens ill the next morning. Mildred tries to care for her husband but finds herself more involved in the "parlor wall" entertainment in the living room – large televisions filling the walls. Montag suggests that maybe he should take a break from being a fireman after what happened last night, and Mildred panics over the thought of losing the house and her parlor wall "family". Captain Beatty, Montag's fire chief, personally visits Montag to see how he is doing. Sensing his concerns, Beatty recounts the history of how books lost their value and how the firemen were adapted for their current role: over the course of several decades, people began to embrace new media (in this case, film and television), sports, and an ever-quickening pace of life. Books were ruthlessly abridged or degraded to accommodate short attention spans while minority groups protested the controversial, outdated content they perceived in literature (yet comic books, trade papers, and sex magazines remained, as these fed into the mainstream population's desire for mindless entertainment). At the same time, advances in technology resulted in nearly all buildings being made out of fireproof materials, and the traditional role of firemen in preventing fires was no longer necessary. The government instead turned the firemen into officers of society's peace of mind: instead of putting out fires they became responsible for starting them, specifically for the purpose of burning books, which were condemned as sources of confusing and depressing thoughts that only complicated people's lives. After an awkward encounter between Mildred and Montag over the book hidden under Montag's pillow, Beatty becomes suspicious and casually adds a passing threat as he leaves, telling Montag that if a fireman had a book, he would be asked to burn it within the next 24 hours. If he refused, the other firemen would come and burn it for him. The encounter leaves Montag shaken.
After Beatty leaves, Montag reveals to Mildred that, over the last year, he has accumulated a stash of books that he has kept hidden in the air-conditioning duct in their ceiling. In a panic, Mildred grabs a book and rushes to throw it in the kitchen incinerator. Montag subdues her and tells her that the two of them are going to read the books to see if they have value. If they do not, he promises the books will be burned and all will return to normal.
Montag and Mildred discuss the stolen books, and Mildred refuses to go along with it, questioning why she or anyone else should care about books. Montag goes on a rant about Mildred's suicide attempt, Clarisse's disappearance and death, the old woman who burned herself, and the imminent threat of war that goes ignored by the masses. He suggests that perhaps the books of the past have messages that can save society from its own destruction. The conversation is interrupted by a call from Mildred's friend, Mrs. Bowles, and they set up a date to watch the "parlor walls" that night at Mildred's house.
Montag concedes that Mildred is a lost cause and he will need help to understand the books. He remembers an old man named Faber, an English professor before books were banned, whom he once met in a park. Montag makes a subway trip to Faber's home along with a rare copy of the Bible, the book he stole at the woman's house. Once there, Montag forces the scared and reluctant Faber into helping him by methodically ripping pages from the Bible. Faber concedes and gives Montag a homemade ear-piece communicator so he can offer constant guidance.
At home, Mildred's friends, Mrs. Bowles and Mrs. Phelps, arrive to watch the "parlor walls". Not interested in this insipid entertainment, Montag turns off the walls and tries to engage the women in meaningful conversation, only for them to reveal just how indifferent, ignorant, and callous they truly are. Enraged by their idiocy, Montag leaves momentarily and returns with a book of poetry. This confuses the women and alarms Faber, who is listening remotely. Mildred tries to dismiss Montag's actions as a tradition firemen act out once a year: they find an old book and read it as a way to make fun of how silly the past is. Montag proceeds to recite the poem "Dover Beach", causing Mrs. Phelps to cry. At the behest of Faber in the ear-piece, Montag burns the book. Mildred's friends leave in disgust, while Mildred locks herself in the bathroom and attempts to kill herself again by overdosing on sleeping pills.
Montag hides his books in the backyard before returning to the firehouse late at night, where he finds Beatty playing cards with the other firemen. Montag hands Beatty a book to cover for the one he believes Beatty knows he stole the night before, which is unceremoniously tossed into the trash. Beatty tells Montag that he had a dream in which they fought endlessly by quoting books to each other. Thus Beatty reveals that, despite his disillusionment, he was once an enthusiastic reader. A fire alarm sounds, and Beatty picks up the address from the dispatcher system. They drive recklessly in the fire truck to the destination: Montag's house.
Beatty orders Montag to destroy his own house with a flamethrower, rather than the more powerful "salamander" that is usually used by the fire team, and tells him that his wife and her friends reported him after what happened the other night. Montag watches as Mildred walks out of the house, too traumatized about losing her parlor wall family to even acknowledge her husband's existence or the situation going on around her, and catches a taxi. Montag obeys the chief, destroying the home piece by piece, but Beatty discovers Montag's earpiece and plans to hunt down Faber. Montag threatens Beatty with the flamethrower and, after Beatty taunts him, Montag burns Beatty alive and knocks his co-workers unconscious. As Montag escapes the scene, the Mechanical Hound attacks him, managing to inject his leg with a tranquilizer. He destroys the Hound with the flamethrower and limps away. Before he escapes, however, he realizes that Beatty had wanted to die a long time ago and had purposely goaded Montag as well as provided him with a weapon.
Montag runs through the city streets towards Faber's house. On his way, he crosses a wide road as a speeding car attempts to run him over, but he manages to evade the vehicle, and realizes he almost suffered the same fate as Clarisse. Faber urges him to make his way to the countryside and contact the exiled book-lovers who live there. He mentions he will be leaving on an early bus heading to St. Louis and that he and Montag can rendezvous there later. On Faber's television, they watch news reports of another Mechanical Hound being released to track down and kill Montag, with news helicopters following it to create a public spectacle. After wiping his scent from around the house in hopes of thwarting the Hound, Montag leaves Faber's house. He escapes the manhunt by wading into a river and floating downstream. Montag leaves the river in the countryside, where he meets the exiled drifters, led by a man named Granger. Granger shows Montag the ongoing manhunt on a portable battery TV and predicts that “Montag” will be caught within the next few minutes; as predicted, an innocent man is then caught and killed.
The drifters are all former intellectuals. They have each memorized books should the day arrive that society comes to an end and is forced to rebuild itself anew, with the survivors learning to embrace the literature of the past. Granger asks Montag what he has to contribute to the group and Montag finds that he had partially memorized the Book of Ecclesiastes, discovering that the group has a special way of unlocking photographic memory. While learning the philosophy of the exiles, Montag and the group watch helplessly as bombers fly overhead and annihilate the city with nuclear weapons: the imminent war has begun and ended in the same night. While Faber would have left on the early bus, everyone else (including Mildred) is immediately killed. Montag and the group are injured and dirtied, but manage to survive the shockwave.
The following morning, Granger teaches Montag and the others about the legendary phoenix and its endless cycle of long life, death in flames, and rebirth. He adds that the phoenix must have some relationship to mankind, which constantly repeats its mistakes, but explains that man has something the phoenix does not: mankind can remember its mistakes and try to never repeat them. Granger then muses that a large factory of mirrors should be built so that people can take a long look at themselves and reflect on their lives. When the meal is over, the exiles return to the city to rebuild society.
The title page of the book explains the title as follows: "Fahrenheit 451—The temperature at which book paper catches fire and burns...". On inquiring about the temperature at which paper would catch fire, Bradbury had been told that was the autoignition temperature of paper. In various studies, scientists have placed the autoignition temperature at a range of temperatures between , depending on the type of paper.
Bradbury's lifelong passion for books began at an early age. After graduating from high school, Bradbury's family could not afford for him to attend college so Bradbury began spending time at the Los Angeles Public Library where he essentially educated himself. As a frequent visitor to his local libraries in the 1920s and 1930s, he recalls being disappointed because they did not stock popular science fiction novels, like those of H. G. Wells, because, at the time, they were not deemed literary enough. Between this and learning about the destruction of the Library of Alexandria, a great impression was made on the young man about the vulnerability of books to censure and destruction. Later, as a teenager, Bradbury was horrified by the Nazi book burnings and later by Joseph Stalin's campaign of political repression, the "Great Purge", in which writers and poets, among many others, were arrested and often executed.
Shortly after the atomic bombings of Hiroshima and Nagasaki at the conclusion of World War II, the United States focused its concern on the Soviet atomic bomb project and the expansion of communism. The House Un-American Activities Committee (HUAC), formed in 1938 to investigate American citizens and organizations suspected of having communist ties, held hearings in 1947 to investigate alleged communist influence in Hollywood movie-making. These hearings resulted in the blacklisting of the so-called "Hollywood Ten", a group of influential screenwriters and directors. This governmental interference in the affairs of artists and creative types greatly angered Bradbury. Bradbury was bitter and concerned about the workings of his government, and a late 1949 nighttime encounter with an overzealous police officer would inspire Bradbury to write "The Pedestrian", a short story which would go on to become "The Fireman" and then "Fahrenheit 451". The rise of Senator Joseph McCarthy's hearings hostile to accused communists, beginning in 1950, deepened Bradbury's contempt for government overreach.
The year HUAC began investigating Hollywood is often considered the beginning of the Cold War, as in March 1947, the Truman Doctrine was announced. By about 1950, the Cold War was in full swing, and the American public's fear of nuclear warfare and communist influence was at a feverish level. The stage was set for Bradbury to write the dramatic nuclear holocaust ending of "Fahrenheit 451", exemplifying the type of scenario feared by many Americans of the time.
Bradbury's early life witnessed the Golden Age of Radio, while the transition to the Golden Age of Television began right around the time he started to work on the stories that would eventually lead to "Fahrenheit 451". Bradbury saw these forms of media as a threat to the reading of books, indeed as a threat to society, as he believed they could act as a distraction from important affairs. This contempt for mass media and technology would express itself through Mildred and her friends and is an important theme in the book.
"Fahrenheit 451" developed out of a series of ideas Bradbury had visited in previously written stories. For many years, he tended to single out "The Pedestrian" in interviews and lectures as sort of a proto-"Fahrenheit 451". In the Preface of his 2006 anthology "Match to Flame: The Fictional Paths to Fahrenheit 451" he states that this is an oversimplification. The full genealogy of "Fahrenheit 451" given in "Match to Flame" is involved. The following covers the most salient aspects.
Between 1947 and 1948, Bradbury wrote the short story "Bright Phoenix" (not published until the May 1963 issue of "The Magazine of Fantasy & Science Fiction") about a librarian who confronts a book-burning "Chief Censor" named Jonathan Barnes.
In late 1949, Bradbury was stopped and questioned by a police officer while walking late one night. When asked "What are you doing?", Bradbury wisecracked, "Putting one foot in front of another." This incident inspired Bradbury to write the 1951 short story "The Pedestrian".
In "The Pedestrian", Leonard Mead is harassed and detained by the city's remotely operated police cruiser (there's only one) for taking nighttime walks, something that has become extremely rare in this future-based setting: everybody else stays inside and watches television ("viewing screens"). Alone and without an alibi, Mead is taken to the "Psychiatric Center for Research on Regressive Tendencies" for his peculiar habit. "Fahrenheit 451" would later echo this theme of an authoritarian society distracted by broadcast media.
Bradbury expanded the book-burning premise of "Bright Phoenix" and the totalitarian future of "The Pedestrian" into "The Fireman", a novella published in the February 1951 issue of "Galaxy Science Fiction". "The Fireman" was written in the basement of UCLA's Powell Library on a typewriter that he rented for a fee of ten cents per half hour. The first draft was 25,000 words long and was completed in nine days.
Urged by a publisher at Ballantine Books to double the length of his story to make a novel, Bradbury returned to the same typing room and expanded his work into "Fahrenheit 451", again taking just nine days. The fixup was published by Ballantine in 1953.
Bradbury has supplemented the novel with various front and back matter, including a 1979 coda, a 1982 afterword, a 1993 foreword, and several introductions.
The first U.S. printing was a paperback version from October 1953 by The Ballantine Publishing Group. Shortly after the paperback, a hardback version was released that included a special edition of 200 signed and numbered copies bound in asbestos. These were technically collections because the novel was published with two short stories: "The Playground" and "And the Rock Cried Out", which have been absent in later printings. A few months later, the novel was serialized in the March, April, and May 1954 issues of nascent "Playboy" magazine.
Starting in January 1967, "Fahrenheit 451" was subject to expurgation by its publisher, Ballantine Books with the release of the "Bal-Hi Edition" aimed at high school students. Among the changes made by the publisher were the censorship of the words "hell", "damn", and "abortion"; the modification of seventy-five passages; and the changing of two episodes.
In the one case, a drunk man became a "sick man" while cleaning fluff out of a human navel became "cleaning ears" in the other. For a while both the censored and uncensored versions were available concurrently but by 1973 Ballantine was publishing only the censored version. This continued until 1979 when it came to Bradbury's attention:
In 1979, one of Bradbury's friends showed him an expurgated copy. Bradbury demanded that Ballantine Books withdraw that version and replace it with the original, and in 1980 the original version once again became available. In this reinstated work, in the Author's Afterword, Bradbury relates to the reader that it is not uncommon for a publisher to expurgate an author's work, but he asserts that he himself will not tolerate the practice of manuscript "mutilation".
The "Bal-Hi" editions are now referred to by the publisher as the "Revised Bal-Hi" editions.
An audiobook version read by Bradbury himself was released in 1976 and received a Spoken Word Grammy nomination. Another audiobook was released in 2005 narrated by Christopher Hurt. The e-book version was released in December 2011.
In 1954, "Galaxy Science Fiction" reviewer Groff Conklin placed the novel "among the great works of the imagination written in English in the last decade or more." The "Chicago Sunday Tribune"'s August Derleth described the book as "a savage and shockingly prophetic view of one possible future way of life", calling it "compelling" and praising Bradbury for his "brilliant imagination". Over half a century later, Sam Weller wrote, "upon its publication, "Fahrenheit 451" was hailed as a visionary work of social commentary." Today, "Fahrenheit 451" is still viewed as an important cautionary tale about conformity and the evils of government censorship.
When the novel was first published, there were those who did not find merit in the tale. Anthony Boucher and J. Francis McComas were less enthusiastic, faulting the book for being "simply padded, occasionally with startlingly ingenious gimmickry, ... often with coruscating cascades of verbal brilliance [but] too often merely with words." Reviewing the book for "Astounding Science Fiction", P. Schuyler Miller characterized the title piece as "one of Bradbury's bitter, almost hysterical diatribes," while praising its "emotional drive and compelling, nagging detail." Similarly, "The New York Times" was unimpressed with the novel and further accused Bradbury of developing a "virulent hatred for many aspects of present-day culture, namely, such monstrosities as radio, TV, most movies, amateur and professional sports, automobiles, and other similar aberrations which he feels debase the bright simplicity of the thinking man's existence."
"Fahrenheit 451" was number seven on the list of "Top Check Outs OF ALL TIME" by the New York Public Library
In the years since its publication, "Fahrenheit 451" has occasionally been banned, censored, or redacted in some schools by parents and teaching staff either unaware of or indifferent to the inherent irony in such censorship. The following are some notable incidents:
Discussions about "Fahrenheit 451" often center on its story foremost as a warning against state-based censorship. Indeed, when Bradbury wrote the novel during the McCarthy era, he was concerned about censorship in the United States. During a radio interview in 1956, Bradbury said:I wrote this book at a time when I was worried about the way things were going in this country four years ago. Too many people were afraid of their shadows; there was a threat of book burning. Many of the books were being taken off the shelves at that time. And of course, things have changed a lot in four years. Things are going back in a very healthy direction. But at the time I wanted to do some sort of story where I could comment on what would happen to a country if we let ourselves go too far in this direction, where then all thinking stops, and the dragon swallows his tail, and we sort of vanish into a limbo and we destroy ourselves by this sort of action.
As time went by, Bradbury tended to dismiss censorship as a chief motivating factor for writing the story. Instead he usually claimed that the real messages of "Fahrenheit 451" were about the dangers of an illiterate society infatuated with mass media and the threat of minority and special interest groups to books. In the late 1950s, Bradbury recounted:In writing the short novel "Fahrenheit 451", I thought I was describing a world that might evolve in four or five decades. But only a few weeks ago, in Beverly Hills one night, a husband and wife passed me, walking their dog. I stood staring after them, absolutely stunned. The woman held in one hand a small cigarette-package-sized radio, its antenna quivering. From this sprang tiny copper wires which ended in a dainty cone plugged into her right ear. There she was, oblivious to man and dog, listening to far winds and whispers and soap-opera cries, sleep-walking, helped up and down curbs by a husband who might just as well not have been there. This was "not" fiction.
This story echoes Mildred's "Seashell ear-thimbles" (i.e., a brand of in-ear headphones) that act as an emotional barrier between her and Montag. In a 2007 interview, Bradbury maintained that people misinterpret his book and that "Fahrenheit 451" is really a statement on how mass media like television marginalizes the reading of literature. Regarding minorities, he wrote in his 1979 Coda:There is more than one way to burn a book. And the world is full of people running about with lit matches. Every minority, be it Baptist/Unitarian, Irish/Italian/Octogenarian/Zen Buddhist, Zionist/Seventh-day Adventist, Women's Lib/Republican, Mattachine/Four Square Gospel feels it has the will, the right, the duty to douse the kerosene, light the fuse. [...] Fire-Captain Beatty, in my novel "Fahrenheit 451", described how the books were burned first by minorities, each ripping a page or a paragraph from this book, then that, until the day came when the books were empty and the minds shut and the libraries closed forever. [...] Only six weeks ago, I discovered that, over the years, some cubby-hole editors at Ballantine Books, fearful of contaminating the young, had, bit by bit, censored some seventy-five separate sections from the novel. Students, reading the novel, which, after all, deals with censorship and book-burning in the future, wrote to tell me of this exquisite irony. Judy-Lynn del Rey, one of the new Ballantine editors, is having the entire book reset and republished this summer with all the damns and hells back in place.
Book-burning censorship, Bradbury would argue, was a side-effect of these two primary factors; this is consistent with Captain Beatty's speech to Montag about the history of the firemen. According to Bradbury, it is the people, not the state, who are the culprit in "Fahrenheit 451". Nevertheless, the role of censorship, state-based or otherwise, is still perhaps the most frequent theme explored in the work.
A variety of other themes in the novel besides censorship have been suggested. Two major themes are resistance to conformity and control of individuals via technology and mass media. Bradbury explores how the government is able to use mass media to influence society and suppress individualism through book burning. The characters Beatty and Faber point out that the American population is to blame. Due to their constant desire for a simplistic, positive image, books must be suppressed. Beatty blames the minority groups, who would take offense to published works that displayed them in an unfavorable light. Faber went further to state that the American population simply stopped reading on their own. He notes that the book burnings themselves became a form of entertainment for the general public.
In a 1994 interview, Bradbury stated that "Fahrenheit 451" was more relevant during this time than in any other, stating that, "it works even better because we have political correctness now. Political correctness is the real enemy these days. The black groups want to control our thinking and you can't say certain things. The homosexual groups don’t want you to criticize them. It's thought control and freedom of speech control."
Bradbury described himself as "a "preventor" of futures, not a predictor of them." He did not believe that book burning was an inevitable part of the future; he wanted to warn against its development. In a later interview, when asked if he believes that teaching "Fahrenheit 451" in schools will prevent his totalitarian vision of the future, Bradbury replied in the negative. Rather, he states that education must be at the kindergarten and first-grade level. If students are unable to read then, they will be unable to read "Fahrenheit 451".
On account of technology, Sam Weller notes that Bradbury "predicted everything from flat-panel televisions to earbud headphones and twenty-four-hour banking machines."
"Playhouse 90" broadcast "A Sound of Different Drummers" on CBS in 1957, written by Robert Alan Aurthur. The play combined plot ideas from "Fahrenheit 451" and "Nineteen Eighty-Four". Bradbury sued and eventually won on appeal.
A film adaptation written and directed by François Truffaut and starring Oskar Werner and Julie Christie was released in 1966.
A new film adaptation directed by Ramin Bahrani and starring Michael B. Jordan, Michael Shannon, Sofia Boutella, and Lilly Singh was released in 2018 for HBO.
In the late 1970s Bradbury adapted his book into a play. At least part of it was performed at the Colony Theatre in Los Angeles in 1979, but it was not in print until 1986 and the official world premiere was only in November 1988 by the Fort Wayne, Indiana Civic Theatre. The stage adaptation diverges considerably from the book and seems influenced by Truffaut's movie. For example, fire chief Beatty's character is fleshed out and is the wordiest role in the play. As in the movie, Clarisse does not simply disappear but in the finale meets up with Montag as a book character (she as Robert Louis Stevenson, he as Edgar Allan Poe).
The UK premiere of Bradbury's stage adaptation was not until 2003 in Nottingham, while it took until 2006 before the Godlight Theatre Company produced and performed its New York City premiere at 59E59 Theaters. After the completion of the New York run, the production then transferred to the Edinburgh Festival where it was a 2006 Edinburgh Festival "Pick of the Fringe".
The Off-Broadway theatre The American Place Theatre presented a one man show adaptation of "Fahrenheit 451" as a part of their 2008–2009 Literature to Life season.
"Fahrenheit 451" inspired the Birmingham Repertory Theatre production "Time Has Fallen Asleep in the Afternoon Sunshine", which was performed at the Birmingham Central Library in April 2012.
BBC Radio produced a dramatization by Gregory Evans of the novel in 1982, starring Michael Pennington as Montag. It was broadcast again on February 12, 2012, and April 7 and 8, 2013, on BBC Radio 4 Extra.
A second BBC adaptation, this one by David Calcutt, was broadcast on BBC Radio 4 in 2003, starring Stephen Tomlin, Christian Rodska, Sunny Ormonde and Tracey Wiles.
In 1984, the novel was adapted into a computer text adventure game of the same name by the software company Trillium.
In June 2009, a graphic novel edition of the book was published. Entitled "Ray Bradbury's Fahrenheit 451: The Authorized Adaptation", the paperback graphic adaptation was illustrated by Tim Hamilton. The introduction in the novel is written by Bradbury.
Michael Moore's 2004 documentary "Fahrenheit 9/11" refers to Bradbury's novel and the September 11 attacks, emphasized by the film's tagline "The temperature where freedom burns". The film takes a critical look at the presidency of George W. Bush, the War on Terror, and its coverage in the news media, and became the highest grossing documentary of all time. Bradbury, a conservative, was upset by what he considered the appropriation of his title, and wanted the film renamed. Moore filmed a subsequent documentary about the election of Donald Trump called "Fahrenheit 11/9" in 2018.
The 1994 video game "System Shock" uses the numerical code "0451" for one of the first locked doors the player experiences in the game, specifically in reference to "Fahrenheit 451". In further immersive sims that follow the same gameplay style of "System Shock", the "0451" is frequently reused at the first code the player encounters referencing its first appearance in "System Shock".
In 2015, the Internet Engineering Steering Group approved the publication of "An HTTP Status Code to Report Legal Obstacles", now RFC 7725, which specifies that websites forced to block resources for legal reasons should return a status code of 451 when users request those resources.
"Centigrade 232" is a poem by Robert Calvert, published in a 1977 book and released as an album in 2007. The title alludes to "Fahrenheit 451" by its metric equivalent, "signifying the writer destroying his rough drafts".
In book 14 of the "Guardians of Ga'Hoole" series by Kathryn Lasky, published in 2008 and entitled , Braithe states that the name of the Place of Living Books, also called "Brad", comes from an author's name: "The author's full name is not known. We call him Ray Brad. We think it's only scraps of his name but what is important is that "he wrote about book burning"", thus referencing "Fahrenheit 451" and Ray Bradbury.
In the 2019 film "Escape Room", the title of the book is used as a false clue in the first stage that causes the room to heat up to the namesake temperature gradually, pressuring players to find a way out or face a fiery death.
In 2017, the literary book, "The Bookshop", was made into a movie, and one of the characters who read "Fahrenheit 451" wrote to the bookshop owner, requesting that she send him more books from Ray Bradbury, rather than books on poems and romance.

</doc>
<doc id="10957" url="https://en.wikipedia.org/wiki?curid=10957" title="Francis Xavier">
Francis Xavier

Francis Xavier (born Francisco de Jasso y Azpilicueta; Latin "Franciscus Xaverius"; Basque: "Frantzisko Xabierkoa"; Spanish: "Francisco Javier"; Portuguese: "Francisco Xavier"; April 7, 1506December 3, 1552), venerated as Saint Francis Xavier, was a Spanish Catholic priest, missionary and saint from Navarre who was the co-founder of the Society of Jesus.
Born in Javier (Xavier in Old Spanish and in Navarro-Aragonese), Kingdom of Navarre (in present-day Spain), he was a companion of Ignatius of Loyola and one of the first seven Jesuits who took vows of poverty and chastity at Montmartre, Paris, in 1534. He led an extensive mission into Asia, mainly in the Portuguese Empire of the time and was influential in evangelization work, most notably in India. Although some sources claim that the Goa Inquisition was proposed by Francis Xavier, his letter to the king of Portugal, John III, asked for a special minister whose sole office would be to further Christianity in Goa. He also was the first Christian missionary to venture into Japan, Borneo, the Maluku Islands, and other areas. In those areas, struggling to learn the local languages and in the face of opposition, he had less success than he had enjoyed in India. Xavier was about to extend his missionary preaching to China when he died on Shangchuan Island.
He was beatified by Pope Paul V on 25 October 1619 and canonized by Pope Gregory XV on 12 March 1622. In 1624 he was made co-patron of Navarre. Known as the "Apostle of the Indies" and "Apostle of Japan", he is considered to be one of the greatest missionaries since Paul the Apostle. In 1927, Pope Pius XI published the decree "Apostolicorum in Missionibus" naming Francis Xavier, along with Thérèse of Lisieux, co-patron of all foreign missions. He is now co-patron saint of Navarre with Fermin. The Day of Navarre in Navarre, Spain, marks the anniversary of Francis Xavier's death, on 3 December 1552.
Francis Xavier was born in the royal castle of Xavier, in the Kingdom of Navarre, on 7 April 1506 according to a family register. He was the youngest son of Juan de Jasso y Atondo, seneschal of Xavier castle, who belonged to a prosperous farming family and had acquired a doctorate in law at the University of Bologna. Basque and Romance were his two mother tongues. Juan later became privy counsellor and finance minister to King John III of Navarre (Jean d'Albret). Francis's mother was Doña María de Azpilcueta y Aznárez, sole heiress of two noble Navarrese families. He was through her related to the great theologian and philosopher Martín de Azpilcueta.
In 1512, Ferdinand, King of Aragon and regent of Castile, invaded Navarre, initiating a war that lasted over 18 years. Three years later, Francis's father died when Francis was only nine years old. In 1516, Francis's brothers participated in a failed Navarrese-French attempt to expel the Spanish invaders from the kingdom. The Spanish Governor, Cardinal Cisneros, confiscated the family lands, demolished the outer wall, the gates, and two towers of the family castle, and filled in the moat. In addition, the height of the keep was reduced by half. Only the family residence inside the castle was left. In 1522 one of Francis's brothers participated with 200 Navarrese nobles in dogged but failed resistance against the Castilian Count of Miranda in Amaiur, Baztan, the last Navarrese territorial position south of the Pyrenees.
In 1525, Francis went to study in Paris at the Collège Sainte-Barbe, University of Paris, where he would spend the next eleven years. In the early days he acquired some reputation as an athlete and a high-jumper.
In 1529, Francis shared lodgings with his friend Pierre Favre. A new student, Ignatius of Loyola, came to room with them. At 38, Ignatius was much older than Pierre and Francis, who were both 23 at the time. Ignatius convinced Pierre to become a priest, but was unable to convince Francis, who had aspirations of worldly advancement. At first Francis regarded the new lodger as a joke and was sarcastic about his efforts to convert students. When Pierre left their lodgings to visit his family and Ignatius was alone with Francis, he was able to slowly break down Francis's resistance. According to most biographies Ignatius is said to have posed the question: "What will it profit a man to gain the whole world, and lose his own soul?" However, according to James Broderick such method is not characteristic of Ignatius and there is no evidence that he employed it at all.
In 1530 Francis received the degree of Master of Arts, and afterwards taught Aristotelian philosophy at Beauvais College, University of Paris.
On 15 August 1534, seven students met in a crypt beneath the Church of Saint Denis (now Saint Pierre de Montmartre), on the hill of Montmartre, overlooking Paris. They were Francis, Ignatius of Loyola, Alfonso Salmeron, Diego Laínez, Nicolás Bobadilla from Spain, Peter Faber from Savoy, and Simão Rodrigues from Portugal. They made private vows of poverty, chastity, and obedience to the Pope, and also vowed to go to the Holy Land to convert infidels. Francis began his study of theology in 1534 and was ordained on 24 June 1537.
In 1539, after long discussions, Ignatius drew up a formula for a new religious order, the Society of Jesus (the Jesuits). Ignatius's plan for the order was approved by Pope Paul III in 1540.
In 1540 King John of Portugal had Pedro Mascarenhas, Portuguese ambassador to the Holy See, request Jesuit missionaries to spread the faith in his new possessions in India, where the king believed that Christian values were eroding among the Portuguese. After successive appeals to the Pope asking for missionaries for the East Indies under the Padroado agreement, John III was encouraged by Diogo de Gouveia, rector of the Collège Sainte-Barbe, to recruit the newly graduated students that would establish the Society of Jesus.
Ignatius promptly appointed Nicholas Bobadilla and Simão Rodrigues. At the last moment, however, Bobadilla became seriously ill. With some hesitance and uneasiness, Ignatius asked Francis to go in Bobadilla's place. Thus, Francis Xavier began his life as the first Jesuit missionary almost accidentally.
Leaving Rome on 15 March 1540, in the Ambassador's train, Francis took with him a breviary, a catechism, and by Croatian humanist Marko Marulić, a Latin book that had become popular in the Counter-Reformation. According to a 1549 letter of F. Balthasar Gago from Goa, it was the only book that Francis read or studied. Francis reached Lisbon in June 1540 and, four days after his arrival, he and Rodrigues were summoned to a private audience with the King and the Queen.
Francis Xavier devoted much of his life to missions in Asia, mainly in four centres: Malacca, Amboina and Ternate, Japan, and off-shore China. His growing information about new places indicated to him that he had to go to what he understood were centres of influence for the whole region. China loomed large from his days in India. Japan was particularly attractive because of its culture. For him, these areas were interconnected; they could not be evangelised separately.
Francis Xavier left Lisbon on 7 April 1541, his thirty-fifth birthday, along with two other Jesuits and the new viceroy Martim Afonso de Sousa, on board the "Santiago". As he departed, Francis was given a brief from the pope appointing him apostolic nuncio to the East. From August until March 1542 he remained in Portuguese Mozambique, and arrived in Goa, then capital of Portuguese India, on 6 May 1542, thirteen months after leaving Lisbon.
The Portuguese, following quickly on the great voyages of discovery, had established themselves at Goa thirty years earlier. Francis's primary mission, as ordered by King John III, was to restore Christianity among the Portuguese settlers. According to Teotonio R. DeSouza, recent critical accounts indicate that apart from the posted civil servants, "the great majority of those who were dispatched as 'discoverers' were the riff-raff of Portuguese society, picked up from Portuguese jails." Nor did the soldiers, sailors, or merchants come to do missionary work, and Imperial policy permitted the outflow of disaffected nobility. Many of the arrivals formed liaisons with local women and adopted Indian culture. Missionaries often wrote against the "scandalous and undisciplined" behaviour of their fellow Christians.
The Christian population had churches, clergy, and a bishop, but there were few preachers and no priests beyond the walls of Goa. The Velliapura family of Velim, Goa, of the St Thomas Christians sect, welcomed the missionaries. Xavier decided that he must begin by instructing the Portuguese themselves, and gave much of his time to the teaching of children. The first five months he spent in preaching and ministering to the sick in the hospitals. After that, he walked through the streets ringing a bell to summon the children and servants to catechism. He was invited to head Saint Paul's College, a pioneer seminary for the education of secular priests, which became the first Jesuit headquarters in Asia.
Xavier soon learned that along the Pearl Fishery Coast, which extends from Cape Comorin on the southern tip of India to the island of Mannar, off Ceylon (Sri Lanka), there was a Jāti of people called Paravas. Many of them had been baptised ten years before, merely to please the Portuguese who had helped them against the Moors, but remained uninstructed in the faith. Accompanied by several native clerics from the seminary at Goa, he set sail for Cape Comorin in October 1542. He taught those who had already been baptised, and preached to those who weren't. His efforts with the high-caste Brahmins remained unavailing.
He devoted almost three years to the work of preaching to the people of southern India and Ceylon, converting many. He built nearly 40 churches along the coast, including St. Stephen's Church, Kombuthurai, mentioned in his letters dated 1544.
During this time, he was able to visit the tomb of Thomas the Apostle in Mylapore (now part of Madras/Chennai then in Portuguese India). He set his sights eastward in 1545 and planned a missionary journey to Makassar on the island of Celebes (today's Indonesia).
As the first Jesuit in India, Francis had difficulty achieving much success in his missionary trips. His successors, such as de Nobili, Matteo Ricci, and Beschi, attempted to convert the noblemen first as a means to influence more people, while Francis had initially interacted most with the lower classes; (later though, in Japan, Francis changed tack by paying tribute to the Emperor and seeking an audience with him).
In the spring of 1545 Xavier started for Portuguese Malacca. He laboured there for the last months of that year. About January 1546, Xavier left Malacca for the Maluku Islands, where the Portuguese had some settlements. For a year and a half he preached the Gospel there. He went first to Ambon Island, where he stayed until mid-June. He then visited other Maluku Islands, including Ternate, Baranura, and Morotai. Shortly after Easter 1547, he returned to Ambon Island; a few months later he returned to Malacca.
In Malacca in December 1547, Francis Xavier met a Japanese man named Anjirō. Anjirō had heard of Francis in 1545 and had travelled from Kagoshima to Malacca to meet him. Having been charged with murder, Anjirō had fled Japan. He told Francis extensively about his former life, and the customs and culture of his homeland. Anjirō became the first Japanese Christian and adopted the name of 'Paulo de Santa Fé'. He later helped Xavier as a mediator and interpreter for the mission to Japan that now seemed much more possible.
In January 1548 Francis returned to Goa to attend to his responsibilities as superior of the mission there. The next 15 months were occupied with various journeys and administrative measures. He left Goa on 15 April 1549, stopped at Malacca, and visited Canton. He was accompanied by Anjiro, two other Japanese men, Father Cosme de Torrès and Brother Juan Fernández. He had taken with him presents for the "King of Japan" since he was intending to introduce himself as the Apostolic Nuncio.
Europeans had already come to Japan: the Portuguese had landed in 1543 on the island of Tanegashima, where they introduced matchlock firearms to Japan.
From Amboina, he wrote to his companions in Europe: "I asked a Portuguese merchant, ... who had been for many days in Anjirō's country of Japan, to give me ... some information on that land and its people from what he had seen and heard. ...All the Portuguese merchants coming from Japan tell me that if I go there I shall do great service for God our Lord, more than with the pagans of India, for they are a very reasonable people. (To His Companions Residing in Rome, From Cochin, 20 January 1548, no. 18, p. 178).
Francis Xavier reached Japan on 27 July 1549, with Anjiro and three other Jesuits, but he was not permitted to enter any port his ship arrived at until 15 August, when he went ashore at Kagoshima, the principal port of Satsuma Province on the island of Kyūshū. As a representative of the Portuguese king, he was received in a friendly manner. Shimazu Takahisa (1514–1571), "daimyō" of Satsuma, gave a friendly reception to Francis on 29 September 1549, but in the following year he forbade the conversion of his subjects to Christianity under penalty of death; Christians in Kagoshima could not be given any catechism in the following years. The Portuguese missionary Pedro de Alcáçova would later write in 1554:
He was hosted by Anjirō's family until October 1550. From October to December 1550, he resided in Yamaguchi. Shortly before Christmas, he left for Kyoto but failed to meet with the Emperor. He returned to Yamaguchi in March 1551, where the daimyo of the province gave him permission to preach. However, lacking fluency in the Japanese language, he had to limit himself to reading aloud the translation of a catechism.
Francis was the first Jesuit to go to Japan as a missionary. He brought with him paintings of the Madonna and the Madonna and Child. These paintings were used to help teach the Japanese about Christianity. There was a huge language barrier as Japanese was unlike other languages the missionaries had previously encountered. For a long time Francis struggled to learn the language.
Having learned that evangelical poverty did not have the appeal in Japan that it had in Europe and in India, he decided to change his approach. Hearing after a time that a Portuguese ship had arrived at a port in the province of Bungo in Kyushu and that the prince there would like to see him, Xavier now set out southward. The Jesuit, in a fine cassock, surplice, and stole, was attended by thirty gentlemen and as many servants, all in their best clothes. Five of them bore on cushions valuable articles, including a portrait of Our Lady and a pair of velvet slippers, these not gifts for the prince, but solemn offerings to Xavier, to impress the onlookers with his eminence. Handsomely dressed, with his companions acting as attendants, he presented himself before Oshindono, the ruler of Nagate, and as a representative of the great kingdom of Portugal, offered him letters and presents: a musical instrument, a watch, and other attractive objects which had been given him by the authorities in India for the emperor.
For forty-five years the Jesuits were the only missionaries in Asia, but the Franciscans also began proselytising in Asia as well. Christian missionaries were later forced into exile, along with their assistants. Some were able to stay behind, however Christianity was then kept underground so as to not be persecuted.
The Japanese people were not easily converted; many of the people were already Buddhist or Shinto. Francis tried to combat the disposition of some of the Japanese that a God who had created everything, including evil, could not be good. The concept of Hell was also a struggle; the Japanese were bothered by the idea of their ancestors living in Hell. Despite Francis's different religion, he felt that they were good people, much like Europeans, and could be converted.
Xavier was welcomed by the Shingon monks since he used the word "Dainichi" for the Christian God; attempting to adapt the concept to local traditions. As Xavier learned more about the religious nuances of the word, he changed to "Deusu" from the Latin and Portuguese "Deus". The monks later realised that Xavier was preaching a rival religion and grew more aggressive towards his attempts at conversion.
With the passage of time, his sojourn in Japan could be considered somewhat fruitful as attested by congregations established in Hirado, Yamaguchi, and Bungo. Xavier worked for more than two years in Japan and saw his successor-Jesuits established. He then decided to return to India. Historians debate the exact path he returned by, but from evidence attributed to the captain of his ship, he may have travelled through Tanegeshima and Minato, and avoided Kagoshima because of the hostility of the daimyo.
During his trip from Japan back to India, a tempest forced him to stop on an island near Guangzhou, Guangdong, China, where he met Diogo Pereira, a rich merchant and an old friend from Cochin. Pereira showed him a letter from Portuguese prisoners in Guangzhou, asking for a Portuguese ambassador to speak to the Chinese Emperor on their behalf. Later during the voyage, he stopped at Malacca on 27 December 1551, and was back in Goa by January 1552.
On 17 April he set sail with Diogo Pereira on the "Santa Cruz" for China. He planned to introduce himself as Apostolic Nuncio and Pereira as ambassador of the King of Portugal. But then he realized that he had forgotten his testimonial letters as an Apostolic Nuncio. Back in Malacca, he was confronted by the captain Álvaro de Ataíde da Gama who now had total control over the harbour. The captain refused to recognize his title of Nuncio, asked Pereira to resign from his title of ambassador, named a new crew for the ship, and demanded the gifts for the Chinese Emperor be left in Malacca.
In late August 1552, the "Santa Cruz" reached the Chinese island of Shangchuan, 14 km away from the southern coast of mainland China, near Taishan, Guangdong, 200 km south-west of what later became Hong Kong. At this time, he was accompanied only by a Jesuit student, Álvaro Ferreira, a Chinese man called António, and a Malabar servant called Christopher. Around mid-November he sent a letter saying that a man had agreed to take him to the mainland in exchange for a large sum of money. Having sent back Álvaro Ferreira, he remained alone with António. He died from a fever at Shangchuan, Taishan, China, on 3 December 1552, while he was waiting for a boat that would take him to mainland China.
Xavier was first buried on a beach at Shangchuan Island, Taishan, Guangdong. His body was taken from the island in February 1553 and temporarily buried in St. Paul's church in Portuguese Malacca on 22 March 1553. An open grave in the church now marks the place of Xavier's burial. Pereira came back from Goa, removed the corpse shortly after 15 April 1553, and moved it to his house. On 11 December 1553, Xavier's body was shipped to Goa.
The body is now in the Basilica of Bom Jesus in Goa, where it was placed in a glass container encased in a silver casket on 2 December 1637. This casket, constructed by Goan silversmiths between 1636 and 1637, was an exemplary blend of Italian and Indian aesthetic sensibilities. There are 32 silver plates on all the four sides of the casket depicting different episodes from the life of the Xavier:
The right forearm, which Xavier used to bless and baptise his converts, was detached by Superior General Claudio Acquaviva in 1614. It has been displayed since in a silver reliquary at the main Jesuit church in Rome, Il Gesù.
Another of Xavier's arm bones was brought to Macau where it was kept in a silver reliquary. The relic was destined for Japan but religious persecution there persuaded the church to keep it in Macau's Cathedral of St. Paul. It was subsequently moved to St. Joseph's and in 1978 to the Chapel of St. Francis Xavier on Coloane Island. More recently the relic was moved to St. Joseph's Church.
In 2006, on the 500th anniversary of his birth, the Xavier Tomb Monument and Chapel on the Shangchuan Island, in ruins after years of neglect under communist rule in China, was restored with support from the alumni of Wah Yan College, a Jesuit high school in Hong Kong.
From December 2017 to February 2018, Catholic Christian Outreach (CCO) in cooperation with the Jesuits, brought Xavier's right forearm to tour throughout Canada. The faithful, especially university students participating with CCO at Rise Up 2017 in Ottawa, venerated the relics. The tour continued to every city where CCO and/or the Jesuits are present in Canada: Quebec City, St. John's, Halifax, St. Francis Xavier University in Antigonish (neither CCO nor the Jesuits are present here), Kingston, Toronto, Winnipeg, Saskatoon, Regina, Calgary, Vancouver, Victoria, and Montreal before returning to Ottawa. The relic was then returned to Rome with a Mass of Thanksgiving celebrated by Archbishop Terrence Prendergast at the Church of the Gesu.
Francis Xavier was beatified by Paul V on 25 October 1619, and was canonized by Gregory XV on 12 March 1622, at the same time as Ignatius Loyola. Pius XI proclaimed him the "Patron of Catholic Missions". His feast day is 3 December.
Saint Francis Xavier's relics are kept in a silver casket, elevated inside the Bom Jesus Basilica and are exposed (being brought to ground level) generally every ten years, but this is discretionary. The sacred relics went on display starting on 22 November 2014 at the XVII Solemn Exposition. The display closed on 4 January 2015. The previous exposition, the sixteenth, was held from 21 November 2004 to 2 January 2005.
Relics of Saint Francis Xavier are also found in the Espirito Santo (Holy Spirit) Church, Margão, in Sanv Fransiku Xavierachi Igorz (Church of St. Francis Xavier), Batpal, Canacona, Goa, and at St. Francis Xavier Chapel, Portais, Panjim.
Other pilgrimage centres include Xavier's birthplace in Navarra, Church of Il Gesu, Rome, Malacca (where he was buried for 2 years, before being brought to Goa), Sancian (place of death), and more.
Xavier is a major venerated saint in both Sonora and the neighbouring U.S. state of Arizona. In Magdalena de Kino in Sonora, Mexico, in the Church of Santa María Magdalena, there is reclining statue of San Francisco Xavier brought by pioneer Jesuit missionary Padre Eusebio Kino in the early 18th century. The statue is said to be miraculous and is the object of pilgrimage for many in the region. Also Mission San Xavier del Bac is a pilgrimage site. The mission is an active parish church ministering to the people of the San Xavier District, Tohono O'odham Nation, and nearby Tucson, Arizona.
The Novena of Grace is a popular devotion to Francis Xavier, typically prayed either on the nine days before 3 December, or on 4 March through 12 March (the anniversary of Pope Gregory XV's canonisation of Xavier in 1622). It began with the Italian Jesuit missionary Marcello Mastrilli. Before he could travel to the Far East, Mastrilli was gravely injured in a freak accident after a festive celebration dedicated to the Immaculate Conception in Naples. Delirious and on the verge of death, Mastrilli saw Xavier, who he later said asked him to choose between travelling or death by holding the respective symbols, to which Mastrilli answered, "I choose that which God wills." Upon regaining his health, Mastrilli made his way via Goa and the Philippines to Satsuma, Japan. The Tokugawa shogunate beheaded the missionary in October 1637, after undergoing three days of tortures involving the volcanic sulphurous fumes from Mt. Unzen, known as the "Hell mouth" or "pit" that had supposedly caused an earlier missionary to renounce his faith.
Francis Xavier is noteworthy for his missionary work, both as organiser and as pioneer, reputed to have converted more people than anyone else has done since Paul the Apostle. Pope Benedict XVI said of both Ignatius of Loyola and Francis Xavier: "not only their history which was interwoven for many years from Paris and Rome, but a unique desire – a unique passion, it could be said – moved and sustained them through different human events: the passion to give to God-Trinity a glory always greater and to work for the proclamation of the Gospel of Christ to the peoples who had been ignored." By consulting with the earlier ancient Christians of St. Thomas in India, Xavier developed Jesuit missionary methods. His success also spurred many Europeans to join the order, as well as become missionaries throughout the world. His personal efforts most affected Christians in India and the East Indies (Indonesia, Malaysia, Timor). India still has numerous Jesuit missions, and many more schools. Xavier also worked to propagate Christianity in China and Japan. However, following the persecutions of Toyotomi Hideyoshi and the subsequent closing of Japan to foreigners, the Christians of Japan were forced to go underground to develop an independent Christian culture. Likewise, while Xavier inspired many missionaries to China, Chinese Christians also were forced underground and developed their own Christian culture.
A small chapel designed by Achille-Antoine Hermitte was completed in 1869 over Xavier's death place on Shangchuan Island, Canton.
It was damaged and restored several times, with the most recent restoration in 2006 to celebrate the 500th anniversary of his birth. 
Francis Xavier is the patron saint of his native Navarre, which celebrates his feast day on 3 December as a government holiday. In addition to Roman Catholic Masses remembering Xavier on that day (now known as the Day of Navarra), celebrations in the surrounding weeks honour the region's cultural heritage. Furthermore, in the 1940s, devoted Catholics instituted the Javierada, an annual day-long pilgrimage (often on foot) from the capital at Pamplona to Xavier, where the Jesuits have built a basilica and museum and restored his family's castle.
As the foremost saint from Navarre and one of the main Jesuit saints, he is much venerated in Spain and the Hispanic countries where "Francisco Javier" or "Javier" are common male given names. The alternative spelling "Xavier" is also popular in the Basque Country, Portugal, Catalonia, Brazil, France, Belgium, and southern Italy. In India, the spelling "Xavier" is almost always used, and the name is quite common among Christians, especially in Goa and the southern states of Tamil Nadu, Kerala, and Karnataka. The names "Francisco Xavier", "António Xavier", "João Xavier", "Caetano Xavier", "Domingos Xavier" et cetera, were very common till quite recently in Goa. "Fransiskus Xaverius" is commonly used as a name for Indonesian Catholics, usually abbreviated as FX. In Austria and Bavaria the name is spelled as "Xaver" (pronounced (ˈk͡saːfɐ)) and often used in addition to Francis as "Franz-Xaver" (frant͡sˈk͡saːfɐ). Many Catalan men are named for him, often using the two-name combination Francesc Xavier. In English speaking countries, "Xavier" until recently was likely to follow "Francis"; in the 2000s, however, "Xavier" by itself has become more popular than "Francis", and since 2001 is now one of the hundred most common male baby names in the U.S.A. Furthermore, the Sevier family name, possibly most famous in the United States for John Sevier, originated from the name Xavier.
Many churches all over the world, often founded by Jesuits, have been named in honour of Xavier. The many in the United States include the historic St. Francis Xavier Shrine at Warwick, Maryland (founded 1720), and the Basilica of St. Francis Xavier in Dyersville, Iowa. There are also the American educational teaching order Xaverian Brothers, and the Mission San Xavier del Bac in Tucson, Arizona (founded in 1692, and known for its Spanish Colonial architecture).
Rubens painted "St Francis Xavier Raising the Dead" for a Jesuit church in Antwerp, in which he depicted one of St Francis's many miracles. The Charles Bridge in Prague, Czech Republic, features a statue of Francis Xavier.
In front of Oita Station of Oita City, in Oita Prefecture, previously known as Bungo Province in Japan, there is one statue of Francis Xavier.
The monument Padrão dos Descobrimentos in Belém (Lisbon), Portugal, features a Francis Xavier image.
Shortly before leaving for the East, Xavier issued a famous instruction to Father Gaspar Barazeuz who was leaving to go to Ormuz (a kingdom on an island in the Persian Gulf, formerly attached to the Empire of Persia, now part of Iran), that he should mix with sinners:
Modern scholars place the number of people converted to Christianity by Francis Xavier at around 30,000. And while some of Xavier's methods have been since criticised (he forced converts to take Portuguese names and dress in Western clothes, approved the persecution of the Eastern Church, and used the Goa government as a missionary tool), he has also earned praise. He insisted that missionaries adapt to many of the customs, and most certainly the language, of the culture they wish to evangelise. And unlike later missionaries, Xavier supported an educated native clergy. Though for a time it seemed his work in Japan was subsequently destroyed by persecution, Protestant missionaries three centuries later discovered that approximately 100,000 Christians still practised in the Nagasaki area.
Francis Xavier's work initiated permanent change in eastern Indonesia, and he was known as the "Apostle of the Indies" where in 1546–1547 he worked in the Maluku Islands among the people of Ambon, Ternate, and Morotai (or Moro), and laid the foundations for a permanent mission. After he left the Maluku Islands, others carried on his work and by the 1560s there were 10,000 Roman Catholics in the area, mostly on Ambon. By the 1590s there were 50,000 to 60,000.
The role of Francis Xavier in the Goa Inquisition is controversial . He had written to King João III of Portugal in 1546, encouraging him to dispatch the Inquisition to Goa. Francis Xavier died in 1552 without living to see the horrors of the Goa Inquisition, but some historians believe that he was aware of the Portuguese Inquisition's brutality. In an interview to an Indian newspaper, historian Teotónio de Souza stated that Francis Xavier and Simão Rodrigues, another founder-member of the Society of Jesus, were together in Lisbon before Francis left for India. Both were asked to assist spiritually the prisoners of the Inquisition and were present at the very first auto-da-fé celebrated in Portugal in September 1540, at which 23 were absolved and two were condemned to be burnt, including a French cleric. Hence, he believes that Xavier was aware of the brutality of the Inquisition.

</doc>
<doc id="10958" url="https://en.wikipedia.org/wiki?curid=10958" title="Fossil">
Fossil

A fossil (from Classical Latin: , literally "obtained by digging") is any preserved remains, impression, or trace of any once-living thing from a past geological age. Examples include bones, shells, exoskeletons, stone imprints of animals or microbes, objects preserved in amber, hair, petrified wood, oil, coal, and DNA remnants. The totality of fossils is known as the "fossil record".
Paleontology is the study of fossils: their age, method of formation, and evolutionary significance. Specimens are usually considered to be fossils if they are over 10,000 years old. The oldest fossils are around 3.48 billion years old to 4.1 billion years old. The observation in the 19th century that certain fossils were associated with certain rock strata led to the recognition of a geological timescale and the relative ages of different fossils. The development of radiometric dating techniques in the early 20th century allowed scientists to quantitatively measure the absolute ages of rocks and the fossils they host.
There are many processes that lead to fossilization, including permineralization, casts and molds, authigenic mineralization, replacement and recrystallization, adpression, carbonization, and bioimmuration.
Fossils vary in size from one-micrometre (1 µm) bacteria to dinosaurs and trees, many meters long and weighing many tons. A fossil normally preserves only a portion of the deceased organism, usually that portion that was partially mineralized during life, such as the bones and teeth of vertebrates, or the chitinous or calcareous exoskeletons of invertebrates. Fossils may also consist of the marks left behind by the organism while it was alive, such as animal tracks or feces (coprolites). These types of fossil are called trace fossils or "ichnofossils", as opposed to "body fossils". Some fossils are biochemical and are called "chemofossils" or biosignatures.
The process of fossilization varies according to tissue type and external conditions.
Permineralization is a process of fossilization that occurs when an organism is buried. The empty spaces within an organism (spaces filled with liquid or gas during life) become filled with mineral-rich groundwater. Minerals precipitate from the groundwater, occupying the empty spaces. This process can occur in very small spaces, such as within the cell wall of a plant cell. Small scale permineralization can produce very detailed fossils. For permineralization to occur, the organism must become covered by sediment soon after death, otherwise decay commences. The degree to which the remains are decayed when covered determines the later details of the fossil. Some fossils consist only of skeletal remains or teeth; other fossils contain traces of skin, feathers or even soft tissues. This is a form of diagenesis.
In some cases, the original remains of the organism completely dissolve or are otherwise destroyed. The remaining organism-shaped hole in the rock is called an "external mold". If this hole is later filled with other minerals, it is a "cast". An endocast, or "internal mold", is formed when sediments or minerals fill the internal cavity of an organism, such as the inside of a bivalve or snail or the hollow of a skull.
This is a special form of cast and mold formation. If the chemistry is right, the organism (or fragment of organism) can act as a nucleus for the precipitation of minerals such as siderite, resulting in a nodule forming around it. If this happens rapidly before significant decay to the organic tissue, very fine three-dimensional morphological detail can be preserved. Nodules from the Carboniferous Mazon Creek fossil beds of Illinois, USA, are among the best documented examples of such mineralization.
Replacement occurs when the shell, bone or other tissue is replaced with another mineral. In some cases mineral replacement of the original shell occurs so gradually and at such fine scales that microstructural features are preserved despite the total loss of original material. A shell is said to be "recrystallized" when the original skeletal compounds are still present but in a different crystal form, as from aragonite to calcite.
Compression fossils, such as those of fossil ferns, are the result of chemical reduction of the complex organic molecules composing the organism's tissues. In this case the fossil consists of original material, albeit in a geochemically altered state. This chemical change is an expression of diagenesis. Often what remains is a carbonaceous film known as a phytoleim, in which case the fossil is known as a compression. Often, however, the phytoleim is lost and all that remains is an impression of the organism in the rock—an impression fossil. In many cases, however, compressions and impressions occur together. For instance, when the rock is broken open, the phytoleim will often be attached to one part (compression), whereas the counterpart will just be an impression. For this reason, one term covers the two modes of preservation: "adpression".
Because of their antiquity, an unexpected exception to the alteration of an organism's tissues by chemical reduction of the complex organic molecules during fossilization has been the discovery of soft tissue in dinosaur fossils, including blood vessels, and the isolation of proteins and evidence for DNA fragments. In 2014, Mary Schweitzer and her colleagues reported the presence of iron particles (goethite-aFeO(OH)) associated with soft tissues recovered from dinosaur fossils. Based on various experiments that studied the interaction of iron in haemoglobin with blood vessel tissue they proposed that solution hypoxia coupled with iron chelation enhances the stability and preservation of soft tissue and provides the basis for an explanation for the unforeseen preservation of fossil soft tissues. However, a slightly older study based on eight taxa ranging in time from the Devonian to the Jurassic found that reasonably well-preserved fibrils that probably represent collagen were preserved in all these fossils and that the quality of preservation depended mostly on the arrangement of the collagen fibers, with tight packing favoring good preservation. There seemed to be no correlation between geological age and quality of preservation, within that timeframe.
Fossils that are carbonized or coalified consist of the organic remains which have been reduced primarily to the chemical element carbon. Carbonized fossils consist of a thin film which forms a silhouette of the original organism, and the original organic remains were typically soft tissues. Coalified fossils consist primarily of coal, and the original organic remains were typically woody in composition. 
Bioimmuration occurs when a skeletal organism overgrows or otherwise subsumes another organism, preserving the latter, or an impression of it, within the skeleton. Usually it is a sessile skeletal organism, such as a bryozoan or an oyster, which grows along a substrate, covering other sessile sclerobionts. Sometimes the bioimmured organism is soft-bodied and is then preserved in negative relief as a kind of external mold. There are also cases where an organism settles on top of a living skeletal organism that grows upwards, preserving the settler in its skeleton. Bioimmuration is known in the fossil record from the Ordovician to the Recent.
Paleontology seeks to map out how life evolved across geologic time. A substantial hurdle is the difficulty of working out fossil ages. Beds that preserve fossils typically lack the radioactive elements needed for radiometric dating. This technique is our only means of giving rocks greater than about 50 million years old an absolute age, and can be accurate to within 0.5% or better. Although radiometric dating requires careful laboratory work, its basic principle is simple: the rates at which various radioactive elements decay are known, and so the ratio of the radioactive element to its decay products shows how long ago the radioactive element was incorporated into the rock. Radioactive elements are common only in rocks with a volcanic origin, and so the only fossil-bearing rocks that can be dated radiometrically are volcanic ash layers, which may provide termini for the intervening sediments.
Consequently, palaeontologists rely on stratigraphy to date fossils. Stratigraphy is the science of deciphering the "layer-cake" that is the sedimentary record. Rocks normally form relatively horizontal layers, with each layer younger than the one underneath it. If a fossil is found between two layers whose ages are known, the fossil's age is claimed to lie between the two known ages. Because rock sequences are not continuous, but may be broken up by faults or periods of erosion, it is very difficult to match up rock beds that are not directly adjacent. However, fossils of species that survived for a relatively short time can be used to match isolated rocks: this technique is called "biostratigraphy". For instance, the conodont "Eoplacognathus pseudoplanus" has a short range in the Middle Ordovician period. If rocks of unknown age have traces of "E. pseudoplanus", they have a mid-Ordovician age. Such index fossils must be distinctive, be globally distributed and occupy a short time range to be useful. Misleading results are produced if the index fossils are incorrectly dated. Stratigraphy and biostratigraphy can in general provide only relative dating ("A" was before "B"), which is often sufficient for studying evolution. However, this is difficult for some time periods, because of the problems involved in matching rocks of the same age across continents. Family-tree relationships also help to narrow down the date when lineages first appeared. For instance, if fossils of B or C date to X million years ago and the calculated "family tree" says A was an ancestor of B and C, then A must have evolved earlier.
It is also possible to estimate how long ago two living clades diverged, in other words approximately how long ago their last common ancestor must have lived, by assuming that DNA mutations accumulate at a constant rate. These "molecular clocks", however, are fallible, and provide only approximate timing: for example, they are not sufficiently precise and reliable for estimating when the groups that feature in the Cambrian explosion first evolved, and estimates produced by different techniques may vary by a factor of two.
Organisms are only rarely preserved as fossils in the best of circumstances, and only a fraction of such fossils have been discovered. This is illustrated by the fact that the number of species known through the fossil record is less than 5% of the number of known living species, suggesting that the number of species known through fossils must be far less than 1% of all the species that have ever lived. Because of the specialized and rare circumstances required for a biological structure to fossilize, only a small percentage of life-forms can be expected to be represented in discoveries, and each discovery represents only a snapshot of the process of evolution. The transition itself can only be illustrated and corroborated by transitional fossils, which will never demonstrate an exact half-way point.
The fossil record is strongly biased toward organisms with hard-parts, leaving most groups of soft-bodied organisms with little to no role. It is replete with the mollusks, the vertebrates, the echinoderms, the brachiopods and some groups of arthropods.
Fossil sites with exceptional preservation—sometimes including preserved soft tissues—are known as Lagerstätten—German for "storage places". These formations may have resulted from carcass burial in an anoxic environment with minimal bacteria, thus slowing decomposition. Lagerstätten span geological time from the Cambrian period to the present. Worldwide, some of the best examples of near-perfect fossilization are the Cambrian Maotianshan shales and Burgess Shale, the Devonian Hunsrück Slates, the Jurassic Solnhofen limestone, and the Carboniferous Mazon Creek localities.
Stromatolites are layered accretionary structures formed in shallow water by the trapping, binding and cementation of sedimentary grains by biofilms of microorganisms, especially cyanobacteria. Stromatolites provide some of the most ancient fossil records of life on Earth, dating back more than 3.5 billion years ago.
Stromatolites were much more abundant in Precambrian times. While older, Archean fossil remains are presumed to be colonies of cyanobacteria, younger (that is, Proterozoic) fossils may be primordial forms of the eukaryote chlorophytes (that is, green algae). One genus of stromatolite very common in the geologic record is "Collenia". The earliest stromatolite of confirmed microbial origin dates to 2.724 billion years ago.
A 2009 discovery provides strong evidence of microbial stromatolites extending as far back as 3.45 billion years ago.
Stromatolites are a major constituent of the fossil record for life's first 3.5 billion years, peaking about 1.25 billion years ago. They subsequently declined in abundance and diversity, which by the start of the Cambrian had fallen to 20% of their peak. The most widely supported explanation is that stromatolite builders fell victims to grazing creatures (the Cambrian substrate revolution), implying that sufficiently complex organisms were common over 1 billion years ago.
The connection between grazer and stromatolite abundance is well documented in the younger Ordovician evolutionary radiation; stromatolite abundance also increased after the end-Ordovician and end-Permian extinctions decimated marine animals, falling back to earlier levels as marine animals recovered. Fluctuations in metazoan population and diversity may not have been the only factor in the reduction in stromatolite abundance. Factors such as the chemistry of the environment may have been responsible for changes.
While prokaryotic cyanobacteria themselves reproduce asexually through cell division, they were instrumental in priming the environment for the evolutionary development of more complex eukaryotic organisms. Cyanobacteria (as well as extremophile Gammaproteobacteria) are thought to be largely responsible for increasing the amount of oxygen in the primeval earth's atmosphere through their continuing photosynthesis. Cyanobacteria use water, carbon dioxide and sunlight to create their food. A layer of mucus often forms over mats of cyanobacterial cells. In modern microbial mats, debris from the surrounding habitat can become trapped within the mucus, which can be cemented by the calcium carbonate to grow thin laminations of limestone. These laminations can accrete over time, resulting in the banded pattern common to stromatolites. The domal morphology of biological stromatolites is the result of the vertical growth necessary for the continued infiltration of sunlight to the organisms for photosynthesis. Layered spherical growth structures termed oncolites are similar to stromatolites and are also known from the fossil record. Thrombolites are poorly laminated or non-laminated clotted structures formed by cyanobacteria common in the fossil record and in modern sediments.
The Zebra River Canyon area of the Kubis platform in the deeply dissected Zaris Mountains of southwestern Namibia provides an extremely well exposed example of the thrombolite-stromatolite-metazoan reefs that developed during the Proterozoic period, the stromatolites here being better developed in updip locations under conditions of higher current velocities and greater sediment influx.
Index fossils (also known as guide fossils, indicator fossils or zone fossils) are fossils used to define and identify geologic periods (or faunal stages). They work on the premise that, although different sediments may look different depending on the conditions under which they were deposited, they may include the remains of the same species of fossil. The shorter the species' time range, the more precisely different sediments can be correlated, and so rapidly evolving species' fossils are particularly valuable. The best index fossils are common, easy to identify at species level and have a broad distribution—otherwise the likelihood of finding and recognizing one in the two sediments is poor.
Trace fossils consist mainly of tracks and burrows, but also include coprolites (fossil feces) and marks left by feeding. Trace fossils are particularly significant because they represent a data source that is not limited to animals with easily fossilized hard parts, and they reflect animal behaviours. Many traces date from significantly earlier than the body fossils of animals that are thought to have been capable of making them. Whilst exact assignment of trace fossils to their makers is generally impossible, traces may for example provide the earliest physical evidence of the appearance of moderately complex animals (comparable to earthworms).
Coprolites are classified as trace fossils as opposed to body fossils, as they give evidence for the animal's behaviour (in this case, diet) rather than morphology. They were first described by William Buckland in 1829. Prior to this they were known as "fossil fir cones" and "bezoar stones." They serve a valuable purpose in paleontology because they provide direct evidence of the predation and diet of extinct organisms. Coprolites may range in size from a few millimetres to over 60 centimetres.
A "transitional fossil" is any fossilized remains of a life form that exhibits traits common to both an ancestral group and its derived descendant group. This is especially important where the descendant group is sharply differentiated by gross anatomy and mode of living from the ancestral group. Because of the incompleteness of the fossil record, there is usually no way to know exactly how close a transitional fossil is to the point of divergence. These fossils serve as a reminder that taxonomic divisions are human constructs that have been imposed in hindsight on a continuum of variation.
Microfossil is a descriptive term applied to fossilized plants and animals whose size is just at or below the level at which the fossil can be analyzed by the naked eye. A commonly applied cutoff point between "micro" and "macro" fossils is 1 mm. Microfossils may either be complete (or near-complete) organisms in themselves (such as the marine plankters foraminifera and coccolithophores) or component parts (such as small teeth or spores) of larger animals or plants. Microfossils are of critical importance as a reservoir of paleoclimate information, and are also commonly used by biostratigraphers to assist in the correlation of rock units.
Fossil resin (colloquially called amber) is a natural polymer found in many types of strata throughout the world, even the Arctic. The oldest fossil resin dates to the Triassic, though most dates to the Cenozoic. The excretion of the resin by certain plants is thought to be an evolutionary adaptation for protection from insects and to seal wounds. Fossil resin often contains other fossils called inclusions that were captured by the sticky resin. These include bacteria, fungi, other plants, and animals. Animal inclusions are usually small invertebrates, predominantly arthropods such as insects and spiders, and only extremely rarely a vertebrate such as a small lizard. Preservation of inclusions can be exquisite, including small fragments of DNA.
A "derived", "reworked" or "remanié fossil" is a fossil found in rock that accumulated significantly later than when the fossilized animal or plant died. Reworked fossils are created by erosion exhuming (freeing) fossils from the rock formation in which they were originally deposited and their redeposition in a younger sedimentary deposit.
Fossil wood is wood that is preserved in the fossil record. Wood is usually the part of a plant that is best preserved (and most easily found). Fossil wood may or may not be petrified. The fossil wood may be the only part of the plant that has been preserved: therefore such wood may get a special kind of botanical name. This will usually include "xylon" and a term indicating its presumed affinity, such as "Araucarioxylon" (wood of "Araucaria" or some related genus), "Palmoxylon" (wood of an indeterminate palm), or "Castanoxylon" (wood of an indeterminate chinkapin).
The term subfossil can be used to refer to remains, such as bones, nests, or defecations, whose fossilization process is not complete, either because the length of time since the animal involved was living is too short (less than 10,000 years) or because the conditions in which the remains were buried were not optimal for fossilization. Subfossils are often found in caves or other shelters where they can be preserved for thousands of years. The main importance of subfossil vs. fossil remains is that the former contain organic material, which can be used for radiocarbon dating or extraction and sequencing of DNA, protein, or other biomolecules. Additionally, isotope ratios can provide much information about the ecological conditions under which extinct animals lived. Subfossils are useful for studying the evolutionary history of an environment and can be important to studies in paleoclimatology.
Subfossils are often found in depositionary environments, such as lake sediments, oceanic sediments, and soils. Once deposited, physical and chemical weathering can alter the state of preservation.
Chemical fossils, or chemofossils, are chemicals found in rocks and fossil fuels (petroleum, coal, and natural gas) that provide an organic signature for ancient life. Molecular fossils and isotope ratios represent two types of chemical fossils. The oldest traces of life on Earth are fossils of this type, including carbon isotope anomalies found in zircons that imply the existence of life as early as 4.1 billion years ago.
It has been suggested that biominerals could be important indicators of extraterrestrial life and thus could play an important role in the search for past or present life on the planet Mars. Furthermore, organic components (biosignatures) that are often associated with biominerals are believed to play crucial roles in both pre-biotic and biotic reactions.
On 24 January 2014, NASA reported that current studies by the "Curiosity" and "Opportunity" rovers on Mars will now be searching for evidence of ancient life, including a biosphere based on autotrophic, chemotrophic and/or chemolithoautotrophic microorganisms, as well as ancient water, including fluvio-lacustrine environments (plains related to ancient rivers or lakes) that may have been habitable. The search for evidence of habitability, taphonomy (related to fossils), and organic carbon on the planet Mars is now a primary NASA objective.
"Pseudofossils" are visual patterns in rocks that are produced by geologic processes rather than biologic processes. They can easily be mistaken for real fossils. Some pseudofossils, such as geological dendrite crystals, are formed by naturally occurring fissures in the rock that get filled up by percolating minerals. Other types of pseudofossils are kidney ore (round shapes in iron ore) and moss agates, which look like moss or plant leaves. Concretions, spherical or ovoid-shaped nodules found in some sedimentary strata, were once thought to be dinosaur eggs, and are often mistaken for fossils as well.
Gathering fossils dates at least to the beginning of recorded history. The fossils themselves are referred to as the fossil record. The fossil record was one of the early sources of data underlying the study of evolution and continues to be relevant to the history of life on Earth. Paleontologists examine the fossil record to understand the process of evolution and the way particular species have evolved.
Fossils have been visible and common throughout most of natural history, and so documented human interaction with them goes back as far as recorded history, or earlier.
There are many examples of paleolithic stone knives in Europe, with fossil echinoderms set precisely at the hand grip, going all the way back to "Homo heidelbergensis" and neanderthals. These ancient peoples also drilled holes through the center of those round fossil shells, apparently using them as beads for necklaces.
The ancient Egyptians gathered fossils of species that resembled the bones of modern species they worshipped. The god Set was associated with the hippopotamus, therefore fossilized bones of hippo-like species were kept in that deity's temples. Five-rayed fossil sea urchin shells were associated with the deity Sopdu, the Morning Star, equivalent of Venus in Roman mythology.
Fossils appear to have directly contributed to the mythology of many civilizations, including the ancient Greeks. Classical Greek historian Herodotos wrote of an area near Hyperborea where gryphons protected golden treasure. There was indeed gold mining in that approximate region, where beaked "Protoceratops" skulls were common as fossils.
A later Greek scholar, Aristotle, eventually realized that fossil seashells from rocks were similar to those found on the beach, indicating the fossils were once living animals. He had previously explained them in terms of vaporous exhalations, which Persian polymath Avicenna modified into the theory of petrifying fluids ("succus lapidificatus"). This was built upon in the 14th century by Albert of Saxony, and accepted in some form by most naturalists by the 16th century.
Roman naturalist Pliny the Elder wrote of "tongue stones", which he called glossopetra. These were fossil shark teeth, thought by some classical cultures to look like the tongues of people or snakes. He also wrote about the horns of Ammon, which are fossil ammonites, from whence the species ultimately draws its modern name. Pliny also makes one of the earlier known references to toadstones, thought until the 18th century to be a magical cure for poison originating in the heads of toads, but which are fossil teeth from "Lepidotes", a Cretaceous ray-finned fish.
The Plains tribes of North America are thought to have similarly associated fossils, such as the many intact pterosaur fossils naturally exposed in the region, with their own mythology of the thunderbird.
There is no such direct mythological connection known from prehistoric Africa, but there is considerable evidence of tribes there excavating and moving fossils to ceremonial sites, apparently treating them with some reverence.
In Japan, fossil shark teeth were associated with the mythical tengu, thought to be the razor-sharp claws of the creature, documented some time after the 8th century AD.
In medieval China, the fossil bones of ancient mammals including "Homo erectus" were often mistaken for "dragon bones" and used as medicine and aphrodisiacs. In addition, some of these fossil bones are collected as "art" by scholars, who left scripts on various artifacts, indicating the time they were added to a collection. One good example is the famous scholar Huang Tingjian of the South Song Dynasty during the 11th century, who kept a specific seashell fossil with his own poem engraved on it. In the West fossilized sea creatures on mountainsides were seen as proof of the biblical deluge.
In 1027, the Persian Avicenna explained fossils' stoniness in "The Book of Healing":
From the 13th century to the present day, scholars pointed out that the fossil skulls of Deinotherium giganteum, found in Crete and Greece, might have been interpreted as being the skulls of the Cyclopes of Greek mythology, and are possibly the origin of that Greek myth. Their skulls appear to have a single eye-hole in the front, just like their modern elephant cousins, though in fact it's actually the opening for their trunk.
In Norse mythology, echinoderm shells (the round five-part button left over from a sea urchin) were associated with the god Thor, not only being incorporated in thunderstones, representations of Thor's hammer and subsequent hammer-shaped crosses as Christianity was adopted, but also kept in houses to garner Thor's protection.
These grew into the shepherd's crowns of English folklore, used for decoration and as good luck charms, placed by the doorway of homes and churches. In Suffolk, a different species was used as a good-luck charm by bakers, who referred to them as fairy loaves, associating them with the similarly shaped loaves of bread they baked.
More scientific views of fossils emerged during the Renaissance. Leonardo da Vinci concurred with Aristotle's view that fossils were the remains of ancient life. For example, da Vinci noticed discrepancies with the biblical flood narrative as an explanation for fossil origins:
In 1666, Nicholas Steno examined a shark, and made the association of its teeth with the "tongue stones" of ancient Greco-Roman mythology, concluding that those were not in fact the tongues of venomous snakes, but the teeth of some long-extinct species of shark.
Robert Hooke (1635-1703) included micrographs of fossils in his "Micrographia" and was among the first to observe fossil forams. His observations on fossils, which he stated to be the petrified remains of creatures some of which no longer existed, were published posthumously in 1705.
William Smith (1769–1839), an English canal engineer, observed that rocks of different ages (based on the law of superposition) preserved different assemblages of fossils, and that these assemblages succeeded one another in a regular and determinable order. He observed that rocks from distant locations could be correlated based on the fossils they contained. He termed this the principle of "faunal succession". This principle became one of Darwin's chief pieces of evidence that biological evolution was real.
Georges Cuvier came to believe that most if not all the animal fossils he examined were remains of extinct species. This led Cuvier to become an active proponent of the geological school of thought called catastrophism. Near the end of his 1796 paper on living and fossil elephants he said:
Interest in fossils, and geology more generally, expanded during the early nineteenth century. In Britain, Mary Anning's discoveries of fossils, including the first complete ichthyosaur and a complete plesiosaurus skeleton, sparked both public and scholarly interest.
Early naturalists well understood the similarities and differences of living species leading Linnaeus to develop a hierarchical classification system still in use today. Darwin and his contemporaries first linked the hierarchical structure of the tree of life with the then very sparse fossil record. Darwin eloquently described a process of descent with modification, or evolution, whereby organisms either adapt to natural and changing environmental pressures, or they perish.
When Darwin wrote "On the Origin of Species by Means of Natural Selection, or the Preservation of Favoured Races in the Struggle for Life", the oldest animal fossils were those from the Cambrian Period, now known to be about 540 million years old. He worried about the absence of older fossils because of the implications on the validity of his theories, but he expressed hope that such fossils would be found, noting that: "only a small portion of the world is known with accuracy." Darwin also pondered the sudden appearance of many groups (i.e. phyla) in the oldest known Cambrian fossiliferous strata.
Since Darwin's time, the fossil record has been extended to between 2.3 and 3.5 billion years. Most of these Precambrian fossils are microscopic bacteria or microfossils. However, macroscopic fossils are now known from the late Proterozoic. The Ediacara biota (also called Vendian biota) dating from 575 million years ago collectively constitutes a richly diverse assembly of early multicellular eukaryotes.
The fossil record and faunal succession form the basis of the science of biostratigraphy or determining the age of rocks based on embedded fossils. For the first 150 years of geology, biostratigraphy and superposition were the only means for determining the relative age of rocks. The geologic time scale was developed based on the relative ages of rock strata as determined by the early paleontologists and stratigraphers.
Since the early years of the twentieth century, absolute dating methods, such as radiometric dating (including potassium/argon, argon/argon, uranium series, and, for very recent fossils, radiocarbon dating) have been used to verify the relative ages obtained by fossils and to provide absolute ages for many fossils. Radiometric dating has shown that the earliest known stromatolites are over 3.4 billion years old.
Paleontology has joined with evolutionary biology to share the interdisciplinary task of outlining the tree of life, which inevitably leads backwards in time to Precambrian microscopic life when cell structure and functions evolved. Earth's deep time in the Proterozoic and deeper still in the Archean is only "recounted by microscopic fossils and subtle chemical signals." Molecular biologists, using phylogenetics, can compare protein amino acid or nucleotide sequence homology (i.e., similarity) to evaluate taxonomy and evolutionary distances among organisms, with limited statistical confidence. The study of fossils, on the other hand, can more specifically pinpoint when and in what organism a mutation first appeared. Phylogenetics and paleontology work together in the clarification of science's still dim view of the appearance of life and its evolution.
Niles Eldredge's study of the "Phacops" trilobite genus supported the hypothesis that modifications to the arrangement of the trilobite's eye lenses proceeded by fits and starts over millions of years during the Devonian. Eldredge's interpretation of the "Phacops" fossil record was that the aftermaths of the lens changes, but not the rapidly occurring evolutionary process, were fossilized. This and other data led Stephen Jay Gould and Niles Eldredge to publish their seminal paper on punctuated equilibrium in 1971.
Synchrotron X-ray tomographic analysis of early Cambrian bilaterian embryonic microfossils yielded new insights of metazoan evolution at its earliest stages. The tomography technique provides previously unattainable three-dimensional resolution at the limits of fossilization. Fossils of two enigmatic bilaterians, the worm-like "Markuelia" and a putative, primitive protostome, "Pseudooides", provide a peek at germ layer embryonic development. These 543-million-year-old embryos support the emergence of some aspects of arthropod development earlier than previously thought in the late Proterozoic. The preserved embryos from China and Siberia underwent rapid diagenetic phosphatization resulting in exquisite preservation, including cell structures. This research is a notable example of how knowledge encoded by the fossil record continues to contribute otherwise unattainable information on the emergence and development of life on Earth. For example, the research suggests "Markuelia" has closest affinity to priapulid worms, and is adjacent to the evolutionary branching of Priapulida, Nematoda and Arthropoda.
"Fossil trading" is the practice of buying and selling fossils. This is many times done illegally with artifacts stolen from research sites, costing many important scientific specimens each year. The problem is quite pronounced in China, where many specimens have been stolen.
"Fossil collecting" (sometimes, in a non-scientific sense, fossil hunting) is the collection of fossils for scientific study, hobby, or profit. Fossil collecting, as practiced by amateurs, is the predecessor of modern paleontology and many still collect fossils and study fossils as amateurs. Professionals and amateurs alike collect fossils for their scientific value.
These is some medicinal and preventive use for some fossils. Largely the use of fossils as medicine is a matter of placebo effect. However, the consumption of certain fossils has been proven to help against stomach acidity and mineral depletion. The use of fossils to address health issues is rooted in traditional medicine and include the use of fossils as talismans. The specific fossil to use to alleviate or cure an illness is often based on its resemblance of the fossils and the symptoms or affected organ. The use dinosaur bones as "dragon bones" has persisted in Traditional Chinese medicine into modern times, with Mid Cretaceous dinosaur bones being used for the purpose in Ruyang County during the early 21st century.

</doc>
<doc id="10960" url="https://en.wikipedia.org/wiki?curid=10960" title="Family Educational Rights and Privacy Act">
Family Educational Rights and Privacy Act

The Family Educational Rights and Privacy Act of 1974 (FERPA or the Buckley Amendment) is a United States federal law that governs the access to educational information and records by public entities such as potential employers, publicly funded educational institutions, and foreign governments.
FERPA gives parents access to their child's education records, an opportunity to seek to have the records amended, and some control over the disclosure of information from the records. With several exceptions, schools must have a student's consent prior to the disclosure of education records "after that student is 18 years old". The law applies only to educational agencies and institutions that receive 
funds under a program administered by the U.S. Department of Education.
Other regulations under this act, effective starting January 3, 2012, allow for greater disclosures of personal and directory student identifying information and regulate student IDs and e-mail addresses. For example, schools may provide external companies a student's personally identifiable information without the student's consent. Conversely, tying student directory information to other information may result in a violation, as the combination creates an education record.
Examples of situations affected by FERPA include school employees divulging information to anyone other than the student about the student's grades or behavior, and school work posted on a bulletin board with a grade. Generally, schools must have written permission from the parent or eligible student in order to release any information from a student's education record.
This privacy policy also governs how state agencies transmit testing data to federal agencies, such as the Education Data Exchange Network.
This U.S. federal law also gave students 18 years of age or older, or students of any age if enrolled in any post-secondary educational institution, the right of privacy regarding grades, enrollment, and even billing information unless the school has specific permission from the student to share that specific type of information.
FERPA also permits a school to disclose personally identifiable information from education records of an "eligible student" (a student age 18 or older or enrolled in a postsecondary institution at any age) to his or her parents if the student is a "dependent student" as that term is defined in Section 152 of the Internal Revenue Code. Generally, if either parent has claimed the student as a dependent on the parent's most recent income tax statement, the school may non-consensually disclose the student's education records to both parents.
The law allowed students who apply to an educational institution such as graduate school permission to view recommendations submitted by others as part of the application. However, on standard application forms, students are given the option to waive this right.
FERPA specifically excludes employees of an educational institution if they are not students.
The act is also referred to as the "Buckley Amendment", for one of its proponents, Senator James L. Buckley of New York.
The citing of FERPA to conceal public records that are not "educational" in nature has been widely criticized, including by the act's primary Senate sponsor. For example, in the "Owasso Independent School District v. Falvo" case, an important part of the debate was determining the relationship between peer-grading and "education records" as defined in FERPA. The plaintiffs argued "that allowing students to score each other's tests[...]as the teachers explain the correct answers to the entire class[...] embarrassed [...] children", but lost in a summary judgment by the district court. In the Court of Appeals, it was ruled that students placing grades on the work of other students made such work into an "education record." Thus, peer-grading was determined as a violation of FERPA privacy policies because students had access to other students' academic performance without full consent. However, on appeal to the Supreme Court, it was unanimously ruled that peer-grading was not a violation of FERPA. This is because a grade written on a student's work does not become an "education record" until the teacher writes the final grade into a grade book.
Legal experts have debated the issue of whether student medical records (for example records of therapy sessions with a therapist at an on-campus counseling center) might be released to the school administration under certain triggering events, such as when a student sued his college or university.
Usually, student medical treatment records will remain under the protection of FERPA, not the Health Insurance Portability and Accountability Act (HIPAA). This is due to the "FERPA Exception" written within HIPAA.

</doc>
<doc id="10963" url="https://en.wikipedia.org/wiki?curid=10963" title="Forgetting">
Forgetting

Forgetting or disremembering is the apparent loss or modification of information already encoded and stored in an individual's short or long-term memory. It is a spontaneous or gradual process in which old memories are unable to be recalled from memory storage. Problems with remembering, learning and retaining new information are a few of the most common complaints of older adults. 
Studies show that retention improves with increased rehearsal. This improvement occurs because rehearsal helps to transfer information into long-term memory – practise makes perfect.
Forgetting curves (amount remembered as a function of time since an event was first experienced) have been extensively analyzed. The most recent evidence suggests that a power function provides the closest mathematical fit to the forgetting function.
Failing to retrieve an event does not mean that this specific event has been forever forgotten. Research has shown that there are a few health behaviors that to some extent can prevent forgetting from happening so often. One of the simplest ways to keep the brain healthy and prevent forgetting is to stay active and exercise. Staying active is important because overall it keeps the body healthy. When the body is healthy the brain is healthy and less inflamed as well. Older adults who were more active were found to have had less episodes of forgetting compared to those older adults who were less active. A healthy diet can also contribute to a healthier brain and aging process which in turn results in less frequent forgetting.
One of the first to study the mechanisms of forgetting was the German psychologist Hermann Ebbinghaus (1885). Using himself as the sole subject in his experiment, he memorized lists of three letter nonsense syllable words—two consonants and one vowel in the middle. He then measured his own capacity to relearn a given list of words after a variety of given time period. He found that forgetting occurs in a systematic manner, beginning rapidly and then leveling off. Although his methods were primitive, his basic premises have held true today and have been reaffirmed by more methodologically sound methods. The Ebbinghaus "forgetting curve" is the name of his results which he plotted out and made 2 conclusions. The first being that much of what we forget is lost soon after it is originally learned. The second being that the amount of forgetting eventually levels off.
Around the same time Ebbinghaus developed the forgetting curve, psychologist Sigmund Freud theorized that people intentionally forgot things in order to push bad thoughts and feelings deep into their unconscious, a process he called "repression".
There is debate as to whether (or how often) memory repression really occurs and mainstream psychology holds that true memory repression occurs only very rarely.
One process model for memory was proposed by Richard Atkinson and Richard Shiffrin in the 1960s as a way to explain the operation of memory. This modal model of memory, also known as the Atkinson-Shiffrin model of memory, suggests there are three types of memory: sensory memory, short-term memory, and long-term memory. Each type of memory is separate in its capacity and duration. In the modal model, how quickly information is forgotten is related to the type of memory where that information is stored. Information in the first stage, sensory memory, is forgotten after only a few seconds. In the second stage, short-term memory, information is forgotten after about 20. While information in long-term memory can be remembered for years or even decades, it may be forgotten when the retrieval processes for that information fail.
Concerning unwanted memories, modern terminology divides motivated forgetting into unconscious repression (which is disputed) and conscious thought suppression.
Forgetting can be measured in different ways all of which are based on recall:
For this type of measurement, a participant has to identify material that was previously learned. The participant is asked to remember a list of material. Later on they are shown the same list of material with additional information and they are asked to identify the material that was on the original list. The more they recognize, the less information is forgotten.
Free recall is a basic paradigm used to study human memory. In a free recall task, a subject is presented a list of to-be-remembered items, one at a time. For example, an experimenter might read a list of 20 words aloud, presenting a new word to the subject every 4 seconds. At the end of the presentation of the list, the subject is asked to recall the items (e.g., by writing down as many items from the list as possible). It is called a free recall task because the subject is free to recall the items in any order that he or she desires.
Prompted recall is a slight variation of free recall that consists of presenting hints or prompts to increase the likelihood that the behavior will be produced. Usually these prompts are stimuli that were not there during the training period. Thus in order to measure the degree of forgetting, one can see how many prompts the subject misses or the number of prompts required to produce the behavior.
This method measures forgetting by the amount of training required to reach the previous level of performance. German psychologist Hermann Ebbinghaus (1885) used this method on himself. He memorized lists of nonsensical syllables until he could repeat the list two times without error. After a certain interval, he relearned the list and saw how long it would take him to do this task. If it took fewer times, then there had been less forgetting. His experiment was one of the first to study forgetting.
The four main theories of forgetting apparent in the study of psychology are as follows:
Cue-dependent forgetting (also, context-dependent forgetting) or retrieval failure, is the failure to recall a memory due to missing stimuli or cues that were present at the time the memory was encoded. Encoding is the first step in creating and remembering a memory. How well something has been encoded in the memory can be measured by completing specific tests of retrieval. Examples of these tests would be explicit ones like cued recall or implicit tests like word fragment completion. Cue-dependent forgetting is one of five cognitive psychology theories of forgetting. This theory states that a memory is sometimes temporarily forgotten purely because it cannot be retrieved, but the proper cue can bring it to mind. A good metaphor for this is searching for a book in a library without the reference number, title, author or even subject. The information still exists, but without these cues retrieval is unlikely. Furthermore, a good retrieval cue must be consistent with the original encoding of the information. If the sound of the word is emphasized during the encoding process, the cue that should be used should also put emphasis on the phonetic quality of the word. Information is available however, just not readily available without these cues. Depending on the age of a person, retrieval cues and skills may not work as well. This is usually common in older adults but that is not always the case. When information is encoded into the memory and retrieved with a technique called spaced retrieval, this helps older adults retrieve the events stored in the memory better. There is also evidence from different studies that show age related changes in memory. These specific studies have shown that episodic memory performance does in fact decline with age and have made known that older adults produce vivid rates of forgetting when two items are combined and not encoded.
Forgetting that occurs through physiological damage or dilapidation to the brain are referred to as organic causes of forgetting. These theories encompass the loss of information already retained in long-term memory or the inability to encode new information again. Examples include Alzheimer's, amnesia, dementia, consolidation theory and the gradual slowing down of the central nervous system due to aging.
Interference theory refers to the idea that when the learning of something new causes forgetting of older material on the basis of competition between the two. This essentially states that memory's information may become confused or combined with other information during encoding, resulting in the distortion or disruption of memories. In nature, the interfering items are said to originate from an overstimulating environment. Interference theory exists in three branches: Proactive, Retroactive and Output. Retroactive and Proactive inhibition each referring in contrast to the other. Retroactive interference is when new information (memories) interferes with older information. On the other hand, proactive interference is when old information interferes with the retrieval of new information. This is sometimes thought to occur especially when memories are similar. Output Interference occurs when the initial act of recalling specific information interferes with the retrieval of the original information. This theory shows a contradiction: an extremely intelligent individual is expected to forget more hastily than one who has a slow mentality. For this reason, an intelligent individual has stored up more memory in his mind which will cause interferences and impair their ability to recall specific information. Based on current research, testing interference has only been carried out by recalling from a list of words rather than using situation from daily lives, thus it's hard to generalize the findings for this theory.
Decay theory states that when something new is learned, a neurochemical, physical "memory trace" is formed in the brain and over time this trace tends to disintegrate, unless it is occasionally used. Decay theory states the reason we eventually forget something or an event is because the memory of it fades with time. If we do not attempt to look back at an event, the greater the interval time between the time when the event from happening and the time when we try to remember, the memory will start to fade. Time is the greatest impact in remembering an event.
Trace decay theory explains memories that are stored in both short-term and long-term memory system, and assumes that the memories leave a trace in the brain. According to this theory, short-term memory (STM) can only retain information for a limited amount of time, around 15 to 30 seconds unless it is rehearsed. If it is not rehearsed, the information will start to gradually fade away and decay. Donald Hebb proposed that incoming information causes a series of neurons to create a neurological memory trace in the brain which would result in change in the morphological and/or chemical changes in the brain and would fade with time. Repeated firing causes a structural change in the synapses. Rehearsal of repeated firing maintains the memory in STM until a structural change is made. Therefore, forgetting happens as a result of automatic decay of the memory trace in brain. This theory states that the events between learning and recall have no effects on recall; the important factor that affects is the duration that the information has been retained. Hence, as longer time passes more of traces are subject to decay and as a result the information is forgotten.
One major problem about this theory is that in real-life situation, the time between encoding a piece of information and recalling it, is going to be filled with all different kinds of events that might happen to the individual. Therefore, it is difficult to conclude that forgetting is a result of only the time duration. It is also important to consider the effectiveness of this theory. Although it seems very plausible, it is about impossible to test. It is difficult to create a situation where there is a blank period of time between presenting the material and recalling it later.
This theory is supposedly contradicted by the fact that one is able to ride a bike even after not having done so for decades. "Flashbulb memories" are another piece of seemingly contradicting evidence. It is believed that certain memories "trace decay" while others don't. Sleep is believed to play a key role in halting trace decay, although the exact mechanism of this is unknown.
Forgetting can have very different causes than simply removal of stored content. Forgetting can mean access problems, availability problems, or can have other reasons such as amnesia caused by an accident.
An inability to forget can cause distress, as with posttraumatic stress disorder and hyperthymesia (in which people have an extremely detailed autobiographical memory).
Psychologists have called attention to "social aspects of forgetting". Though often loosely defined, social amnesia is generally considered to be the opposite of collective memory. "Social amnesia" was first discussed by Russell Jacoby, yet his use of the term was restricted to a narrow approach, which was limited to what he perceived to be a relative neglect of psychoanalytical theory in psychology. The cultural historian Peter Burke suggested that "it may be worth investigating the social organization of forgetting, the rules of exclusion, suppression or repression, and the question of who wants whom to forget what". In an in-depth historical study spanning two centuries, Guy Beiner proposed the term "social forgetting", which he distinguished from crude notions of "collective amnesia" and "total oblivion", arguing that "social forgetting is to be found in the interface of public silence and more private remembrance".

</doc>
<doc id="10965" url="https://en.wikipedia.org/wiki?curid=10965" title="Fay Wray">
Fay Wray

Vina Fay Wray (September 15, 1907 – August 8, 2004) was a Canadian-born American actress best remembered for starring as Ann Darrow in the 1933 film "King Kong". Through an acting career that spanned nearly six decades, Wray attained international recognition as an actress in horror films. She has been dubbed one of the early "scream queens".
After appearing in minor film roles, Wray gained media attention after being selected as one of the "WAMPAS Baby Stars" in 1926. This led to her being contracted to Paramount Pictures as a teenager, where she made more than a dozen feature films. After leaving Paramount, she signed deals with various film companies, being cast in her first horror film roles, in addition to many other types of roles, including in "The Bowery" (1933) and "Viva Villa" (1934), both of which starred Wallace Beery. For RKO Radio Pictures, Inc., she starred in the film with which she is most identified, "King Kong" (1933). After the success of "King Kong", Wray made numerous appearances in both film and television; she retired in 1980.
Wray was born on a ranch near Cardston in the province of Alberta, Canada to parents who were members of The Church of Jesus Christ of Latter-day Saints, Elvina Marguerite Jones, who was from Salt Lake City, Utah, and Joseph Heber Wray, who was from Kingston upon Hull, England. She was one of six children and was a granddaughter of LDS pioneer Daniel Webster Jones. Her ancestors came from England, Scotland, Ireland and Wales. Wray was never baptized a member of The Church of Jesus Christ of Latter-day Saints.
Her family returned to the United States a few years after she was born; they moved to Salt Lake City in 1912 and moved to Lark, Utah, in 1914. In 1919, the Wray family returned to Salt Lake City, and then relocated to Hollywood, where Fay attended Hollywood High School.
In 1923, Wray appeared in her first film at the age of 16, when she landed a role in a short historical film sponsored by a local newspaper. In the 1920s, Wray landed a major role in the silent film "The Coast Patrol" (1925), as well as uncredited bit parts at the Hal Roach Studios.
In 1926, the Western Association of Motion Picture Advertisers selected Wray as one of the "WAMPAS Baby Stars", a group of women whom they believed to be on the threshold of movie stardom. She was at the time under contract to Universal Studios, mostly co-starring in low-budget Westerns opposite Buck Jones.
The following year, Wray was signed to a contract with Paramount Pictures. In 1926, director Erich von Stroheim cast her as the main female lead in his film "The Wedding March", released by Paramount two years later. While the film was noted for its high budget and production values, it was a financial failure. It also gave Wray her first lead role. Wray stayed with Paramount to make more than a dozen films and made the transition from silent films to "talkies".
After leaving Paramount, Wray signed with various film companies. Under these deals, Wray was cast in a variety of horror films, including "Doctor X" (1932) and "Mystery of the Wax Museum" (1933). However, her best known films were produced under her deal with RKO Radio Pictures. Her first film with RKO was "The Most Dangerous Game" (1932), co-starring Joel McCrea. The production was filmed at night on the same jungle sets that were being used for "King Kong" during the day, and with Wray and Robert Armstrong starring in both movies.
"The Most Dangerous Game" was followed by the release of Wray's most memorable film, "King Kong". According to Wray, Jean Harlow had been RKO's original choice, but because MGM put Harlow under exclusive contract during the pre-production phase of the film, she became unavailable. Wray was approached by director Merian C. Cooper to play the blonde captive of King Kong, the role of Ann Darrow was one she would most be associated with and she was paid $10,000 ($ in dollars) to play her. The film was a commercial success and Wray was reportedly proud that the film saved RKO from bankruptcy.
She continued to star in various films, including "The Richest Girl in the World", a second film with Joel McCrea, but by the early 1940s, her appearances became less frequent. She retired from acting in 1942 after her second marriage but due to financial exigencies she soon resumed her acting career, and over the next three decades, Wray appeared in several films and she also frequently appeared on television.
Wray was cast as Catherine Morrison in the 1953–54 sitcom "The Pride of the Family." Paul Hartman played her husband, Albie Morrison. Natalie Wood and Robert Hyatt played their children, Ann and Junior Morrison, respectively. Wray appeared with fellow WAMPAS Baby Star Joan Crawford in "Queen Bee", released in 1955.
Wray appeared in three episodes of "Perry Mason": "The Case of the Prodigal Parent" (1958); "The Case of the Watery Witness" (1959), as murder victim Lorna Thomas; and "The Case of the Fatal Fetish" (1965), as voodoo practitioner Mignon Germaine. In 1959, Wray was cast as Tula Marsh in the episode "The Second Happiest Day" of "Playhouse 90". Other roles around this time were in the episodes "Dip in the Pool" (1958) and "The Morning After" of CBS's "Alfred Hitchcock Presents". In 1960, she appeared as Clara in an episode of "77 Sunset Strip", "Who Killed Cock Robin?" Another 1960 role was that of Mrs. Staunton, with Gigi Perreau as her daughter, in the episode "Flight from Terror" of "The Islanders".
Wray appeared in a 1961 episode of "The Real McCoys" titled "Theatre in the Barn". In 1963, she played Mrs. Brubaker in the episode "You're So Smart, Why Can't You Be Good?" of "The Eleventh Hour". She ended her acting career in the 1980 made-for-television film "Gideon's Trumpet".
In 1988, she published her autobiography "On the Other Hand". In her later years, Wray continued to make public appearances. In 1991, she was crowned Queen of the Beaux Arts Ball, presiding with King Herbert Huncke.
She was approached by James Cameron to play the part of Rose Dawson Calvert for his blockbuster "Titanic" (1997) with Kate Winslet to play her younger self, but she turned down the role, which ended up being played by Gloria Stuart. She was a special guest at the 70th Academy Awards, where the show's host Billy Crystal introduced her as the "Beauty who charmed the Beast." She was the only 1920s Hollywood actress in attendance that evening (with fellow 1930s actress Gloria Stuart nominated for an award. On October 3, 1998, she appeared at the Pine Bluff Film Festival, which showed "The Wedding March" (with live orchestral accompaniment).
In January 2003, the 95-year-old Wray appeared at the 2003 Palm Beach International Film Festival to celebrate the Rick McKay documentary film "", where she was honored with a "Legend in Film" award. In her later years, she visited the Empire State Building frequently; in 1991, she was a guest of honor at the building's 60th anniversary, and in May 2004, she made one of her later public appearances. Her final public appearance was at an after-party at Sardi's restaurant in New York City, following the premiere of the documentary film "Broadway: The Golden Age, by the Legends Who Were There". 
Wray married three times – to writers John Monk Saunders and Robert Riskin and the neurosurgeon Sanford Rothenberg (January 28, 1919 – January 4, 1991). She had three children: Susan Saunders, Victoria Riskin, and Robert Riskin Jr.
She became a naturalized citizen of the United States in 1933.
In 2004, Wray was approached by director Peter Jackson to appear in a small cameo for the 2005 remake of "King Kong". She met with Naomi Watts, who was to play the role of Ann Darrow. She politely declined the cameo, and claimed that the original "Kong" was the true "King." Before the filming of the remake commenced, Wray died in her sleep of natural causes on August 8, 2004 in her apartment in Manhattan, five weeks before her 97th birthday. Wray is interred at the Hollywood Forever Cemetery in Hollywood, California.
Two days after her death, the lights of the Empire State Building were lowered for 15 minutes in her memory.
In 1989, Wray was awarded the Women in Film Crystal Award. Wray was honored with a Legend in Film award at the 2003 Palm Beach International Film Festival. For her contribution to the motion picture industry, Wray was honored with a star on the Hollywood Walk of Fame at 6349 Hollywood Blvd. She received a star posthumously on Canada's Walk of Fame in Toronto on June 5, 2005. A small park near Lee's Creek on Main Street in Cardston, Alberta, her birthplace, was named Fay Wray Park in her honour. The small sign at the edge of the park on Main Street has a silhouette of King Kong on it, remembering her role in the film "King Kong". A large oil portrait of Wray by Alberta artist Neil Boyle is on display in the Empress Theatre in Fort Macleod, Alberta. In May 2006, Wray became one of the first four entertainers to be honored by Canada Post by being featured on a postage stamp.

</doc>
<doc id="10967" url="https://en.wikipedia.org/wiki?curid=10967" title="Forgetting curve">
Forgetting curve

The forgetting curve hypothesizes the decline of memory retention in time. This curve shows how information is lost over time when there is no attempt to retain it. A related concept is the strength of memory that refers to the durability that memory traces in the brain. The stronger the memory, the longer period of time that a person is able to recall it. A typical graph of the forgetting curve purports to show that humans tend to halve their memory of newly learned knowledge in a matter of days or weeks unless they consciously review the learned material.
The forgetting curve supports one of the seven kinds of memory failures: transience, which is the process of forgetting that occurs with the passage of time.
From 1880 to 1885, Hermann Ebbinghaus ran a limited, incomplete study on himself and published his hypothesis in 1885 as "" (later translated into English as "Memory: A Contribution to Experimental Psychology"). Ebbinghaus studied the memorisation of nonsense syllables, such as "WID" and "ZOF" (CVCs or Consonant–Vowel–Consonant) by repeatedly testing himself after various time periods and recording the results. He plotted these results on a graph creating what is now known as the "forgetting curve". Ebbinghaus investigated the rate of forgetting, but not the effect of spaced repetition on the increase in retrievability of memories.
Ebbinghaus's publication also included an equation to approximate his forgetting curve:
formula_1
Here, formula_2 represents 'Savings' expressed as a percentage, and formula_3 represents time in minutes. Savings is defined as the relative amount of time saved on the second learning trial as a result of having had the first. A savings of 100% would indicate that all items were still known from the first trial. A 75% savings would mean that relearning missed items required 25% as long as the original learning session (to learn all items). 'Savings' is thus, analogous to retention rate.
In 2015, an attempt to replicate the forgetting curve with one study subject has shown the experimental results similar to Ebbinghaus' original data.
Hermann's experiment contributed a lot to experimental psychology. He was the first to carry out a series of well-designed experiments on the subject of forgetting, and he was one of the first to choose artificial stimuli in the research of experimental psychology. Since his introduction of nonsense syllables, a large number of experiments in experimental psychology has been based on highly controlled artificial stimuli.
Hermann Ebbinghaus hypothesized that the speed of forgetting depends on a number of factors such as the difficulty of the learned material (e.g. how meaningful it is), its representation and other physiological factors such as stress and sleep. He further hypothesized that the basal forgetting rate differs little between individuals. He concluded that the difference in performance can be explained by mnemonic representation skills.
He went on to hypothesize that basic training in mnemonic techniques can help overcome those differences in part. He asserted that the best methods for increasing the strength of memory are:
His premise was that each repetition in learning increases the optimum interval before the next repetition is needed (for near-perfect retention, initial repetitions may need to be made within days, but later they can be made after years). He discovered that information is easier to recall when it's built upon things you already know, and the forgetting curve was flattened by every repetition. It appeared that by applying frequent training in learning, the information was solidified by repeated recalling.
Later research also suggested that, other than the two factors Ebbinghaus proposed, higher original learning would also produce slower forgetting. The more information was originally learned, the slower the forgetting rate would be.
Spending time each day to remember information will greatly decrease the effects of the forgetting curve. Some learning consultants claim reviewing material in the first 24 hours after learning information is the optimum time to re-read notes and reduce the amount of knowledge forgotten. Evidence suggests waiting 10–20% of the time towards when the information will be needed is the optimum time for a single review.
However, some memories remain free from the detrimental effects of interference and do not necessarily follow the typical forgetting curve as various noise and outside factors influence what information would be remembered. There is debate among supporters of the hypothesis about the shape of the curve for events and facts that are more significant to the subject. Some supporters, for example, suggest that memories of shocking events such as the Kennedy Assassination or 9/11 are vividly imprinted in memory (flashbulb memory). Others have compared contemporaneous written recollections with recollections recorded years later, and found considerable variations as the subject's memory incorporates after-acquired information. There is considerable research in this area as it relates to eyewitness identification testimony, and eyewitness accounts are found demonstrably unreliable.
Many equations have since been proposed to approximate forgetting, perhaps the simplest being an exponential curve described by the equation
formula_4
where formula_5 is retrievability (a measure of how easy it is to retrieve a piece of information from memory), formula_6 is stability of memory (determines how fast formula_5 falls over time in the absence of training, testing or other recall), and formula_3 is time.
Simple equations such as this one were not found to provide a good fit to the available data.

</doc>
<doc id="10969" url="https://en.wikipedia.org/wiki?curid=10969" title="Field-programmable gate array">
Field-programmable gate array

A field-programmable gate array (FPGA) is an integrated circuit designed to be configured by a customer or a designer after manufacturinghence the term "field-programmable". The FPGA configuration is generally specified using a hardware description language (HDL), similar to that used for an application-specific integrated circuit (ASIC). Circuit diagrams were previously used to specify the configuration, but this is increasingly rare due to the advent of electronic design automation tools.
FPGAs contain an array of programmable logic blocks, and a hierarchy of "reconfigurable interconnects" that allow the blocks to be "wired together", like many logic gates that can be inter-wired in different configurations. Logic blocks can be configured to perform complex combinational functions, or merely simple logic gates like AND and XOR. In most FPGAs, logic blocks also include memory elements, which may be simple flip-flops or more complete blocks of memory. Many FPGAs can be reprogrammed to implement different logic functions, allowing flexible reconfigurable computing as performed in computer software.
FPGAs have a remarkable role in the embedded system development due to capability to start system software (SW) development simultaneously with hardware (HW), 
enable system performance simulations at a very early phase of the development, and allow various system partitioning (SW and HW) trials and iterations before final freezing of the system architecture.
Contemporary field-programmable gate arrays (FPGAs) have large resources of logic gates and RAM blocks to implement complex digital computations. As FPGA designs employ very fast I/O rates and bidirectional data buses, it becomes a challenge to verify correct timing of valid data within setup time and hold time.
Floor planning enables resource allocation within FPGAs to meet these time constraints. FPGAs can be used to implement any logical function that an ASIC can perform. The ability to update the functionality after shipping, partial re-configuration of a portion of the design and the low non-recurring engineering costs relative to an ASIC design (notwithstanding the generally higher unit cost), offer advantages for many applications.
Some FPGAs have analog features in addition to digital functions. The most common analog feature is a programmable slew rate on each output pin, allowing the engineer to set low rates on lightly loaded pins that would otherwise ring or couple unacceptably, and to set higher rates on heavily loaded pins on high-speed channels that would otherwise run too slowly. Also common are quartz-crystal oscillators, on-chip resistance-capacitance oscillators, and phase-locked loops with embedded voltage-controlled oscillators used for clock generation and management and for high-speed serializer-deserializer (SERDES) transmit clocks and receiver clock recovery. Fairly common are differential comparators on input pins designed to be connected to differential signaling channels. A few "mixed signal FPGAs" have integrated peripheral analog-to-digital converters (ADCs) and digital-to-analog converters (DACs) with analog signal conditioning blocks allowing them to operate as a system-on-a-chip (SoC). Such devices blur the line between an FPGA, which carries digital ones and zeros on its internal programmable interconnect fabric, and field-programmable analog array (FPAA), which carries analog values on its internal programmable interconnect fabric.
The FPGA industry sprouted from programmable read-only memory (PROM) and programmable logic devices (PLDs). PROMs and PLDs both had the option of being programmed in batches in a factory or in the field (field-programmable). However, programmable logic was hard-wired between logic gates.
Altera was founded in 1983 and delivered the industry's first reprogrammable logic device in 1984 – the EP300 – which featured a quartz window in the package that allowed users to shine an ultra-violet lamp on the die to erase the EPROM cells that held the device configuration. In December 2015, Intel acquired Altera.
Xilinx co-founders Ross Freeman and Bernard Vonderschmitt invented the first commercially viable field-programmable gate array in 1985 – the XC2064. The XC2064 had programmable gates and programmable interconnects between gates, the beginnings of a new technology and market. The XC2064 had 64 configurable logic blocks (CLBs), with two three-input lookup tables (LUTs). More than 20 years later, Freeman was entered into the National Inventors Hall of Fame for his invention.
In 1987, the Naval Surface Warfare Center funded an experiment proposed by Steve Casselman to develop a computer that would implement 600,000 reprogrammable gates. Casselman was successful and a patent related to the system was issued in 1992.
Altera and Xilinx continued unchallenged and quickly grew from 1985 to the mid-1990s, when competitors sprouted up, eroding significant market share. By 1993, Actel (now Microsemi) was serving about 18 percent of the market. By 2013, Altera (31 percent), Actel (10 percent) and Xilinx (36 percent) together represented approximately 77 percent of the FPGA market.
The 1990s were a period of rapid growth for FPGAs, both in circuit sophistication and the volume of production. In the early 1990s, FPGAs were primarily used in telecommunications and networking. By the end of the decade, FPGAs found their way into consumer, automotive, and industrial applications.
Companies like Microsoft have started to use FPGAs to accelerate high-performance, computationally intensive systems (like the data centers that operate their Bing search engine), due to the performance per watt advantage FPGAs deliver. Microsoft began using FPGAs to accelerate Bing in 2014, and in 2018 began deploying FPGAs across other data center workloads for their Azure cloud computing platform.
In 2012 the coarse-grained architectural approach was taken a step further by combining the logic blocks and interconnects of traditional FPGAs with embedded microprocessors and related peripherals to form a complete "system on a programmable chip". This work mirrors the architecture created by Ron Perloff and Hanan Potash of Burroughs Advanced Systems Group in 1982 which combined a reconfigurable CPU architecture on a single chip called the SB24.
Examples of such hybrid technologies can be found in the Xilinx Zynq-7000 all Programmable SoC, which includes a 1.0 GHz dual-core ARM Cortex-A9 MPCore processor embedded within the FPGA's logic fabric or in the Altera Arria V FPGA, which includes an 800 MHz dual-core ARM Cortex-A9 MPCore. The Atmel FPSLIC is another such device, which uses an AVR processor in combination with Atmel's programmable logic architecture. The Microsemi SmartFusion devices incorporate an ARM Cortex-M3 hard processor core (with up to 512 kB of flash and 64 kB of RAM) and analog peripherals such as a multi-channel analog-to-digital converters and digital-to-analog converters to their flash memory-based FPGA fabric.
An alternate approach to using hard-macro processors is to make use of soft processor IP cores that are implemented within the FPGA logic. Nios II, MicroBlaze and Mico32 are examples of popular softcore processors. Many modern FPGAs are programmed at "run time", which has led to the idea of reconfigurable computing or reconfigurable systems – CPUs that reconfigure themselves to suit the task at hand. Additionally, new, non-FPGA architectures are beginning to emerge. Software-configurable microprocessors such as the Stretch S5000 adopt a hybrid approach by providing an array of processor cores and FPGA-like programmable cores on the same chip.
A "design start" is a new custom design for implementation on an FPGA.
Historically, FPGAs have been slower, less energy efficient and generally achieved less functionality than their fixed ASIC counterparts. An older study showed that designs implemented on FPGAs need on average 40 times as much area, draw 12 times as much dynamic power, and run at one third the speed of corresponding ASIC implementations.
More recently, FPGAs such as the Xilinx Virtex-7 or the Altera Stratix 5 have come to rival corresponding ASIC and ASSP ("Application-specific standard part", such as a standalone USB interface chip) solutions by providing significantly reduced power usage, increased speed, lower materials cost, minimal implementation real-estate, and increased possibilities for re-configuration 'on-the-fly'. A design that included 6 to 10 ASICs can now be achieved using only one FPGA.
Advantages of FPGAs include the ability to re-program when already deployed (i.e. "in the field") to fix bugs, and often include shorter time to market and lower non-recurring engineering costs. Vendors can also take a middle road via FPGA prototyping: developing their prototype hardware on FPGAs, but manufacture their final version as an ASIC so that it can no longer be modified after the design has been committed. This is often also the case with new processor designs.
Xilinx claimed that several market and technology dynamics are changing the ASIC/FPGA paradigm as of February 2009:
These trends make FPGAs a better alternative than ASICs for a larger number of higher-volume applications than they have been historically used for, to which the company attributes the growing number of FPGA design starts (see ).
Some FPGAs have the capability of partial re-configuration that lets one portion of the device be re-programmed while other portions continue running.
The primary differences between complex programmable logic devices (CPLDs) and FPGAs are architectural. A CPLD has a comparatively restrictive structure consisting of one or more programmable sum-of-products logic arrays feeding a relatively small number of clocked registers. As a result, CPLDs are less flexible, but have the advantage of more predictable timing delays and FPGA architectures, on the other hand, are dominated by interconnect. This makes them far more flexible (in terms of the range of designs that are practical for implementation on them) but also far more complex to design for, or at least requiring more complex electronic design automation (EDA) software.
In practice, the distinction between FPGAs and CPLDs is often one of size as FPGAs are usually much larger in terms of resources than CPLDs. Typically only FPGAs contain more complex embedded functions such as adders, multipliers, memory, and serializer/deserializers. Another common distinction is that CPLDs contain embedded flash memory to store their configuration while FPGAs usually require external non-volatile memory (but not always).
When a design requires simple instant-on (logic is already configured at power-up) CPLDs are generally preferred. For most other applications FPGAs are generally preferred. Sometimes both CPLDs and FPGAs are used in a single system design. In those designs, CPLDs generally perform glue logic functions, and are responsible for “booting” the FPGA as well as controlling reset and boot sequence of the complete circuit board. Therefore, depending on the application it may be judicious to use both FPGAs and CPLDs in a single design.
FPGAs have both advantages and disadvantages as compared to ASICs or secure microprocessors, concerning hardware security. FPGAs' flexibility makes malicious modifications during fabrication a lower risk. Previously, for many FPGAs, the design bitstream was exposed while the FPGA loads it from external memory (typically on every power-on). All major FPGA vendors now offer a spectrum of security solutions to designers such as bitstream encryption and authentication. For example, Altera and Xilinx offer AES encryption (up to 256-bit) for bitstreams stored in an external flash memory.
FPGAs that store their configuration internally in nonvolatile flash memory, such as Microsemi's ProAsic 3 or Lattice's XP2 programmable devices, do not expose the bitstream and do not need encryption. In addition, flash memory for a lookup table provides single event upset protection for space applications. Customers wanting a higher guarantee of tamper resistance can use write-once, antifuse FPGAs from vendors such as Microsemi.
With its Stratix 10 FPGAs and SoCs, Altera introduced a Secure Device Manager and physically uncloneable functions to provide high levels of protection against physical attacks.
In 2012 researchers Sergei Skorobogatov and Christopher Woods demonstrated that FPGAs can be vulnerable to hostile intent. They discovered a critical backdoor vulnerability had been manufactured in silicon as part of the Actel/Microsemi ProAsic 3 making it vulnerable on many levels such as reprogramming crypto and access keys, accessing unencrypted bitstream, modifying low-level silicon features, and extracting configuration data.
An FPGA can be used to solve any problem which is computable. This is trivially proven by the fact that FPGAs can be used to implement a soft microprocessor, such as the Xilinx MicroBlaze or Altera Nios II. Their advantage lies in that they are significantly faster for some applications because of their parallel nature and optimality in terms of the number of gates used for certain processes.
FPGAs originally began as competitors to CPLDs to implement glue logic for printed circuit boards. As their size, capabilities, and speed increased, FPGAs took over additional functions to the point where some are now marketed as full systems on chips (SoCs). Particularly with the introduction of dedicated multipliers into FPGA architectures in the late 1990s, applications which had traditionally been the sole reserve of digital signal processor hardware (DSPs) began to incorporate FPGAs instead.
Another trend in the use of FPGAs is hardware acceleration, where one can use the FPGA to accelerate certain parts of an algorithm and share part of the computation between the FPGA and a generic processor. The search engine Bing is noted for adopting FPGA acceleration for its search algorithm in 2014. , FPGAs are seeing increased use as AI accelerators including Microsoft's so-termed "Project Catapult" and for accelerating artificial neural networks for machine learning applications.
Traditionally, FPGAs have been reserved for specific vertical applications where the volume of production is small. For these low-volume applications, the premium that companies pay in hardware cost per unit for a programmable chip is more affordable than the development resources spent on creating an ASIC. , new cost and performance dynamics have broadened the range of viable applications.
The most common FPGA architecture consists of an array of logic blocks, I/O pads, and routing channels. Generally, all the routing channels have the same width (number of wires). Multiple I/O pads may fit into the height of one row or the width of one column in the array.
An application circuit must be mapped into an FPGA with adequate resources. While the number of CLBs/LABs and I/Os required is easily determined from the design, the number of routing tracks needed may vary considerably even among designs with the same amount of logic.
For example, a crossbar switch requires much more routing than a systolic array with the same gate count. Since unused routing tracks increase the cost (and decrease the performance) of the part without providing any benefit, FPGA manufacturers try to provide just enough tracks so that most designs that will fit in terms of lookup tables (LUTs) and I/Os can be routed. This is determined by estimates such as those derived from Rent's rule or by experiments with existing designs. , network-on-chip architectures for routing and interconnection are being developed.
In general, a logic block consists of a few logical cells (called ALM, LE, slice etc.). A typical cell consists of a 4-input LUT, a full adder (FA) and a D-type flip-flop, as shown above. The LUTs are in this figure split into two 3-input LUTs. In "normal mode" those are combined into a 4-input LUT through the left multiplexer (mux). In "arithmetic" mode, their outputs are fed to the adder. The selection of mode is programmed into the middle MUX. The output can be either synchronous or asynchronous, depending on the programming of the mux to the right, in the figure example. In practice, entire or parts of the adder are stored as functions into the LUTs in order to save space.
Modern FPGA families expand upon the above capabilities to include higher level functionality fixed in silicon. Having these common functions embedded in the circuit reduces the area required and gives those functions increased speed compared to building them from logical primitives. Examples of these include multipliers, generic DSP blocks, embedded processors, high speed I/O logic and embedded memories.
Higher-end FPGAs can contain high speed multi-gigabit transceivers and "hard IP cores" such as processor cores, Ethernet medium access control units, PCI/PCI Express controllers, and external memory controllers. These cores exist alongside the programmable fabric, but they are built out of transistors instead of LUTs so they have ASIC-level performance and power consumption without consuming a significant amount of fabric resources, leaving more of the fabric free for the application-specific logic. The multi-gigabit transceivers also contain high performance analog input and output circuitry along with high-speed serializers and deserializers, components which cannot be built out of LUTs. Higher-level physical layer (PHY) functionality such as line coding may or may not be implemented alongside the serializers and deserializers in hard logic, depending on the FPGA.
Most of the circuitry built inside of an FPGA is synchronous circuitry that requires a clock signal. FPGAs contain dedicated global and regional routing networks for clock and reset so they can be delivered with minimal skew. Also, FPGAs generally contain analog phase-locked loop and/or delay-locked loop components to synthesize new clock frequencies as well as attenuate jitter. Complex designs can use multiple clocks with different frequency and phase relationships, each forming separate clock domains. These clock signals can be generated locally by an oscillator or they can be recovered from a high speed serial data stream. Care must be taken when building clock domain crossing circuitry to avoid metastability. FPGAs generally contain block RAMs that are capable of working as dual port RAMs with different clocks, aiding in the construction of building FIFOs and dual port buffers that connect differing clock domains.
To shrink the size and power consumption of FPGAs, vendors such as Tabula and Xilinx have introduced 3D or stacked architectures. Following the introduction of its 28 nm 7-series FPGAs, Xilinx said that several of the highest-density parts in those FPGA product lines will be constructed using multiple dies in one package, employing technology developed for 3D construction and stacked-die assemblies.
Xilinx's approach stacks several (three or four) active FPGA dies side by side on a silicon interposer – a single piece of silicon that carries passive interconnect. The multi-die construction also allows different parts of the FPGA to be created with different process technologies, as the process requirements are different between the FPGA fabric itself and the very high speed 28 Gbit/s serial transceivers. An FPGA built in this way is called a "heterogeneous FPGA".
Altera's heterogeneous approach involves using a single monolithic FPGA die and connecting other die/technologies to the FPGA using Intel's embedded multi-die interconnect bridge (EMIB) technology.
To define the behavior of the FPGA, the user provides a design in a hardware description language (HDL) or as a schematic design. The HDL form is more suited to work with large structures because it's possible to specify high-level functional behavior rather than drawing every piece by hand. However, schematic entry can allow for easier visualization of a design and its component modules.
Using an electronic design automation tool, a technology-mapped netlist is generated. The netlist can then be fit to the actual FPGA architecture using a process called place-and-route, usually performed by the FPGA company's proprietary place-and-route software. The user will validate the map, place and route results via timing analysis, simulation, and other verification and validation methodologies. Once the design and validation process is complete, the binary file generated, typically using the FPGA vendor's proprietary software, is used to (re-)configure the FPGA. This file is transferred to the FPGA/CPLD via a serial interface (JTAG) or to an external memory device like an EEPROM.
The most common HDLs are VHDL and Verilog as well as extensions such as SystemVerilog. However, in an attempt to reduce the complexity of designing in HDLs, which have been compared to the equivalent of assembly languages, there are moves to raise the abstraction level through the introduction of alternative languages. National Instruments' LabVIEW graphical programming language (sometimes referred to as "G") has an FPGA add-in module available to target and program FPGA hardware.
To simplify the design of complex systems in FPGAs, there exist libraries of predefined complex functions and circuits that have been tested and optimized to speed up the design process. These predefined circuits are commonly called "intellectual property (IP) cores", and are available from FPGA vendors and third-party IP suppliers. They are rarely free, and typically released under proprietary licenses. Other predefined circuits are available from developer communities such as OpenCores (typically released under free and open source licenses such as the GPL, BSD or similar license), and other sources. Such designs are known as "open-source hardware."
In a typical design flow, an FPGA application developer will simulate the design at multiple stages throughout the design process. Initially the RTL description in VHDL or Verilog is simulated by creating test benches to simulate the system and observe results. Then, after the synthesis engine has mapped the design to a netlist, the netlist is translated to a gate-level description where simulation is repeated to confirm the synthesis proceeded without errors. Finally the design is laid out in the FPGA at which point propagation delays can be added and the simulation run again with these values back-annotated onto the netlist.
More recently, OpenCL (Open Computing Language) is being used by programmers to take advantage of the performance and power efficiencies that FPGAs provide. OpenCL allows programmers to develop code in the C programming language and target FPGA functions as OpenCL kernels using OpenCL constructs. For further information, see high-level synthesis and C to HDL.
In 2016, long-time industry rivals Xilinx and Altera (now an Intel subsidiary) were the FPGA market leaders. At that time, they controlled nearly 90 percent of the market.
Both Xilinx and Altera provide proprietary electronic design automation software for Windows and Linux (ISE/Vivado and Quartus) which enables engineers to design, analyze, simulate, and synthesize (compile) their designs.
Other manufacturers include:
In March 2010, Tabula announced their FPGA technology that uses time-multiplexed logic and interconnect that claims potential cost savings for high-density applications. On March 24, 2015, Tabula officially shut down.
On June 1, 2015, Intel announced it would acquire Altera for approximately $16.7 billion and completed the acquisition on December 30, 2015.

</doc>
<doc id="10971" url="https://en.wikipedia.org/wiki?curid=10971" title="Free-running sleep">
Free-running sleep

Free-running sleep is a sleep pattern that is not adjusted (entrained) to the 24-hour cycle in nature nor to any artificial cycle.
It occurs as the sleep disorder non-24-hour sleep-wake disorder or artificially as part of experiments used in the study of circadian and other rhythms in biology. Study subjects are shielded from all time cues, often by a constant light protocol, by a constant dark protocol or by the use of light/dark conditions to which the organism cannot entrain such as the ultrashort protocol of one hour dark and two hours light. Also, limited amounts of food may be made available at short intervals so as to avoid entrainment to mealtimes. Subjects are thus forced to live by their internal circadian "clocks".
The individual's or animal's circadian phase can be known only by the monitoring of some kind of output of the circadian system, the internal "body clock". The researcher can precisely determine, for example, the daily cycles of gene-activity, body temperature, blood pressure, hormone secretion and/or sleep and activity/alertness. Alertness in humans can be determined by many kinds of verbal and non-verbal tests; activity in animals by observation, for example of wheel-running in rodents.
When animals or people "free-run", experiments can be done to see what sort of signals, known as zeitgebers, are effective in entrainment. Also, much work has been done to see how long or short a circadian cycle can be entrained to various organisms. For example, some animals can be entrained to a 22-hour day, but they can not be entrained to a 20-hour day. In recent studies funded by the U.S. space industry, it has been shown that most humans can be entrained to a 23.5-hour day and to a 24.65-hour day.
The effect of unintended time cues is called "masking" and can totally confound experimental results. Examples of masking are morning rush traffic audible to the subjects, or researchers or maintenance staff visiting subjects on a regular schedule.
Non-24-hour sleep–wake disorder, also referred to as "free-running disorder" (FRD) or "Non-24", is one of the circadian rhythm sleep disorders in humans. It affects more than half of people who are totally blind and a smaller number of sighted individuals.
Among blind people, the cause is the inability to register, and therefore to entrain to, light cues. The many blind people who do entrain to the 24-hour light/dark cycle have eyes with functioning retinas including operative non-visual light-sensitive cells, ipRGCs. These ganglion cells, which contain melanopsin, convey their signals to the "circadian clock" via the retinohypothalamic tract (branching off from the optic nerve), linking the retina to the pineal gland.
Among sighted individuals, FRD usually first appears in the teens or early twenties. As with delayed sleep phase disorder (DSPS or DSPD), in the absence of neurological damage due to trauma or stroke, cases almost never appear after the age of 30. FRD affects more sighted males than sighted females. A quarter of sighted individuals with FRD also have an associated psychiatric condition, and a quarter of them have previously shown symptoms of DSPS.

</doc>
<doc id="10972" url="https://en.wikipedia.org/wiki?curid=10972" title="Fenrir">
Fenrir

In the "Prose Edda", additional information is given about Fenrir, including that, due to the gods' knowledge of prophecies foretelling great trouble from Fenrir and his rapid growth, the gods bound him and as a result Fenrir bit off the right hand of the god Týr. Depictions of Fenrir have been identified on various objects and scholarly theories have been proposed regarding Fenrir's relation to other canine beings in Norse mythology. Fenrir has been the subject of artistic depictions and he appears in literature.
Fenrir is mentioned in three stanzas of the poem "Völuspá" and in two stanzas of the poem "Vafþrúðnismál". In stanza 40 of the poem "Völuspá", a völva divulges to Odin that, in the east, an old woman sat in the forest Járnviðr "and bred there the broods of Fenrir. There will come from them all one of that number to be a moon-snatcher in troll's skin." Further into the poem the völva foretells that Odin will be consumed by Fenrir at Ragnarök:
In the stanza that follows the völva describes that Odin's "tall child of Triumph's Sire" (Odin's son Víðarr) will then come to "strike at the beast of slaughter" and with his hands he will drive a sword into the heart of "Hveðrungr's son," avenging the death of his father.
In the first of two stanzas mentioning Fenrir in "Vafþrúðnismál" Odin poses a question to the wise jötunn Vafþrúðnir:
In the stanza that follows Vafþrúðnir responds that Sól (here referred to as "Álfröðull") will bear a daughter before Fenrir attacks her, and that this daughter shall continue the paths of her deceased mother through the heavens.
In the "Prose Edda", Fenrir is mentioned in three books: "Gylfaginning", "Skáldskaparmál" and "Háttatal".
In chapter 13 of the "Prose Edda" book "Gylfaginning", Fenrir is first mentioned in a stanza quoted from "Völuspá". Fenrir is first mentioned in prose in chapter 25, where the enthroned figure of High tells Gangleri (described as King Gylfi in disguise) about the god Týr. High says that one example of Týr's bravery is that when the Æsir were luring Fenrir (referred to here as "Fenrisúlfr") to place the fetter Gleipnir on the wolf, Týr placed his hand within the wolf's mouth as a pledge. This was done at Fenrir's own request because he did not trust that the Æsir would let him go. As a result, when the Æsir refused to release him, he bit off Týr's hand at a location "now called the wolf-joint" (the wrist), causing Týr to be one-handed and "not considered to be a promoter of settlements between people."
In chapter 34, High describes Loki, and says that Loki had three children with a woman named Angrboða located in the land of Jötunheimr; Fenrisúlfr, the serpent Jörmungandr, and the female being Hel. High continues that, once the gods found that these three children were being brought up in the land of Jötunheimr, and when the gods "traced prophecies that from these siblings great mischief and disaster would arise for them" the gods expected a lot of trouble from the three children, partially due to the nature of the mother of the children, yet worse so due to the nature of their father.
High says that Odin sent the gods to gather the children and bring them to him. Upon their arrival, Odin threw Jörmungandr into "that deep sea that lies round all lands", and then threw Hel into Niflheim, and bestowed upon her authority over nine worlds. However, the Æsir brought up the wolf "at home", and only Týr had the courage to approach Fenrir, and give Fenrir food. The gods noticed that Fenrir was growing rapidly every day, and since all prophecies foretold that Fenrir was destined to cause them harm, the gods formed a plan. The gods prepared three fetters: The first, greatly strong, was called Leyding. They brought Leyding to Fenrir and suggested that the wolf try his strength with it. Fenrir judged that it was not beyond his strength, and so let the gods do what they wanted with it. At Fenrir's first kick the bind snapped, and Fenrir loosened himself from Leyding. The gods made a second fetter, twice as strong, and named it Dromi. The gods asked Fenrir to try the new fetter, and that should he break this feat of engineering, Fenrir would achieve great fame for his strength. Fenrir considered that the fetter was very strong, yet also that his strength had grown since he broke Leyding, yet that he would have to take some risks if he were to become famous. Fenrir allowed them to place the fetter.
When the Æsir exclaimed that they were ready, Fenrir shook himself, knocked the fetter to the ground, strained hard, and kicking with his feet, snapped the fetter – breaking it into pieces that flew far into the distance. High says that, as a result, to "loose from Leyding" or to "strike out of Dromi" have become sayings for when something is achieved with great effort. The Æsir started to fear that they would not be able to bind Fenrir, and so Odin sent Freyr's messenger Skírnir down into the land of Svartálfaheimr to "some dwarfs" and had them make a fetter called Gleipnir. The dwarves constructed Gleipnir from six mythical ingredients. After an exchange between Gangleri and High, High continues that the fetter was smooth and soft as a silken ribbon, yet strong and firm. The messenger brought the ribbon to the Æsir, and they thanked him heartily for completing the task.
The Æsir went out on to the lake Amsvartnir sent for Fenrir to accompany them, and continued to the island Lyngvi (Old Norse "a place overgrown with heather"). The gods showed Fenrir the silken fetter Gleipnir, told him to tear it, stated that it was much stronger than it appeared, passed it among themselves, used their hands to pull it, and yet it did not tear. However, they said that Fenrir would be able to tear it, to which Fenrir replied:
"It looks to me that with this ribbon as though I will gain no fame from it if I do tear apart such a slender band, but if it is made with art and trickery, then even if it does look thin, this band is not going on my legs."
The Æsir said Fenrir would quickly tear apart a thin silken strip, noting that Fenrir earlier broke great iron binds, and added that if Fenrir wasn't able to break slender Gleipnir then Fenrir is nothing for the gods to fear, and as a result would be freed. Fenrir responded:
"If you bind me so that I am unable to release myself, then you will be standing by in such a way that I should have to wait a long time before I got any help from you. I am reluctant to have this band put on me. But rather than that you question my courage, let someone put his hand in my mouth as a pledge that this is done in good faith."
With this statement, all of the Æsir look to one another, finding themselves in a dilemma. Everyone refused to place their hand in Fenrir's mouth until Týr put out his right hand and placed it into the wolf's jaws. When Fenrir kicked, Gleipnir caught tightly, and the more Fenrir struggled, the stronger the band grew. At this, everyone laughed, except Týr, who there lost his right hand. When the gods knew that Fenrir was fully bound, they took a cord called Gelgja (Old Norse "fetter") hanging from Gleipnir, inserted the cord through a large stone slab called Gjöll (Old Norse "scream"), and the gods fastened the stone slab deep into the ground. After, the gods took a great rock called Thviti (Old Norse "hitter, batterer"), and thrust it even further into the ground as an anchoring peg. Fenrir reacted violently; he opened his jaws very wide, and tried to bite the gods. 
Then the gods thrust a sword into his mouth. Its hilt touched the lower jaw and its point the upper one; by means of it the jaws of the wolf were spread apart and the wolf gagged.
Fenrir "howled horribly," saliva ran from his mouth, and this saliva formed the river Ván (Old Norse "hope"). There Fenrir will lie until Ragnarök. Gangleri comments that Loki created a "pretty terrible family" though important, and asks why the Æsir did not just kill Fenrir there since they expected great malice from him. High replies that "so greatly did the gods respect their holy places and places of sanctuary that they did not want to defile them with the wolf's blood even though the prophecies say that he will be the death of Odin."
In chapter 38, High says that there are many men in Valhalla, and many more who will arrive, yet they will "seem too few when the wolf comes." In chapter 51, High foretells that as part of the events of Ragnarök, after Fenrir's son Sköll has swallowed the sun and his other son Hati Hróðvitnisson has swallowed the moon, the stars will disappear from the sky. The earth will shake violently, trees will be uprooted, mountains will fall, and all binds will snap – Fenrisúlfr will be free. Fenrisúlfr will go forth with his mouth opened wide, his upper jaw touching the sky and his lower jaw the earth, and flames will burn from his eyes and nostrils. Later, Fenrisúlfr will arrive at the field Vígríðr with his sibling Jörmungandr. With the forces assembled there, an immense battle will take place. During this, Odin will ride to fight Fenrisúlfr. During the battle, Fenrisúlfr will eventually swallow Odin, killing him, and Odin's son Víðarr will move forward and kick one foot into the lower jaw of the wolf. This foot will bear a legendary shoe "for which the material has been collected throughout all time." With one hand, Víðarr will take hold of the wolf's upper jaw and tear apart his mouth, killing Fenrisúlfr. High follows this prose description by citing various quotes from "Völuspá" in support, some of which mention Fenrir.
In the Epilogue section of the "Prose Edda" book "Skáldskaparmál", a euhemerized monologue equates Fenrisúlfr to Pyrrhus, attempting to rationalize that "it killed Odin, and Pyrrhus could be said to be a wolf according to their religion, for he paid no respect to places of sanctuary when he killed the king in the temple in front of Thor's altar." In chapter 2, "wolf's enemy" is cited as a kenning for Odin as used by the 10th century skald Egill Skallagrímsson. In chapter 9, "feeder of the wolf" is given as a kenning for Týr and, in chapter 11, "slayer of Fenrisúlfr" is presented as a kenning for Víðarr. In chapter 50, a section of "Ragnarsdrápa" by the 9th century skald Bragi Boddason is quoted that refers to Hel, the being, as "the monstrous wolf's sister." In chapter 75, names for wargs and wolves are listed, including both "Hróðvitnir" and "Fenrir."
"Fenrir" appears twice in verse as a common noun for a "wolf" or "warg" in chapter 58 of "Skáldskaparmál", and in chapter 56 of the book "Háttatal". Additionally, the name "Fenrir" can be found among a list of jötnar in chapter 75 of "Skáldskaparmál".
At the end of the "Heimskringla" saga "Hákonar saga góða", the poem "Hákonarmál" by the 10th century skald Eyvindr skáldaspillir is presented. The poem is about the fall of King Haakon I of Norway; although he is Christian, he is taken by two valkyries to Valhalla, and is there received as one of the Einherjar. Towards the end of the poem, a stanza relates sooner will the bonds of Fenrir snap than as good a king as Haakon shall stand in his place:
, a partially surviving runestone erected at Kirk Andreas on the Isle of Man, depicts a bearded human holding a spear downward at a wolf, his right foot in its mouth, while a large bird sits at his shoulder. Rundata dates it to 940, while Pluskowski dates it to the 11th century. This depiction has been interpreted as Odin, with a raven or eagle at his shoulder, being consumed by Fenrir at Ragnarök. On the reverse of the stone is another image parallel to it that has been described as Christ triumphing over Satan. These combined elements have led to the cross as being described as "syncretic art"; a mixture of pagan and Christian beliefs.
The mid-11th century Gosforth Cross, located in Cumbria, England, has been described as depicting a combination of scenes from the Christian Judgement Day and the pagan Ragnarök. The cross features various figures depicted in Borre style, including a man with a spear facing a monstrous head, one of whose feet is thrust into the beast's forked tongue and on its lower jaw, while a hand is placed against its upper jaw, a scene interpreted as Víðarr fighting Fenrir. This depiction has been theorized as a metaphor for Christ's defeat of Satan.
The 11th century Ledberg stone in Sweden, similarly to Thorwald's Cross, features a figure with his foot at the mouth of a four-legged beast, and this may also be a depiction of Odin being devoured by Fenrir at Ragnarök. Below the beast and the man is a depiction of a legless, helmeted man, with his arms in a prostrate position. The Younger Futhark inscription on the stone bears a commonly seen memorial dedication, but is followed by an encoded runic sequence that has been described as "mysterious," and "an interesting magic formula which is known from all over the ancient Norse world."
If the images on the Tullstorp Runestone are correctly identified as depicting Ragnarök, then Fenrir is shown above the ship Naglfar.
Meyer Schapiro theorizes a connection between the "Hell Mouth" that appears in medieval Christian iconography and Fenrir. According to Schapiro, "the Anglo-Saxon taste for the Hell Mouth was perhaps influenced by the northern pagan myth of the Crack of Doom and the battle with the wolf, who devoured Odin."
Scholars propose that a variety of objects from the archaeological record depict Týr. For example, a Migration Period gold bracteate from Trollhättan, Sweden, features a person receiving a bite on the hand from a beast, which may depict Týr and Fenrir. A Viking Age hogback in Sockburn, County Durham, North East England may depict Týr and Fenrir.
In reference to Fenrir's presentation in the "Prose Edda", Andy Orchard theorizes that "the hound (or wolf)" Garmr, Sköll, and Hati Hróðvitnisson were originally simply all Fenrir, stating that "Snorri, characteristically, is careful to make distinctions, naming the wolves who devour the sun and moon as Sköll and Hati Hróðvitnisson respectively, and describing an encounter between Garm and Týr (who, one would have thought, might like to get his hand on Fenrir) at Ragnarök."
John Lindow says that it is unclear why the gods decide to raise Fenrir as opposed to his siblings Hel and Jörmungandr in "Gylfaginning" chapter 35, theorizing that it may be "because Odin had a connection with wolves? Because Loki was Odin's blood brother?" Referring to the same chapter, Lindow comments that neither of the phrases that Fenrir's binding result in have left any other traces. Lindow compares Fenrir's role to his father Loki and Fenrir's sibling Jörmungandr, in that they all spend time with the gods, are bound or cast out by them, return "at the end of the current mythic order to destroy them, only to be destroyed himself as a younger generation of gods, one of them his slayer, survives into the new world order." He also points to Fenrir's binding as part of a recurring theme of the bound monster, where an enemy of the gods is bound, but destined to break free at Ragnarok.
Indo-European parallels have been proposed between myths of Fenrir and the Persian demon Ahriman. The Yashts refer to a story where Taxma Urupi rode Angra Mainyu as a horse for thirty years. An elaboration of this allusion is found only in a late Parsi commentary. The ruler Taxmoruw (Taxma Urupi) managed to lasso Ahriman (Angra Mainyu) and keep him tied up while taking him for a ride three times a day. After thirty years, Ahriman outwitted and swallowed Taxmoruw. In a sexual encounter with Ahriman, Jamshid, Taxmoruw's brother, inserted his hand into Ahriman's anus and pulled out his brother's corpse. His hand withered from contact with the diabolic innards. The suggested parallels with Fenrir myths are the binding of an evil being by a ruler figure and the subsequent swallowing of the ruler figure by the evil being (Odin and Fenrir), trickery involving the thrusting of a hand into a monster's orifice and the affliction of the inserted limb (Týr and Fenrir).
Ethologist Valerius Geist wrote that Fenrir's maiming and ultimate killing of Odin, who had previously nurtured him, was likely based on true experiences of wolf-behaviour, seeing as wolves are genetically encoded to rise up in the pack hierarchy and have, on occasion, been recorded to rebel against, and kill, their parents. Geist states that "apparently, even the ancients knew that wolves may turn on their parents and siblings and kill them."
Fenrir has been depicted in the artwork "Odin and Fenris" (1909) and "The Binding of Fenris" (around 1900) by Dorothy Hardy, "Odin und Fenriswolf" and "Fesselung des Fenriswolfe" (1901) by Emil Doepler, and is the subject of the metal sculpture "Fenrir" by Arne Vinje Gunnerud located on the island of Askøy, Norway.
Fenrir appears in modern literature in the poem "Om Fenrisulven og Tyr" (1819) by Adam Gottlob Oehlenschläger (collected in "Nordens Guder"), the novel "Der Fenriswolf" by K. H. Strobl, and "Til kamp mod dødbideriet" (1974) by E. K. Reich and E. Larsen.
Fenrir also appears in at least three Young Adult fiction books. First, he inspired the werewolf Fenrir Greyback in the "Harry Potter" series by J.K. Rowling. He also appears in the form of Fenris Wolf in "Magnus Chase and the Gods of Asgard", by Rick Riordan, as the main adversary in the first book of the series. His influence is also seen in Sarah J. Maas' "Throne of Glass" series in the character Fenrys, who can transform into a large wolf.
Fenris Ulf (also known as Maugrim) is a wolf and the Captain of the White Witch's Secret Police in C. S. Lewis's novel "The Lion, the Witch and the Wardrobe". The character is named "Fenris Ulf" in American editions of the book until the 1990s, as well as in the 1979 animated adaptation.
Fenris as a minion of Hela appears in the 2017 Marvel Studios film "".
Fenrir was also the influence for Carcharoth, an evil wolf serving Morgoth in J. R. R. Tolkien's fantasy world of Middle-earth.

</doc>
<doc id="10974" url="https://en.wikipedia.org/wiki?curid=10974" title="Final Fantasy">
Final Fantasy

"Final Fantasy" installments are generally stand-alone stories or role playing games, each with different settings, plots and main characters, but the franchise is linked by several recurring elements, including game mechanics and recurring character names. Each plot centers on a particular group of heroes who are battling a great evil, but also explores the characters' internal struggles and relationships. Character names are frequently derived from the history, languages, pop culture, and mythologies of cultures worldwide. The mechanics of each game involve similar battle systems and maps.
The "Final Fantasy" video game series has been both critically and commercially successful, selling more than software units worldwide, making it one of the best-selling video game franchises of all time. The series is well known for its innovation, visuals, and music, such as the inclusion of full-motion videos (FMVs), photorealistic character models, and music by Nobuo Uematsu. It has popularized many features now common in role-playing games, also popularizing the genre as a whole in markets outside Japan.
The first installment of the series was released in Japan on December 18, 1987. Subsequent games are numbered and given a story unrelated to previous games, so the numbers refer to volumes rather than to sequels. Many "Final Fantasy" games have been localized for markets in North America, Europe, and Australia on numerous video game consoles, personal computers (PC), and mobile phones. Future installments will appear on seventh and eighth generation consoles. As of November 2016, the series includes the main installments from "Final Fantasy" to "Final Fantasy XV", as well as direct sequels and spin-offs, both released and confirmed as being in development. Most of the older games have been remade or re-released on multiple platforms.
Three "Final Fantasy" installments were released on the Nintendo Entertainment System (NES). "Final Fantasy" was released in Japan in 1987 and in North America in 1990. It introduced many concepts to the console RPG genre, and has since been remade on several platforms. "Final Fantasy II", released in 1988 in Japan, has been bundled with "Final Fantasy" in several re-releases. The last of the NES installments, "Final Fantasy III", was released in Japan in 1990; however, it was not released elsewhere until a Nintendo DS remake in 2006.
The Super Nintendo Entertainment System (SNES) also featured three installments of the main series, all of which have been re-released on several platforms. "Final Fantasy IV" was released in 1991; in North America, it was released as "Final Fantasy II". It introduced the "Active Time Battle" system. "Final Fantasy V", released in 1992 in Japan, was the first game in the series to spawn a sequel: a short anime series, "". "Final Fantasy VI" was released in Japan in 1994, titled "Final Fantasy III" in North America.
The PlayStation console saw the release of three main "Final Fantasy" games. "Final Fantasy VII" (1997) moved away from the two-dimensional (2D) graphics used in the first six games to three-dimensional (3D) computer graphics; the game features polygonal characters on pre-rendered backgrounds. It also introduced a more modern setting, a style that was carried over to the next game. It was also the second in the series to be released in Europe, with the first being "Final Fantasy Mystic Quest". "Final Fantasy VIII" was published in 1999, and was the first to consistently use realistically proportioned characters and feature a vocal piece as its theme music. "Final Fantasy IX", released in 2000, returned to the series' roots by revisiting a more traditional "Final Fantasy" setting rather than the more modern worlds of "VII" and "VIII".
Three main installments, as well as one online game, were published for the PlayStation 2 (PS2). "Final Fantasy X" (2001) introduced full 3D areas and voice acting to the series, and was the first to spawn a sub-sequel ("Final Fantasy X-2", published in 2003). The first massively multiplayer online role-playing game (MMORPG) in the series, "Final Fantasy XI", was released on the PS2 and PC in 2002, and later on the Xbox 360. It introduced real-time battles instead of random encounters. "Final Fantasy XII", published in 2006, also includes real-time battles in large, interconnected playfields. The game is also the first in the main series to utilize a world used in a previous game, namely the land of Ivalice, which had previously featured in "Final Fantasy Tactics" and "Vagrant Story".
In 2009, "Final Fantasy XIII" was released in Japan, and in North America and Europe the following year, for PlayStation 3 and Xbox 360. It is the flagship installment of the "Fabula Nova Crystallis Final Fantasy" series and became the first mainline game to spawn two sub-sequels ("XIII-2" and '). It was also the first game released in Chinese and high definition along with being released on two consoles at once. "Final Fantasy XIV", a MMORPG, was released worldwide on Microsoft Windows in 2010, but it received heavy criticism when it was launched, prompting Square Enix to rerelease the game as ', this time to the PlayStation 3 as well, in 2013. "Final Fantasy XV" is an action role-playing game that was released for PlayStation 4 and Xbox One in 2016. Originally a "XIII" spin-off titled "Versus XIII", "XV" uses the mythos of the "Fabula Nova Crystallis" series, although in many other respects the game stands on its own and has since been distanced from the series by its developers. The next mainline entry, "Final Fantasy XVI", was announced in September 2020 for the PlayStation 5.
"Final Fantasy" has spawned numerous spin-offs and metaseries. Several are, in fact, not "Final Fantasy" games, but were rebranded for North American release. Examples include the "SaGa" series, rebranded "The Final Fantasy Legend", and its two sequels, "Final Fantasy Legend II" and "Final Fantasy Legend III". "Final Fantasy Mystic Quest" was specifically developed for a United States audience, and "Final Fantasy Tactics" is a tactical RPG that features many references and themes found in the series. The spin-off "Chocobo" series, "Crystal Chronicles" series, and "Kingdom Hearts" series also include multiple "Final Fantasy" elements. In 2003, the "Final Fantasy" series' first sub-sequel, "Final Fantasy X-2", was released. "Final Fantasy XIII" was originally intended to stand on its own, but the team wanted to explore the world, characters and mythos more, resulting in the development and release of two sequels in 2011 and respectively, creating the series' first official trilogy. "Dissidia Final Fantasy" was released in 2009, a fighting game that features heroes and villains from the first ten games of the main series. It was followed by a prequel in 2011. Other spin-offs have taken the form of subseries—"Compilation of Final Fantasy VII", "Ivalice Alliance", and "Fabula Nova Crystallis Final Fantasy". Enhanced 3D remakes of "Final Fantasy III" and "Final Fantasy IV" would be released in 2006 and 2007 respectively. "Final Fantasy VII Remake" was released on the PlayStation 4 in 2020.
Square Enix has expanded the "Final Fantasy" series into various media. Multiple anime and computer-generated imagery (CGI) films have been produced that are based either on individual "Final Fantasy" games or on the series as a whole. The first was an original video animation (OVA), ', a sequel to "Final Fantasy V". The story was set in the same world as the game, although 200 years in the future. It was released as four 30-minute episodes, first in Japan in 1994 and later in the United States by Urban Vision in 1998. In 2001, Square Pictures released its first feature film, '. The film is set on a future Earth invaded by alien life forms. "The Spirits Within" was the first animated feature to seriously attempt to portray photorealistic CGI humans, but was considered a box office bomb and garnered mixed reviews.
A 25-episode anime television series, "," was released in 2001 based on the common elements of the "Final Fantasy" series. It was broadcast in Japan by TV Tokyo and released in North America by ADV Films.
In 2005, ', a feature length direct-to-DVD CGI film, and ', a non-canon OVA, were released as part of the "Compilation of Final Fantasy VII". "Advent Children" was animated by Visual Works, which helped the company create CG sequences for the games. The film, unlike "The Spirits Within", became a commercial success. "Last Order," on the other hand, was released in Japan in a special DVD bundle package with "Advent Children". "Last Order" sold out quickly and was positively received by Western critics, though fan reaction was mixed over changes to established story scenes.
Two animated tie-ins for "Final Fantasy XV" were announced at the Uncovered Final Fantasy XV fan and press event, forming part of a larger multimedia project dubbed the "Final Fantasy XV" Universe. ' is a series of five 10-to-20-minute-long episodes developed by A-1 Pictures and Square Enix detailing the backstories of the main cast. ', a CGI movie set for release prior to the game in Summer 2016, is set during the game's opening and follows new and secondary characters.
On February 26, 2019 Square Enix released a short anime, produced by Satelight Inc, called "Final Fantasy XV: Episode Ardyn – Prologue" on their YouTube channel which acts as the background story for the final piece of DLC for "Final Fantasy XV" giving insight into Ardyn's past.
Square Enix also released "", an 8-episode Japanese soap opera. It features a mix of live-action scenes and "Final Fantasy XIV" gameplay footage. It premiered in Japan on April 16, 2017 and became available worldwide via Netflix in September of the same year.
It was announced in June 2019 that Sony Pictures Television is working on a first ever live-action adaptation of the series with Hivemind and Square Enix. Jason F. Brown, Sean Daniel and Dinesh Shamdasani for Hivemind will be the producers while Ben Lustig and Jake Thornton will write for the series and will serve as executive producers.
Several video games have either been adapted into or have had spin-offs in the form of manga and novels. The first was the novelization of "Final Fantasy II" in 1989, and was followed by a manga adaptation of "Final Fantasy III" in 1992. The past decade has seen an increase in the number of non-video game adaptations and spin-offs. "Final Fantasy: The Spirits Within" has been adapted into a novel, the spin-off game "Final Fantasy Crystal Chronicles" has been adapted into a manga, and "Final Fantasy XI" has had a novel and manga set in its continuity. Seven novellas based on the "Final Fantasy VII" universe have also been released. The "Final Fantasy: Unlimited" story was partially continued in novels and a manga after the anime series ended. The "Final Fantasy X" and "Final Fantasy XIII" series have also had novellas and audio dramas released. Two games, "Final Fantasy Tactics Advance" and "Final Fantasy: Unlimited", have been adapted into radio dramas.
A trading card game named the "Final Fantasy trading card game" is produced by Square Enix and Hobby Japan, first released Japan in 2012 with an English version in 2016. The game has been compared to "", and a tournament circuit for the game also takes place.
Although most "Final Fantasy" installments are independent, many gameplay elements recur throughout the series. Most games contain elements of fantasy and science fiction and feature recycled names often inspired from various cultures' history, languages and mythology, including Asian, European, and Middle-Eastern. Examples include weapon names like Excalibur and Masamune—derived from Arthurian legend and the Japanese swordsmith Masamune respectively—as well as the spell names Holy, Meteor, and Ultima. Beginning with "Final Fantasy IV", the main series adopted its current logo style that features the same typeface and an emblem designed by Japanese artist Yoshitaka Amano. The emblem relates to a game's plot and typically portrays a character or object in the story. Subsequent remakes of the first three games have replaced the previous logos with ones similar to the rest of the series.
The central conflict in many "Final Fantasy" games focuses on a group of characters battling an evil, and sometimes ancient, antagonist that dominates the game's world. Stories frequently involve a sovereign state in rebellion, with the protagonists taking part in the rebellion. The heroes are often destined to defeat the evil, and occasionally gather as a direct result of the antagonist's malicious actions. Another staple of the series is the existence of two villains; the main villain is not always who it appears to be, as the primary antagonist may actually be subservient to another character or entity. The main antagonist introduced at the beginning of the game is not always the final enemy, and the characters must continue their quest beyond what appears to be the final fight.
Stories in the series frequently emphasize the internal struggles, passions, and tragedies of the characters, and the main plot often recedes into the background as the focus shifts to their personal lives. Games also explore relationships between characters, ranging from love to rivalry. Other recurring situations that drive the plot include amnesia, a hero corrupted by an evil force, mistaken identity, and self-sacrifice. Magical orbs and crystals are recurring in-game items that are frequently connected to the themes of the games' plots. Crystals often play a central role in the creation of the world, and a majority of the "Final Fantasy" games link crystals and orbs to the planet's life force. As such, control over these crystals drives the main conflict. The classical elements are also a recurring theme in the series related to the heroes, villains, and items. Other common plot and setting themes include the Gaia hypothesis, an apocalypse, and conflicts between advanced technology and nature.
The series features a number of recurring character archetypes. Most famously, every game since "Final Fantasy II", including subsequent remakes of the original "Final Fantasy", features a character named Cid. Cid's appearance, personality, goals, and role in the game (non-playable ally, party member, villain) vary dramatically. However, two characteristics many versions of Cid have in common are 1) being a scientist or engineer, and 2) being tied in some way to an airship the party eventually acquires. Every Cid has at least one of these two traits.
Biggs and Wedge, inspired by two "Star Wars" characters of the same name, appear in numerous games as minor characters, sometimes as comic relief. The later games in the series feature several males with effeminate characteristics. Recurring creatures include Chocobos and Moogles. Chocobos are large, often flightless birds that appear in several installments as a means of long-distance travel for characters. Moogles, on the other hand, are white, stout creatures resembling teddy bears with wings and a single antenna. They serve different capacities in games including mail delivery, weaponsmiths, party members, and saving the game. Chocobo and Moogle appearances are often accompanied by specific musical themes that have been arranged differently for separate games. Final Fantasy is also well known for its enemy monsters and creatures.
In "Final Fantasy" games, players command a party of characters as they progress through the game's story by exploring the game world and defeating opponents. Enemies are typically encountered randomly through exploring, a trend which changed in "Final Fantasy XI" and "Final Fantasy XII". The player issues combat orders—like "Fight", "Magic", and "Item"—to individual characters via a menu-driven interface while engaging in battles. Throughout the series, the games have used different battle systems. Prior to "Final Fantasy XI", battles were turn-based with the protagonists and antagonists on different sides of the battlefield. "Final Fantasy IV" introduced the "Active Time Battle" (ATB) system that augmented the turn-based nature with a perpetual time-keeping system. Designed by Hiroyuki Ito, it injected urgency and excitement into combat by requiring the player to act before an enemy attacks, and was used until "Final Fantasy X", which implemented the "Conditional Turn-Based" (CTB) system. This new system returned to the previous turn-based system, but added nuances to offer players more challenge. "Final Fantasy XI" adopted a real-time battle system where characters continuously act depending on the issued command. "Final Fantasy XII" continued this gameplay with the "Active Dimension Battle" system. "Final Fantasy XIII"s combat system, designed by the same man who worked on "X", was meant to have an action-oriented feel, emulating the cinematic battles in "Final Fantasy VII: Advent Children". The latest installment to the franchise, Final Fantasy XV, introduces a new "Open Combat" system. Unlike previous battle systems in the franchise, the "Open Combat" system (OCS) allows players to take on a fully active battle scenario, allowing for free range attacks and movement, giving a much more fluid feel of combat. This system also incorporates a "Tactical" Option during battle, which pauses active battle to allow use of items.
Like most RPGs, the "Final Fantasy" installments use an experience level system for character advancement, in which experience points are accumulated by killing enemies. Character classes, specific jobs that enable unique abilities for characters, are another recurring theme. Introduced in the first game, character classes have been used differently in each game. Some restrict a character to a single job to integrate it into the story, while other games feature dynamic job systems that allow the player to choose from multiple classes and switch throughout the game. Though used heavily in many games, such systems have become less prevalent in favor of characters that are more versatile; characters still match an archetype, but are able to learn skills outside their class.
Magic is another common RPG element in the series. The method by which characters gain magic varies between installments, but is generally divided into classes organized by color: "White magic", which focuses on spells that assist teammates; "Black magic", which focuses on harming enemies; "Red magic", which is a combination of white and black magic, "Blue magic", which mimics enemy attacks; and "Green magic" which focuses on applying status effects to either allies or enemies. Other types of magic frequently appear such as "Time magic", focusing on the themes of time, space, and gravity; and "Summoning magic", which evokes legendary creatures to aid in battle and is a feature that has persisted since "Final Fantasy III". Summoned creatures are often referred to by names like "Espers" or "Eidolons" and have been inspired by mythologies from Arabic, Hindu, Norse, and Greek cultures.
Different means of transportation have appeared through the series. The most common is the airship for long range travel, accompanied by chocobos for travelling short distances, but others include sea and land vessels. Following "Final Fantasy VII", more modern and futuristic vehicle designs have been included.
In the mid-1980s, Square entered the Japanese video game industry with simple RPGs, racing games, and platformers for Nintendo's Famicom Disk System. In 1987, Square designer Hironobu Sakaguchi chose to create a new fantasy role-playing game for the cartridge-based NES, and drew inspiration from popular fantasy games: Enix's "Dragon Quest", Nintendo's "The Legend of Zelda", and Origin Systems's "Ultima" series. Though often attributed to the company allegedly facing bankruptcy, Sakaguchi explained that the game was his personal last-ditch effort in the game industry and that its title, "Final Fantasy", stemmed from his feelings at the time; had the game not sold well, he would have quit the business and gone back to university. Despite his explanation, publications have also attributed the name to the company's hopes that the project would solve its financial troubles. In 2015, Sakaguchi explained the name's origin: the team wanted a title that would abbreviate to "FF", which would sound good in Japanese. The name was originally going to be "Fighting Fantasy", but due to concerns over trademark conflicts with the roleplaying gamebook series of the same name, they needed to settle for something else. As the word "Final" was a famous word in Japan, Sakaguchi settled on that. According to Sakaguchi, any title that created the "FF" abbreviation would have done.
The game indeed reversed Square's lagging fortunes, and it became the company's flagship franchise. Following the success, Square immediately developed a second installment. Because Sakaguchi assumed "Final Fantasy" would be a stand-alone game, its story was not designed to be expanded by a sequel. The developers instead chose to carry over only thematic similarities from its predecessor, while some of the gameplay elements, such as the character advancement system, were overhauled. This approach has continued throughout the series; each major "Final Fantasy" game features a new setting, a new cast of characters, and an upgraded battle system. Video game writer John Harris attributed the concept of reworking the game system of each installment to Nihon Falcom's "Dragon Slayer" series, with which Square was previously involved as a publisher. The company regularly released new games in the main series. However, the time between the releases of "Final Fantasy XI" (2002), "Final Fantasy XII" (2006), and "Final Fantasy XIII" (2009) were much longer than previous games. Following "Final Fantasy XIV", Square Enix stated that it intended to release "Final Fantasy" games either annually or biennially. This switch was to mimic the development cycles of Western games in the "Call of Duty", "Assassin's Creed" and "Battlefield" series, as well as maintain fan-interest.
For the original "Final Fantasy", Sakaguchi required a larger production team than Square's previous games. He began crafting the game's story while experimenting with gameplay ideas. Once the gameplay system and game world size were established, Sakaguchi integrated his story ideas into the available resources. A different approach has been taken for subsequent games; the story is completed first and the game built around it. Designers have never been restricted by consistency, though most feel each game should have a minimum number of common elements. The development teams strive to create completely new worlds for each game, and avoid making new games too similar to previous ones. Game locations are conceptualized early in development and design details like building parts are fleshed out as a base for entire structures.
The first five games were directed by Sakaguchi, who also provided the original concepts. He drew inspiration for game elements from anime films by Hayao Miyazaki; series staples like the airships and chocobos are inspired by elements in "Castle in the Sky" and "Nausicaä of the Valley of the Wind", respectively. Sakaguchi served as a producer for subsequent games until he left Square in 2001. Yoshinori Kitase took over directing the games until "Final Fantasy VIII", and has been followed by a new director for each new game. Hiroyuki Ito designed several gameplay systems, including "Final Fantasy V"s "Job System", "Final Fantasy VIII"s "Junction System" and the Active Time Battle concept, which was used from "Final Fantasy IV" until "Final Fantasy IX". In designing the Active Time Battle system, Ito drew inspiration from Formula One racing; he thought it would be interesting if character types had different speeds after watching race cars pass each other. Ito also co-directed "Final Fantasy VI" with Kitase. Kenji Terada was the scenario writer for the first three games; Kitase took over as scenario writer for "Final Fantasy V" through "Final Fantasy VII". Kazushige Nojima became the series' primary scenario writer from "Final Fantasy VII" until his resignation in October 2003; he has since formed his own company, Stellavista. Nojima partially or completely wrote the stories for "Final Fantasy VII", "Final Fantasy VIII", "Final Fantasy X", and "Final Fantasy X-2". He also worked as the scenario writer for the spin-off series, "Kingdom Hearts". Daisuke Watanabe co-wrote the scenarios for "Final Fantasy X" and "XII", and was the main writer for the "XIII" games.
Artistic design, including character and monster creations, was handled by Japanese artist Yoshitaka Amano from "Final Fantasy" through "Final Fantasy VI". Amano also handled title logo designs for all of the main series and the image illustrations from "Final Fantasy VII" onward. Tetsuya Nomura was chosen to replace Amano because Nomura's designs were more adaptable to 3D graphics. He worked with the series from "Final Fantasy VII" through "Final Fantasy X"; for "Final Fantasy IX", however, character designs were handled by Shukō Murase, Toshiyuki Itahana, and Shin Nagasawa. Nomura is also the character designer of the "Kingdom Hearts" series, "Compilation of Final Fantasy VII", and "Fabula Nova Crystallis: Final Fantasy". Other designers include Nobuyoshi Mihara and Akihiko Yoshida. Mihara was the character designer for "Final Fantasy XI", and Yoshida served as character designer for "Final Fantasy Tactics", the Square-produced "Vagrant Story", and "Final Fantasy XII".
Because of graphical limitations, the first games on the NES feature small sprite representations of the leading party members on the main world screen. Battle screens use more detailed, full versions of characters in a side-view perspective. This practice was used until "Final Fantasy VI", which uses detailed versions for both screens. The NES sprites are 26 pixels high and use a color palette of 4 colors. 6 frames of animation are used to depict different character statuses like "healthy" and "fatigued". The SNES installments use updated graphics and effects, as well as higher quality audio than in previous games, but are otherwise similar to their predecessors in basic design. The SNES sprites are 2 pixels shorter, but have larger palettes and feature more animation frames: 11 colors and 40 frames respectively. The upgrade allowed designers to have characters be more detailed in appearance and express more emotions. The first game includes non-player characters (NPCs) the player could interact with, but they are mostly static in-game objects. Beginning with the second game, Square used predetermined pathways for NPCs to create more dynamic scenes that include comedy and drama.
In 1995, Square showed an interactive SGI technical demonstration of "Final Fantasy VI" for the then next generation of consoles. The demonstration used Silicon Graphics's prototype Nintendo 64 workstations to create 3D graphics. Fans believed the demo was of a new "Final Fantasy" game for the Nintendo 64 console; however, 1997 saw the release of "Final Fantasy VII" for the Sony PlayStation. The switch was due to a dispute with Nintendo over its use of faster but more expensive cartridges, as opposed to the slower and cheaper, but much higher capacity Compact Discs used on rival systems. "Final Fantasy VII" introduced 3D graphics with fully pre-rendered backgrounds. It was because of this switch to 3D that a CD-ROM format was chosen over a cartridge format. The switch also led to increased production costs and a greater subdivision of the creative staff for "Final Fantasy VII" and subsequent 3D games in the series.
Starting with "Final Fantasy VIII", the series adopted a more photo-realistic look. Like "Final Fantasy VII", full motion video (FMV) sequences would have video playing in the background, with the polygonal characters composited on top. "Final Fantasy IX" returned to the more stylized design of earlier games in the series, although it still maintained, and in many cases slightly upgraded, most of the graphical techniques used in the previous two games. "Final Fantasy X" was released on the PlayStation 2, and used the more powerful hardware to render graphics in real-time instead of using pre-rendered material to obtain a more dynamic look; the game features full 3D environments, rather than have 3D character models move about pre-rendered backgrounds. It is also the first "Final Fantasy" game to introduce voice acting, occurring throughout the majority of the game, even with many minor characters. This aspect added a whole new dimension of depth to the character's reactions, emotions, and development.
Taking a temporary divergence, "Final Fantasy XI" used the PlayStation 2's online capabilities as an MMORPG. Initially released for the PlayStation 2 with a PC port arriving six months later, "Final Fantasy XI" was also released on the Xbox 360 nearly four years after its original release in Japan. This was the first "Final Fantasy" game to use a free rotating camera. "Final Fantasy XII" was released in 2006 for the PlayStation 2 and uses only half as many polygons as "Final Fantasy X", in exchange for more advanced textures and lighting. It also retains the freely rotating camera from "Final Fantasy XI". "Final Fantasy XIII" and "Final Fantasy XIV" both make use of Crystal Tools, a middleware engine developed by Square Enix. 
The "Final Fantasy" games feature a variety of music, and frequently reuse themes. Most of the games open with a piece called "Prelude", which has evolved from a simple, 2-voice arpeggio in the early games to a complex, melodic arrangement in recent installments. Victories in combat are often accompanied by a victory fanfare, a theme that has become one of the most recognized pieces of music in the series. The basic theme that accompanies Chocobo appearances has been rearranged in a different musical style for each installment. A piece called "Prologue" (and sometimes "Final Fantasy"), originally featured in the first game, is often played during the ending credits. Although leitmotifs are common in the more character-driven installments, theme music is typically reserved for main characters and recurring plot elements.
Nobuo Uematsu was the primary composer of the "Final Fantasy" series until his resignation from Square Enix in November 2004. Other notable composers who have worked on main entries in the series include Masashi Hamauzu, Hitoshi Sakimoto, and Yoko Shimomura. Uematsu was allowed to create much of the music with little direction from the production staff. Sakaguchi, however, would request pieces to fit specific game scenes including battles and exploring different areas of the game world. Once a game's major scenarios were completed, Uematsu would begin writing the music based on the story, characters, and accompanying artwork. He started with a game's main theme, and developed other pieces to match its style. In creating character themes, Uematsu read the game's scenario to determine the characters' personality. He would also ask the scenario writer for more details to scenes he was unsure about. Technical limitations were prevalent in earlier games; Sakaguchi would sometimes instruct Uematsu to only use specific notes. It was not until "Final Fantasy IV" on the SNES that Uematsu was able to add more subtlety to the music.
Overall, the "Final Fantasy" series has been critically acclaimed and commercially successful, though each installment has seen different levels of success. The series has seen a steady increase in total sales; it sold over 10 million software units worldwide by early 1996, 45 million by August 2003, 63 million by December 2005, and 85 million by July 2008. In June 2011, Square Enix announced that the series had sold over units, and by March 2014, it had sold over 110 million units. Its high sales numbers have ranked it as one of the best-selling video game franchises in the industry; in January 2007, the series was listed as number three, and later in July as number four. As of 2019, the series had sold over 149 million units worldwide. As of April 2020, the series has sold over 154.5million units worldwide.
Several games within the series have become best-selling games. At the end of 2007, the seventh, eighth, and ninth best-selling RPGs were "Final Fantasy VII", "Final Fantasy VIII", and "Final Fantasy X" respectively. "Final Fantasy VII" has shipped over 12.3 million copies worldwide, earning it the position of the best-selling "Final Fantasy" game. Within two days of "Final Fantasy VIII"s North American release on September 9, 1999, it became the top-selling video game in the United States, a position it held for more than three weeks. "Final Fantasy X" sold over 1.4 million Japanese units in pre-orders alone, which set a record for the fastest-selling console RPG. The MMORPG, "Final Fantasy XI", reached over 200,000 active daily players in March 2006 and had reached over half a million subscribers by July 2007. "Final Fantasy XII" sold more than 1.7 million copies in its first week in Japan. By November 6, 2006—one week after its release—"Final Fantasy XII" had shipped approximately 1.5 million copies in North America. "Final Fantasy XIII" became the fastest-selling game in the franchise, and sold one million units on its first day of sale in Japan. "Final Fantasy XIV: A Realm Reborn", in comparison to its predecessor, was a runaway success, originally suffering from servers being overcrowded, and eventually gaining over one million unique subscribers within two months of its launch.
The series has received critical acclaim for the quality of its visuals and soundtracks. In 1996, "Next Generation" ranked the series collectively as the 17th best game of all time, speaking very highly of its graphics, music and stories. In 1999, "Next Generation" listed the "Final Fantasy" series as number 16 on their "Top 50 Games of All Time", commenting that, "By pairing state-of-the-art technology with memorable, sometimes shamelessly melodramatic storylines, the series has successfully outlasted its competitors [...] and improved with each new installation." It was awarded a star on the Walk of Game in 2006, making it the first franchise to win a star on the event (other winners were individual games, not franchises). WalkOfGame.com commented that the series has sought perfection as well as having been a risk taker in innovation. In 2006, GameFAQs held a contest for the best video game series ever, with "Final Fantasy" finishing as the runner-up to "The Legend of Zelda". In a 2008 public poll held by The Game Group plc, "Final Fantasy" was voted the best game series, with five games appearing in their "Greatest Games of All Time" list.
Many "Final Fantasy" games have been included in various lists of top games. Several games have been listed on multiple IGN "Top Games" lists. Twelve games were listed on "Famitsu"'s 2006 "Top 100 Favorite Games of All Time", four of which were in the top ten, with "Final Fantasy X" and "Final Fantasy VII" coming first and second, respectively. The series holds seven Guinness World Records in the "Guinness World Records Gamer's Edition 2008", which include the "Most Games in an RPG Series" (13 main games, seven enhanced games, and 32 spin-off games), the "Longest Development Period" (the production of "Final Fantasy XII" took five years), and the "Fastest-Selling Console RPG in a Single Day" ("Final Fantasy X"). The 2009 edition listed two games from the series among the top 50 consoles games: "Final Fantasy XII" at number 8 and "Final Fantasy VII" at number 20. In 2018, "Final Fantasy VII" was inducted as a member of the World Video Game Hall of Fame.
However, the series has garnered some criticism. IGN has commented that the menu system used by the games is a major detractor for many and is a "significant reason why they haven't touched the series." The site has also heavily criticized the use of random encounters in the series' battle systems. IGN further stated the various attempts to bring the series into film and animation have either been unsuccessful, unremarkable, or did not live up to the standards of the games. In 2007, "Edge" criticized the series for a number of related games that include the phrase "Final Fantasy" in their titles, which are considered inferior to previous games. It also commented that with the departure of Hironobu Sakaguchi, the series might be in danger of growing stale.
Several individual "Final Fantasy" games have garnered extra attention; some for their positive reception and others for their negative reception. "Final Fantasy VII" topped "GamePro's" "26 Best RPGs of All Time" list, as well as GameFAQs "Best Game Ever" audience polls in 2004 and 2005. Despite the success of "Final Fantasy VII", it is sometimes criticized as being overrated. In 2003, GameSpy listed it as the seventh most overrated game of all time, while IGN presented views from both sides. "" shipped 392,000 units in its first week of release, but received review scores that were much lower than that of other "Final Fantasy" games. A delayed, negative review after the Japanese release of "Dirge of Cerberus" from Japanese gaming magazine "Famitsu" hinted at a controversy between the magazine and Square Enix. Though "Final Fantasy: The Spirits Within" was praised for its visuals, the plot was criticized and the film was considered a box office bomb. "Final Fantasy Crystal Chronicles" for the GameCube received overall positive review scores, but reviews stated that the use of Game Boy Advances as controllers was a big detractor. The predominantly negative reception of the original version of "Final Fantasy XIV" caused then-president Yoichi Wada to issue an official apology during a Tokyo press conference, stating that the brand had been "greatly damaged" by the game's reception.
Various video game publications have created rankings of the mainline "Final Fantasy" games. In the table below, the lower the number given, the better the game is in the view of the respective publication. By way of comparison, the rating provided by the review aggregator "Metacritic" is also given; in this row, higher numbers indicate better reviews.
Final Fantasy has been very influential in the history of video game mechanics. "Final Fantasy IV" is considered a milestone for the genre, introducing a dramatic storyline with a strong emphasis on character development and personal relationships. "Final Fantasy VII" is credited as having the largest industry impact of the series, and with allowing console role-playing games to gain mass-market appeal. "Final Fantasy VII" is considered to be one of the most important and influential video games of all time.
The series affected Square's business on several levels. The commercial failure of "Final Fantasy: The Spirits Within" resulted in hesitation and delays from Enix during merger discussions with Square. Square's decision to produce games exclusively for the Sony PlayStation—a move followed by Enix's decision with the "Dragon Quest" series—severed their relationship with Nintendo. "Final Fantasy" games were absent from Nintendo consoles, specifically the Nintendo 64, for seven years. Critics attribute the switch of strong third-party games like the "Final Fantasy" and "Dragon Quest" games to Sony's PlayStation, and away from the Nintendo 64, as one of the reasons behind PlayStation being the more successful of the two consoles. The release of the Nintendo GameCube, which used optical disc media, in 2001 caught the attention of Square. To produce games for the system, Square created the shell company The Game Designers Studio and released "Final Fantasy Crystal Chronicles", which spawned its own metaseries within the main franchise. "Final Fantasy XI"s lack of an online method of subscription cancellation prompted the creation of legislation in Illinois that requires internet gaming services to provide such a method to the state's residents.
The series' popularity has resulted in its appearance and reference in numerous facets of popular culture like anime, TV series, and webcomics. Music from the series has permeated into different areas of culture. "Final Fantasy IV"s "Theme of Love" was integrated into the curriculum of Japanese school children and has been performed live by orchestras and metal bands. In 2003, Uematsu co-founded The Black Mages, an instrumental rock group independent of Square that has released albums of arranged "Final Fantasy" tunes. Bronze medalists Alison Bartosik and Anna Kozlova performed their synchronized swimming routine at the 2004 Summer Olympics to music from "Final Fantasy VIII". Many of the soundtracks have also been released for sale. Numerous companion books, which normally provide in-depth game information, have been published. In Japan, they are published by Square and are called "Ultimania" books.
The series has inspired numerous game developers. "Fable" creator Peter Molyneux considers "Final Fantasy VII" to be the RPG that "defined the genre" for him. BioWare founder Greg Zeschuk cited "Final Fantasy VII" as "the first really emotionally engaging game" he played and said it had "a big impact" on BioWare's work. "The Witcher 3" senior environmental artist Jonas Mattsson cited "Final Fantasy" as "a huge influence" and said it was "the first RPG" he played through. "Mass Effect" art director Derek Watts cited "Final Fantasy: The Spirits Within" as a major influence on the visual design and art direction of the series. BioWare senior product manager David Silverman cited "Final Fantasy XII"s gambit system as an influence on the gameplay of "". Ubisoft Toronto creative director Maxime Beland cited the original "Final Fantasy" as a major influence on him. Media Molecule's Constantin Jupp credited "Final Fantasy VII" with getting him into game design. Tim Schafer also cited "Final Fantasy VII" as one of his favourite games of all time.

</doc>
<doc id="10975" url="https://en.wikipedia.org/wiki?curid=10975" title="Fatty acid">
Fatty acid

The concept of fatty acid ("acide gras") was introduced in 1813 by Michel Eugène Chevreul, though he initially used some variant terms: "graisse acide" and "acide huileux" ("acid fat" and "oily acid").
Fatty acids are classified in many ways: by length, by saturation vs unsaturation, by even vs odd carbon content, and by linear vs branched.
Saturated fatty acids have no C=C double bonds. They have the same formula CH(CH)COOH, with variations in "n". An important saturated fatty acid is stearic acid (n = 16), which when neutralized with lye is the most common form of soap.
Unsaturated fatty acids have one or more C=C double bonds. The C=C double bonds can give either "cis" or "trans" isomers.
In most naturally occurring unsaturated fatty acids, each double bond has three (n-3), six (n-6), or nine (n-9) carbon atoms after it, and all double bonds have a cis configuration. Most fatty acids in the "trans" configuration (trans fats) are not found in nature and are the result of human processing (e.g., hydrogenation). Some trans fatty acids also occur naturally in the milk and meat of ruminants (such as cattle and sheep). They are produced, by fermentation, in the rumen of these animals. They are also found in dairy products from milk of ruminants, and may be also found in breast milk of women who obtained them from their diet.
The geometric differences between the various types of unsaturated fatty acids, as well as between saturated and unsaturated fatty acids, play an important role in biological processes, and in the construction of biological structures (such as cell membranes).
Most fatty acids are even-chained, e.g. stearic (C18) and oleic (C18), meaning that an even number of carbon atoms comprise them. Some fatty acids have odd numbers of carbon; they are referred to as odd-chained fatty acids (OCFA). The most common OCFA are the saturated C15 and C17 derivatives, respectively pentadecanoic acid and heptadecanoic acid, which are found in dairy products. On a molecular level, OCFAs are biosynthesized and metabolized slightly differently from the even-chained relatives.
Most naturally occurring fatty acids have an unbranched chain of carbon atoms, with a carboxyl group (–COOH) at one end, and a methyl group (–CH3) at the other end.
The position of the carbon atoms in the backbone of a fatty acid are usually indicated by counting from 1 at the −COOH end. Carbon number "x" is often abbreviated or C-"x" (or sometimes C"x"), with "x"=1, 2, 3, etc. This is the numbering scheme recommended by the IUPAC.
Another convention uses letters of the Greek alphabet in sequence, starting with the first carbon "after" the carboxyl. Thus carbon α (alpha) is C-2, carbon β (beta) is C-3, and so forth. 
Although fatty acids can be of diverse lengths, in this second convention the last carbon in the chain is always labelled as ω (omega), which is the last letter in the Greek alphabet. A third numbering convention counts the carbons from that end, using the labels "ω", "ω−1", "ω−2". Alternatively, the label "ω−"x"" is written "n−"x"", where the "n" is meant to represent the number of carbons in the chain.
In either numbering scheme, the position of a double bond in a fatty acid chain is always specified by giving the label of the carbon closest to the carboxyl end. Thus, in an 18 carbon fatty acid, a double bond between C-12 (or ω−6) and C-13 (or ω−5) is said to be "at" position C-12 or ω−6. The IUPAC naming of the acid, such as "octadec-12-enoic acid" (or the more pronounceable variant "12-octadecanoic acid") is always based on the "C" numbering. 
The notation Δ is traditionally used to specify a fatty acid with double bonds at positions "x","y"... (The capital Greek letter "Δ" (delta) corresponds to Roman "D", for Double bond). Thus, for example, the 20-carbon arachidonic acid is Δ, meaning that it has double bonds between carbons 5 and 6, 8 and 9, 11 and 12, and 14 and 15.
In the context of human diet and fat metabolism, unsaturated fatty acids are often classified the position of the double bond closest to the ω carbon (only), even in the case of multiple double bonds such as the essential fatty acids. Thus arachidonic acid, linoleic acid (18 carbons, Δ), and myristoleic acid (14-carbon, Δ) are all classified as "ω−3" fatty acids; meaning that their formula ends with –CH=CH––.
Fatty acids with an odd number of carbon atoms are called odd-chain fatty acids, whereas the rest are even-chain fatty acids. The difference is relevant to gluconeogenesis.
The following table describes the most common systems of naming fatty acids.
When circulating in the plasma (plasma fatty acids), not in their ester, fatty acids are known as non-esterified fatty acids (NEFAs) or free fatty acids (FFAs). FFAs are always bound to a transport protein, such as albumin.
Fatty acids are usually produced industrially by the hydrolysis of triglycerides, with the removal of glycerol (see oleochemicals). Phospholipids represent another source. Some fatty acids are produced synthetically by hydrocarboxylation of alkenes.
Hyper-oxygenated fatty acids are produced by a specific industrial processes for topical skin creams. The process is based on the introduction or saturation of peroxides into fatty acid esters via the presence of ultraviolet light and gaseous oxygen bubbling under controlled temperatures. Specifically linolenic acids have been shown to play an important role in maintaining the moisture barrier function of the skin (preventing water loss and skin dehydration). A study in Spain reported in the Journal of Wound Care in March 2005 compared a commercial product with a greasy placebo and that specific product was more effective and also cost-effective. A range of such OTC medical products is now widely available. However, topically applied olive oil was not found to be inferior in a "randomised triple-blind controlled non-inferiority" trial conducted in Spain during 2015. Commercial products are likely to be less messy to handle and more washable than either olive oil or petroleum jelly, both of which, if applied topically may stain clothing and bedding.
In animals, fatty acids are formed from carbohydrates predominantly in the liver, adipose tissue, and the mammary glands during lactation.
Carbohydrates are converted into pyruvate by glycolysis as the first important step in the conversion of carbohydrates into fatty acids. Pyruvate is then decarboxylated to form acetyl-CoA in the mitochondrion. However, this acetyl CoA needs to be transported into cytosol where the synthesis of fatty acids occurs. This cannot occur directly. To obtain cytosolic acetyl-CoA, citrate (produced by the condensation of acetyl-CoA with oxaloacetate) is removed from the citric acid cycle and carried across the inner mitochondrial membrane into the cytosol. There it is cleaved by ATP citrate lyase into acetyl-CoA and oxaloacetate. The oxaloacetate is returned to the mitochondrion as malate. The cytosolic acetyl-CoA is carboxylated by acetyl CoA carboxylase into malonyl-CoA, the first committed step in the synthesis of fatty acids.
Malonyl-CoA is then involved in a repeating series of reactions that lengthens the growing fatty acid chain by two carbons at a time. Almost all natural fatty acids, therefore, have even numbers of carbon atoms. When synthesis is complete the free fatty acids are nearly always combined with glycerol (three fatty acids to one glycerol molecule) to form triglycerides, the main storage form of fatty acids, and thus of energy in animals. However, fatty acids are also important components of the phospholipids that form the phospholipid bilayers out of which all the membranes of the cell are constructed (the cell wall, and the membranes that enclose all the organelles within the cells, such as the nucleus, the mitochondria, endoplasmic reticulum, and the Golgi apparatus).
The "uncombined fatty acids" or "free fatty acids" found in the circulation of animals come from the breakdown (or lipolysis) of stored triglycerides. Because they are insoluble in water, these fatty acids are transported bound to plasma albumin. The levels of "free fatty acids" in the blood are limited by the availability of albumin binding sites. They can be taken up from the blood by all cells that have mitochondria (with the exception of the cells of the central nervous system). Fatty acids can only be broken down in mitochondria, by means of beta-oxidation followed by further combustion in the citric acid cycle to CO and water. Cells in the central nervous system, although they possess mitochondria, cannot take free fatty acids up from the blood, as the blood-brain barrier is impervious to most free fatty acids, excluding short-chain fatty acids and medium-chain fatty acids. These cells have to manufacture their own fatty acids from carbohydrates, as described above, in order to produce and maintain the phospholipids of their cell membranes, and those of their organelles.
Studies on the cell membranes of mammals and reptiles discovered that mammalian cell membranes are composed of a higher proportion of polyunsaturated fatty acids (DHA, omega-3 fatty acid) than reptiles. Studies on bird fatty acid composition have noted similar proportions to mammals but with 1/3rd less omega-3 fatty acids as compared to omega-6 for a given body size. This fatty acid composition results in a more fluid cell membrane but also one that is permeable to various ions (H+ & Na+), resulting in cell membranes that are more costly to maintain. This maintenance cost has been argued to be one of the key causes for the high metabolic rates and concomitant warm-bloodedness of mammals and birds. However polyunsaturation of cell membranes may also occur in response to chronic cold temperatures as well. In fish increasingly cold environments lead to increasingly high cell membrane content of both monounsaturated and polyunsaturated fatty acids, to maintain greater membrane fluidity (and functionality) at the lower temperatures.
The following table gives the fatty acid, vitamin E and cholesterol composition of some common dietary fats.
Fatty acids exhibit reactions like other carboxylic acids, i.e. they undergo esterification and acid-base reactions.
Fatty acids do not show a great variation in their acidities, as indicated by their respective p"K". Nonanoic acid, for example, has a p"K" of 4.96, being only slightly weaker than acetic acid (4.76). As the chain length increases, the solubility of the fatty acids in water decreases, so that the longer-chain fatty acids have minimal effect on the pH of an aqueous solution. Even those fatty acids that are insoluble in water will dissolve in warm ethanol, and can be titrated with sodium hydroxide solution using phenolphthalein as an indicator. This analysis is used to determine the free fatty acid content of fats; i.e., the proportion of the triglycerides that have been hydrolyzed.
Neutralization of fatty acids, i.e. saponification, is a widely practiced route to metallic soaps.
Hydrogenation of unsaturated fatty acids is widely practiced. Typical conditions involve 2.0–3.0 MPa of H pressure, 150 °C, and nickel supported on silica as a catalyst. This treatment affords saturated fatty acids. The extent of hydrogenation is indicated by the iodine number. Hydrogenated fatty acids are less prone toward rancidification. Since the saturated fatty acids are higher melting than the unsaturated precursors, the process is called hardening. Related technology is used to convert vegetable oils into margarine. The hydrogenation of triglycerides (vs fatty acids) is advantageous because the carboxylic acids degrade the nickel catalysts, affording nickel soaps. During partial hydrogenation, unsaturated fatty acids can be isomerized from "cis" to "trans" configuration.
More forcing hydrogenation, i.e. using higher pressures of H and higher temperatures, converts fatty acids into fatty alcohols. Fatty alcohols are, however, more easily produced from fatty acid esters.
In the Varrentrapp reaction certain unsaturated fatty acids are cleaved in molten alkali, a reaction which was, at one point of time, relevant to structure elucidation.
Unsaturated fatty acids undergo a chemical change known as auto-oxidation. The process requires oxygen (air) and is accelerated by the presence of trace metals. Vegetable oils resist this process to a small degree because they contain antioxidants, such as tocopherol. Fats and oils often are treated with chelating agents such as citric acid to remove the metal catalysts.
Unsaturated fatty acids are susceptible to degradation by ozone. This reaction is practiced in the production of azelaic acid ((CH)(COH)) from oleic acid.
Short- and medium-chain fatty acids are absorbed directly into the blood via intestine capillaries and travel through the portal vein just as other absorbed nutrients do. However, long-chain fatty acids are not directly released into the intestinal capillaries. Instead they are absorbed into the fatty walls of the intestine villi and reassemble again into triglycerides. The triglycerides are coated with cholesterol and protein (protein coat) into a compound called a chylomicron.
From within the cell, the chylomicron is released into a lymphatic capillary called a lacteal, which merges into larger lymphatic vessels. It is transported via the lymphatic system and the thoracic duct up to a location near the heart (where the arteries and veins are larger). The thoracic duct empties the chylomicrons into the bloodstream via the left subclavian vein. At this point the chylomicrons can transport the triglycerides to tissues where they are stored or metabolized for energy.
When metabolized, fatty acids yield large quantities of ATP. Many cell types can use either glucose or fatty acids for this purpose. Fatty acids (provided either by ingestion or by drawing on triglycerides stored in fatty tissues) are distributed to cells to serve as a fuel for muscular contraction and general metabolism. They are broken down to CO and water by the intra-cellular mitochondria, releasing large amounts of energy, captured in the form of ATP through beta oxidation and the citric acid cycle.
Fatty acids that are required for good health but cannot be made in sufficient quantity from other substrates, and therefore must be obtained from food, are called essential fatty acids. There are two series of essential fatty acids: one has a double bond three carbon atoms away from the methyl end; the other has a double bond six carbon atoms away from the methyl end. Humans lack the ability to introduce double bonds in fatty acids beyond carbons 9 and 10, as counted from the carboxylic acid side. Two essential fatty acids are linoleic acid (LA) and alpha-linolenic acid (ALA). These fatty acids are widely distributed in plant oils. The human body has a limited ability to convert ALA into the longer-chain omega-3 fatty acids — eicosapentaenoic acid (EPA) and docosahexaenoic acid (DHA), which can also be obtained from fish. Omega-3 and omega-6 fatty acids are biosynthetic precursors to endocannabinoids with antinociceptive, anxiolytic, and neurogenic properties.
Blood fatty acids adopt distinct forms in different stages in the blood circulation. They are taken in through the intestine in chylomicrons, but also exist in very low density lipoproteins (VLDL) and low density lipoproteins (LDL) after processing in the liver. In addition, when released from adipocytes, fatty acids exist in the blood as free fatty acids.
It is proposed that the blend of fatty acids exuded by mammalian skin, together with lactic acid and pyruvic acid, is distinctive and enables animals with a keen sense of smell to differentiate individuals.
The chemical analysis of fatty acids in lipids typically begins with an interesterification step that breaks down their original esters (triglycerides, waxes, phospholipids etc) and converts them to methyl esters, which are then separated by gas chromatography. or analyzed by gas chromatography and mid-infrared spectroscopy. 
Separation of unsaturated isomers is possible by silver ion (argentation) thin-layer chromatography. Other separation techniques include high-performance liquid chromatography (with short columns packed with silica gel with bonded phenylsulfonic acid groups whose hydrogen atoms have been exchanged for silver ions). The role of silver lies in its ability to form complexes with unsaturated compounds.
Fatty acids are mainly used in the production of soap, both for cosmetic purposes and, in the case of metallic soaps, as lubricants. Fatty acids are also converted, via their methyl esters, to fatty alcohols and fatty amines, which are precursors to surfactants, detergents, and lubricants. Other applications include their use as emulsifiers, texturizing agents, wetting agents, anti-foam agents, or stabilizing agents.
Esters of fatty acids with simpler alcohols (such as methyl-, ethyl-, n-propyl-, isopropyl- and butyl esters) are used as emollients in cosmetics and other personal care products and as synthetic lubricants. Esters of fatty acids with more complex alcohols, such as sorbitol, ethylene glycol, diethylene glycol, and polyethylene glycol are consumed in food, or used for personal care and water treatment, or used as synthetic lubricants or fluids for metal working.

</doc>
<doc id="10977" url="https://en.wikipedia.org/wiki?curid=10977" title="Fearless (1993 film)">
Fearless (1993 film)

Fearless is a 1993 American drama film directed by Peter Weir and starring Jeff Bridges, Isabella Rossellini, Rosie Perez and John Turturro. It was written by Rafael Yglesias, adapted from his novel of the same name.
Rosie Perez was nominated for an Academy Award for Best Supporting Actress for her role as Carla Rodrigo. The film was also entered into the 44th Berlin International Film Festival. Jeff Bridges' role as Max Klein is widely regarded as one of the best performances of his career. The film's soundtrack features part of the first movement of Henryk Górecki's Symphony No. 3, subtitled "Symphony of Sorrowful Songs". The film's screenwriter was inspired to write the script after he was in a car accident. Yglesias began writing the story after reading about United Airlines Flight 232, that crashed in Sioux City, Iowa, in 1989.
Max Klein survives an airline crash. The plane plummets, but strangely Max is calm. His calm enables him to dispel fear in the flight cabin. He sits next to Byron Hummel, a young boy flying alone. Flight attendants move through the cabin, telling another passenger, Carla Rodrigo, traveling with an infant, to hold the infant in her lap as the plane plummets out of control, while telling other passengers to buckle into their seats. Max was telling his business partner, Jeff Gordon, of his fear of flying as they took off.
In the aftermath of the crash, most passengers died. Among the few survivors, most are terribly injured. Max is unhurt. The crash site is chaotic, filled with first responders and other emergency personnel. Focusing on the survivors, a team of investigators from the FAA and the airline company conduct interviews. Max is repelled by all the chaos. He is disgusted by the investigators wanting to interview him.
Max rents a car and starts driving home. Along the way he meets an old girlfriend from high school, Alison. They haven't met for the last 20 years. At the restaurant Alison notices Max eating a strawberry. Max is allergic to strawberries. Max grins. He finishes the strawberry without an allergic reaction. The next morning, he is accosted by FBI investigators. They question his choice to not contact family to tell them he is fine. The airline representative offers him train tickets. Max asks for airline tickets. He wants to fly home, having no fear of air travel. The airline books him on the flight. They seat him next to Dr. Bill Perlman, the airline's psychiatrist.
Dr. Perlman annoyingly tags behind Max back to his home, prodding him for information about the crash. Max is forced to snap back at the psychiatrist rudely, to be rid of him. Laura Klein, Max's wife, notices the strange behavior. Max seems different, changed somehow. Max's late business partner's wife, Nan Gordon, asks about Jeff's last moments. Max says Jeff died in the crash.
The media call Max "The Good Samaritan" in news reports. The boy Max sat next to, Byron, publicly thanks him in television interviews, for the way he comforted passengers while the plane fell out of control during the crash. Max is a hero.
Max avoids the press and becomes distant from Laura and his son Jonah. His persona is radically changed. He is preoccupied with his new perspective on life following his near death experience. He begins drawing abstract pictures of the crash. As he survived without injury, he thinks himself invulnerable to death. Because of his confidence, Dr. Perlman encourages Max to meet with another survivor, Carla Rodrigo, whose infant was held in her lap while the plane fell. Carla struggles with survivor's guilt, and is traumatized for not holding onto him tightly enough, although she was following the flight attendant's instructions (this was before child safety seats were required for infant passengers). Max and Carla develop a close friendship. He helps her to get past the trauma, to free herself from guilt, deliberately crashing his car to show that it was physically impossible for any person to hold onto anything due to the forces of the crash.
Attorney Steven Brillstein encourages Max to exaggerate testimony, to maximize the settlement offer from the airline. Max reluctantly agrees when he is confronted with Nan's financial predicament as a widow. Cognitive dissonance spurs Max to a panic attack. He runs out of the office, to the roof of the building. He climbs onto the roof's edge. As Max stands on the ledge, looking down at the streets below, his panic subsides. He rejoices in fearlessness. Laura finds Max on the ledge. He is spinning around on the ledge with his overcoat billowing across his face.
Brillstein arrives at the Klein home to celebrate the airline's settlement offer. He brings a fruit basket. Max eats one of the strawberries. This time he experiences an allergic reaction. Max is resuscitated by Laura and survives. He recovers his emotional connection to his family, to the world and to the reality of yet another chance at life.
A book containing the painting "The Ascent into the Empyrean" by Hieronymus Bosch is shown, and it is said that the dying go into the light of heaven "naked and alone". Near the finale as Max lies on the ground, he relives moving from the fuselage of the aircraft and for a moment moves towards the tunnel of light that appears to be modeled on the painting.
The film was positively received by critics as review aggregator Rotten Tomatoes evaluates that 85% of critics have given the film a positive review, based on 39 reviews, with an average score of 7.81/10. Its consensus states "This underrated gem from director Peter Weir features an outstanding performance from Jeff Bridges as a man dealing with profound grief."
With video and audio quality superseding previous home video releases, "Fearless" was released on Blu-ray Disc by the Warner Archive Collection in November 2013.

</doc>
<doc id="10979" url="https://en.wikipedia.org/wiki?curid=10979" title="Franklin D. Roosevelt">
Franklin D. Roosevelt

Franklin Delano Roosevelt (, ; January 30, 1882April 12, 1945), often referred to by his initials FDR, was an American politician who served as the 32nd president of the United States from 1933 until his death in 1945. A member of the Democratic Party, he won a record four presidential elections and became a central figure in world events during the first half of the 20th century. Roosevelt directed the federal government during most of the Great Depression, implementing his New Deal domestic agenda in response to the worst economic crisis in U.S. history. As a dominant leader of his party, he built the New Deal Coalition, which realigned American politics into the Fifth Party System and defined modern liberalism in the United States throughout the middle third of the 20th century. His third and fourth terms were dominated by World War II, which ended shortly after he died in office.
Roosevelt was born in Hyde Park, New York, to the Roosevelt family made well known by the reputation of Theodore Roosevelt, the 26th president of the United States, as well as by the reputation of prominent businessman William Henry Aspinwall. FDR graduated from Groton School and Harvard College, and attended Columbia Law School but left after passing the bar exam to practice law in New York City. In 1905, he married his fifth cousin once removed, Eleanor Roosevelt. They had six children, of whom five survived into adulthood. He won election to the New York State Senate in 1910, and then served as Assistant Secretary of the Navy under President Woodrow Wilson during World War I. Roosevelt was James M. Cox's running mate on the Democratic Party's 1920 national ticket, but Cox was defeated by Republican Warren G. Harding. In 1921, Roosevelt contracted a paralytic illness, believed at the time to be polio, and his legs became permanently paralyzed. While attempting to recover from his condition, Roosevelt founded a rehabilitation center in Warm Springs, Georgia, for people with poliomyelitis. In spite of being unable to walk unaided, Roosevelt returned to public office by winning election as Governor of New York in 1928. He served as governor from 1929 to 1933, promoting programs to combat the economic crisis besetting the United States.
In the 1932 presidential election, Roosevelt defeated Republican President Herbert Hoover in a landslide. Roosevelt took office in the midst of the Great Depression, the worst economic crisis in U.S. history. During the first 100 days of the 73rd United States Congress, Roosevelt spearheaded unprecedented federal legislation and issued a profusion of executive orders that instituted the New Deal — a variety of programs designed to produce relief, recovery, and reform. He created numerous programs to provide relief to the unemployed and farmers while seeking economic recovery with the National Recovery Administration and other programs. He also instituted major regulatory reforms related to finance, communications, and labor, and presided over the end of Prohibition. He used radio to speak directly to the American people, giving 30 "fireside chat" radio addresses during his presidency and becoming the first American president to be televised. With the economy having improved rapidly from 1933 to 1936, Roosevelt won a landslide reelection in 1936. However, the economy then relapsed into a deep recession in 1937 and 1938. After the 1936 election, Roosevelt sought passage of the Judiciary Reorganization Bill of 1937 (the "court packing plan"), which would have expanded the size of the Supreme Court of the United States. The bipartisan Conservative Coalition that formed in 1937 prevented passage of the bill and blocked the implementation of further New Deal programs and reforms. Major surviving programs and legislation implemented under Roosevelt include the Securities and Exchange Commission, the National Labor Relations Act, the Federal Deposit Insurance Corporation, Social Security, and the Fair Labor Standards Act of 1938.
The United States reelected FDR in 1940 for his third term, making him the only U.S. president to serve for more than two terms. With World War II looming after 1938, the U.S. remained officially neutral, but Roosevelt gave strong diplomatic and financial support to China, the United Kingdom and eventually the Soviet Union. Following the Japanese attack on Pearl Harbor on December 7, 1941, an event he famously called "a date which will live in infamy", Roosevelt obtained a congressional declaration of war on Japan, and, a few days later, on Germany and Italy. Assisted by his top aide Harry Hopkins and with very strong national support, he worked closely with British Prime Minister Winston Churchill, Soviet leader Joseph Stalin and Chinese Generalissimo Chiang Kai-shek in leading the Allied Powers against the Axis Powers. Roosevelt supervised the mobilization of the U.S. economy to support the war effort, and implemented a Europe first strategy, making the defeat of Germany a priority over that of Japan. He also initiated the development of the world's first atomic bomb, and worked with other Allied leaders to lay the groundwork for the United Nations and other post-war institutions. Roosevelt won reelection in 1944, but with his physical health declining during the war years, he died in April 1945, less than three months into his fourth term. The Axis Powers surrendered to the Allies in the months following Roosevelt's death, during the presidency of his successor, Harry S. Truman. Roosevelt is usually rated by scholars among the nation's greatest presidents, after George Washington and Abraham Lincoln, but has also been subject to substantial criticism.
Franklin Delano Roosevelt was born on January 30, 1882, in the Hudson Valley town of Hyde Park, New York, to businessman James Roosevelt I and his second wife, Sara Ann Delano. Roosevelt's parents, who were sixth cousins, both came from wealthy old New York families, the Roosevelts, the Aspinwalls and the Delanos, respectively. Roosevelt's patrilineal ancestor migrated to New Amsterdam in the 17th century, and the Roosevelts flourished as merchants and landowners. The Delano family progenitor, Philip Delano, traveled to the New World on the "Fortune" in 1621, and the Delanos prospered as merchants and shipbuilders in Massachusetts. Franklin had a half-brother, James "Rosy" Roosevelt, from his father's previous marriage.
Roosevelt grew up in a wealthy family. His father James graduated from Harvard Law School in 1851, but chose not to practice law after receiving an inheritance from his grandfather, James Roosevelt. Roosevelt's father was a prominent Bourbon Democrat who once took Franklin to meet President Grover Cleveland in the White House. His mother Sara was the dominant influence in Franklin's early years. She once declared, "My son Franklin is a Delano, not a Roosevelt at all." James, who was 54 when Franklin was born, was considered by some as a remote father, though biographer James MacGregor Burns indicates James interacted with his son more than was typical at the time.
Roosevelt learned to ride, shoot, row, and to play polo and lawn tennis. He took up golf in his teen years, becoming a skilled long hitter. He learned to sail early, and when he was 16, his father gave him a sailboat.
Frequent trips to Europe — he made his first excursion at the age of two and went with his parents every year from the ages of seven to fifteen — helped Roosevelt become conversant in German and French. Except for attending public school in Germany at age nine, Roosevelt was home-schooled by tutors until age 14. He then attended Groton School, an Episcopal boarding school in Groton, Massachusetts, joining the third form. Its headmaster, Endicott Peabody, preached the duty of Christians to help the less fortunate and urged his students to enter public service. Peabody remained a strong influence throughout Roosevelt's life, officiating at his wedding and visiting him as president.
Like most of his Groton classmates, Roosevelt went to Harvard College. Roosevelt was an average student academically, and he later declared, "I took economics courses in college for four years, and everything I was taught was wrong." He was a member of the Alpha Delta Phi fraternity and the Fly Club, and served as a school cheerleader. Roosevelt was relatively undistinguished as a student or athlete, but he became editor-in-chief of "The Harvard Crimson" daily newspaper, a position that required great ambition, energy, and the ability to manage others.
Roosevelt's father died in 1900, causing great distress for him. The following year, Roosevelt's fifth cousin Theodore Roosevelt became President of the United States. Theodore's vigorous leadership style and reforming zeal made him Franklin's role model and hero. Roosevelt graduated from Harvard in 1903 with an A.B. in history. He entered Columbia Law School in 1904, but dropped out in 1907 after passing the New York bar exam. In 1908, he took a job with the prestigious law firm of Carter Ledyard & Milburn, working in the firm's admiralty law division.
In mid-1902, Franklin began courting his future wife Eleanor Roosevelt, with whom he had been acquainted as a child. Eleanor and Franklin were fifth cousins, once removed, and Eleanor was a niece of Theodore Roosevelt. They began corresponding with each other in 1902, and in October 1903, Franklin proposed marriage to Eleanor.
On March 17, 1905, Roosevelt married Eleanor, despite the fierce resistance of his mother. While she did not dislike Eleanor, Sara Roosevelt was very possessive of her son, believing he was too young for marriage. She attempted to break the engagement several times. Eleanor's uncle, President Theodore Roosevelt, stood in at the wedding for Eleanor's deceased father, Elliott. The young couple moved into Springwood, his family's estate at Hyde Park. The home was owned by Sara Roosevelt until her death in 1941 and was very much her home as well. In addition, Franklin and Sara Roosevelt did the planning and furnishing of a townhouse Sara had built for the young couple in New York City; Sara had a twin house built alongside for herself. Eleanor never felt at home in the houses at Hyde Park or New York, but she loved the family's vacation home on Campobello Island, which Sara gave to the couple.
Biographer James MacGregor Burns said that young Roosevelt was self-assured and at ease in the upper-class. In contrast, Eleanor at the time was shy and disliked social life, and at first, stayed at home to raise their several children. As his father had, Franklin left the raising of the children to his wife, while Eleanor in turn largely relied on hired caregivers to raise the children. Referring to her early experience as a mother, she later stated that she knew "absolutely nothing about handling or feeding a baby." Although Eleanor had an aversion to sexual intercourse and considered it "an ordeal to be endured", she and Franklin had six children. Anna, James, and Elliott were born in 1906, 1907, and 1910, respectively. The couple's second son, Franklin, died in infancy in 1909. Another son, also named Franklin, was born in 1914, and the youngest child, John, was born in 1916.
Roosevelt had several extra-marital affairs, including one with Eleanor's social secretary Lucy Mercer, which began soon after she was hired in early 1914. In September 1918, Eleanor found letters revealing the affair in Roosevelt's luggage. Franklin contemplated divorcing Eleanor, but Sara objected strongly and Lucy would not agree to marry a divorced man with five children. Franklin and Eleanor remained married, and Roosevelt promised never to see Lucy again. Eleanor never truly forgave him, and their marriage from that point on was more of a political partnership. Eleanor soon thereafter established a separate home in Hyde Park at Val-Kill, and increasingly devoted herself to various social and political causes independently of her husband. The emotional break in their marriage was so severe that when Roosevelt asked Eleanor in 1942 — in light of his failing health — to come back home and live with him again, she refused. He was not always aware of when she visited the White House and for some time she could not easily reach him on the telephone without his secretary's help; Roosevelt, in turn, did not visit Eleanor's New York City apartment until late 1944.
Franklin broke his promise to Eleanor to refrain from having affairs. He and Lucy maintained a formal correspondence, and began seeing each other again in 1941, or perhaps earlier. Lucy was with Roosevelt on the day he died in 1945. Despite this, Roosevelt's affair was not widely known until the 1960s. Roosevelt's son Elliott claimed that his father had a 20-year affair with his private secretary, Marguerite "Missy" LeHand. Another son, James, stated that "there is a real possibility that a romantic relationship existed" between his father and Crown Princess Märtha of Norway, who resided in the White House during part of World War II. Aides began to refer to her at the time as "the president's girlfriend", and gossip linking the two romantically appeared in the newspapers.
Roosevelt held little passion for the practice of law and confided to friends that he planned to eventually enter politics. Despite his admiration for his cousin Theodore, Franklin inherited his father's affiliation with the Democratic Party. Prior to the 1910 elections, the local Democratic Party recruited Roosevelt to run for a seat in the New York State Assembly. Roosevelt was an attractive recruit for the party because Theodore was still one of the country's most prominent politicians, and a Democratic Roosevelt was good publicity; the candidate could also pay for his own campaign. Roosevelt's campaign for the state assembly ended after the Democratic incumbent, Lewis Stuyvesant Chanler, chose to seek re-election. Rather than putting his political hopes on hold, Roosevelt ran for a seat in the state senate. The senate district, located in Dutchess County, Columbia County, and Putnam County, was strongly Republican. Roosevelt feared that open opposition from Theodore could effectively end his campaign, but Theodore privately encouraged his cousin's candidacy despite their differences in partisan affiliation. Acting as his own campaign manager, Roosevelt traveled throughout the senate district via automobile at a time when many could not afford cars. Due to his aggressive and effective campaign, the Roosevelt name's influence in the Hudson Valley, and the Democratic landslide that year, Roosevelt won the election, surprising almost everyone.
Though legislative sessions rarely lasted more than ten weeks, Roosevelt treated his new position as a full-time career. Taking his seat on January 1, 1911, Roosevelt immediately became the leader of a group of "Insurgents" who opposed the bossism of the Tammany Hall machine that dominated the state Democratic Party. In the 1911 U.S. Senate election, which was determined in a joint session of the New York state legislature, Roosevelt and nineteen other Democrats caused a prolonged deadlock by opposing a series of Tammany-backed candidates. Finally, Tammany threw its backing behind James A. O'Gorman, a highly regarded judge who Roosevelt found acceptable, and O'Gorman won the election in late March. Roosevelt soon became a popular figure among New York Democrats, though he had not yet become an eloquent speaker. News articles and cartoons began depicting "the second coming of a Roosevelt" that sent "cold shivers down the spine of Tammany".
Roosevelt, again in opposition to Tammany Hall, supported New Jersey Governor Woodrow Wilson's successful bid for the 1912 Democratic nomination, earning an informal designation as an original Wilson man. The election became a three-way contest, as Theodore Roosevelt left the Republican Party to launch a third party campaign against Wilson and sitting Republican President William Howard Taft. Franklin's decision to back Wilson over Theodore Roosevelt in the general election alienated some members of his family, although Theodore himself was not offended. Wilson's victory over the divided Republican Party made him the first Democrat to win a presidential election since 1892. Overcoming a bout with typhoid fever, and with extensive assistance from journalist Louis McHenry Howe, Roosevelt was re-elected in the 1912 elections. After the election, he served for a short time as chairman of the Agriculture Committee, and his success with farm and labor bills was a precursor to his New Deal policies twenty years later. By this time he had become more consistently progressive, in support of labor and social welfare programs for women and children; cousin Theodore was of some influence on these issues.
Roosevelt's support of Wilson led to his appointment in March 1913 as Assistant Secretary of the Navy, the second-ranking official in the Navy Department after Secretary Josephus Daniels. Roosevelt had a lifelong affection for the Navy — he had already collected almost 10,000 naval books and claimed to have read all but one — and was more ardent than Daniels in supporting a large and efficient naval force. With Wilson's support, Daniels and Roosevelt instituted a merit-based promotion system and made other reforms to extend civilian control over the autonomous departments of the Navy. Roosevelt oversaw the Navy's civilian employees and earned the respect of union leaders for his fairness in resolving disputes. Not a single strike occurred during his seven-plus years in the office, during which Roosevelt gained experience in labor issues, government management during wartime, naval issues, and logistics, all valuable areas for future office.
In 1914, Roosevelt made an ill-conceived decision to run for the seat of retiring Republican Senator Elihu Root of New York. Though Roosevelt won the backing of Treasury Secretary William Gibbs McAdoo and Governor Martin H. Glynn, he faced a formidable opponent in the Tammany-backed James W. Gerard. He also lacked Wilson's backing, as Wilson needed Tammany's forces to help marshal his legislation and secure his 1916 re-election. Roosevelt was soundly defeated in the Democratic primary by Gerard, who in turn lost the general election to Republican James Wolcott Wadsworth Jr. Roosevelt learned a valuable lesson, that federal patronage alone, without White House support, could not defeat a strong local organization. After the election, Roosevelt and the boss of the Tammany Hall machine, Charles Francis Murphy, sought an accommodation with one another and became political allies.
Following his defeat in the Senate primary, Roosevelt refocused on the Navy Department. World War I broke out in July 1914, with the Central Powers of Germany, Austria-Hungary, and the Ottoman Empire seeking to defeat the Allied Powers of Britain, France, and Russia. Though he remained publicly supportive of Wilson, Roosevelt sympathized with the Preparedness Movement, whose leaders strongly favored the Allied Powers and called for a military build-up. The Wilson administration initiated an expansion of the Navy after the sinking of the RMS Lusitania by a German submarine, and Roosevelt helped establish the United States Navy Reserve and the Council of National Defense. In April 1917, after Germany declared it would engage in unrestricted submarine warfare and attacked several U.S. ships, Wilson asked Congress for a declaration of war. Congress approved the declaration of war on Germany on April 6.
Roosevelt requested that he be allowed to serve as a naval officer, but Wilson insisted that he continue to serve as Assistant Secretary of the Navy. For the next year, Roosevelt remained in Washington to coordinate the mobilization, supply, and deployment of naval vessels and personnel. In the first six months after the U.S. entered the war, the Navy expanded fourfold. In the summer of 1918, Roosevelt traveled to Europe to inspect naval installations and meet with French and British officials. In September, he returned to the United States on board the USS "Leviathan", a large troop carrier. On the 11-day voyage, the pandemic influenza virus struck and killed many on board. Roosevelt became very ill with influenza and a complicating pneumonia, but he recovered by the time the ship landed in New York. After Germany signed an armistice in November 1918, surrendering and ending the fighting, Daniels and Roosevelt supervised the demobilization of the Navy. Against the advice of older officers such as Admiral William Bensonwho claimed he could not "conceive of any use the fleet will ever have for aviation"Roosevelt personally ordered the preservation of the Navy's . With the Wilson administration coming to an end, Roosevelt began planning for his next run for office. Roosevelt and his associates approached Herbert Hoover about running for the 1920 Democratic presidential nomination, with Roosevelt as his running mate.
Roosevelt's plan to convince Hoover to run for the Democratic nomination fell through after Hoover publicly declared himself to be a Republican, but Roosevelt nonetheless decided to seek the 1920 vice presidential nomination. After Governor James M. Cox of Ohio won the party's presidential nomination at the 1920 Democratic National Convention, he chose Roosevelt as his running mate, and the party formally nominated Roosevelt by acclamation. Although his nomination surprised most people, Roosevelt balanced the ticket as a moderate, a Wilsonian, and a prohibitionist with a famous name. Roosevelt had just turned 38, four years younger than Theodore had been when he received the same nomination from his party. Roosevelt resigned as Assistant Secretary of the Navy after the Democratic convention and campaigned across the nation for the Cox–Roosevelt ticket.
During the campaign, Cox and Roosevelt defended the Wilson administration and the League of Nations, both of which were unpopular in 1920. Roosevelt personally supported U.S. membership in the League of Nations, but, unlike Wilson, he favored compromising with Senator Henry Cabot Lodge and other "Reservationists." The Cox–Roosevelt ticket was defeated by Republicans Warren G. Harding and Calvin Coolidge in the presidential election by a wide margin, and the Republican ticket carried every state outside of the South. Roosevelt accepted the loss without issue and later reflected that the relationships and good will that he built in the 1920 campaign proved to be a major asset in his 1932 campaign. The 1920 election also saw the first public participation of Eleanor Roosevelt who, with the support of Louis Howe, established herself as a valuable political ally.
After the election, Roosevelt returned to New York City, where he practiced law and served as a vice president of the Fidelity and Deposit Company. He also sought to build support for a political comeback in the 1922 elections, but his career was derailed by illness. While the Roosevelts were vacationing at Campobello Island in August 1921, he fell ill. His main symptoms were fever; symmetric, ascending paralysis; facial paralysis; bowel and bladder dysfunction; numbness and hyperesthesia; and a descending pattern of recovery. Roosevelt was left permanently paralyzed from the waist down. He was diagnosed with poliomyelitis at the time, but his symptoms are more consistent with Guillain–Barré syndrome – an autoimmune neuropathy which Roosevelt's doctors failed to consider as a diagnostic possibility.
Though his mother favored his retirement from public life, Roosevelt, his wife, and Roosevelt's close friend and adviser, Louis Howe, were all determined that Roosevelt continue his political career. Roosevelt convinced many people that he was improving, which he believed to be essential prior to running for public office again. He laboriously taught himself to walk short distances while wearing iron braces on his hips and legs by swiveling his torso, supporting himself with a cane. Roosevelt was careful never to be seen using his wheelchair in public, and great care was taken to prevent any portrayal in the press that would highlight his disability. However, his disability was well known before and during his presidency and became a major part of his image. He usually appeared in public standing upright, supported on one side by an aide or one of his sons.
Beginning in 1925, Roosevelt spent most of his time in the Southern United States, at first on his houseboat, the "Larooco". Intrigued by the potential benefits of hydrotherapy, he established a rehabilitation center at Warm Springs, Georgia, in 1926. To create the rehabilitation center, Roosevelt assembled a staff of physical therapists and used most of his inheritance to purchase the Merriweather Inn. In 1938, Roosevelt founded the National Foundation for Infantile Paralysis, leading to the development of polio vaccines.
Roosevelt maintained contacts with the Democratic Party during the 1920s, and he remained active in New York politics while also establishing contacts in the South, particularly in Georgia. Roosevelt issued an open letter endorsing Al Smith's successful campaign in New York's 1922 gubernatorial election, which both aided Smith and showed Roosevelt's continuing relevance as a political figure. Roosevelt and Smith came from different backgrounds and never fully trusted one another, but Roosevelt supported Smith's progressive policies, while Smith was happy to have the backing of the prominent and well-respected Roosevelt. Roosevelt gave presidential nominating speeches for Smith at the 1924 and 1928 Democratic National Conventions; the speech at the 1924 convention marked a return to public life following his illness and convalescence. The Democrats were badly divided between an urban wing, led by Smith, and a conservative, rural wing, led by William Gibbs McAdoo, and the party suffered a landslide defeat in the 1924 presidential election. Like many others throughout the United States, Roosevelt did not abstain from alcohol during the Prohibition era, but publicly he sought to find a compromise on Prohibition acceptable to both wings of the party.
In 1925, Smith appointed Roosevelt to the Taconic State Park Commission, and his fellow commissioners chose him as chairman. In this role, he came into conflict with Robert Moses, a Smith protégé, who was the primary force behind the Long Island State Park Commission and the New York State Council of Parks. Roosevelt accused Moses of using the name recognition of prominent individuals including Roosevelt to win political support for state parks, but then diverting funds to the ones Moses favored on Long Island, while Moses worked to block the appointment of Howe to a salaried position as the Taconic commission's secretary. Roosevelt served on the commission until the end of 1928, and his contentious relationship with Moses continued as their careers progressed.
As the Democratic Party presidential nominee in the 1928 election, Smith, in turn, asked Roosevelt to run for governor in the state election. Roosevelt initially resisted the entreaties of Smith and others within the party, as he was reluctant to leave Warm Springs and feared a Republican landslide in 1928. He agreed to run when party leaders convinced him that only he could defeat the Republican gubernatorial nominee, New York Attorney General Albert Ottinger. Roosevelt won the party's gubernatorial nomination by acclamation, and he once again turned to Louis Howe to lead his campaign. Roosevelt was also joined on the campaign trail by Samuel Rosenman, Frances Perkins, and James Farley, all of whom would become important political associates. While Smith lost the presidency in a landslide, and was defeated in his home state, Roosevelt was elected governor by a one-percent margin. Roosevelt's election as governor of the most populous state immediately made him a contender in the next presidential election.
Upon taking office in January 1929, Roosevelt proposed the construction of a series of hydroelectric power plants and sought to address the ongoing farm crisis of the 1920s. Relations between Roosevelt and Smith suffered after Roosevelt chose not to retain key Smith appointees like Robert Moses. Roosevelt and Eleanor established a political understanding that would last for the duration of his political career; she would dutifully serve as the governor's wife but would also be free to pursue her own agenda and interests. He also began holding "fireside chats", in which he directly addressed his constituents via radio, often using these chats to pressure the New York State Legislature to advance his agenda. In October 1929, the Wall Street Crash occurred, and the country began sliding into the Great Depression. While President Hoover and many state governors believed that the economic crisis would subside, Roosevelt saw the seriousness of the situation and established a state employment commission. He also became the first governor to publicly endorse the idea of unemployment insurance.
When Roosevelt began his run for a second term in May 1930, he reiterated his doctrine from the campaign two years before: "that progressive government by its very terms must be a living and growing thing, that the battle for it is never-ending and that if we let up for one single moment or one single year, not merely do we stand still but we fall back in the march of civilization." He ran on a platform that called for aid to farmers, full employment, unemployment insurance, and old-age pensions. His Republican opponent could not overcome the public's criticism of the Republican Party during the economic downturn, and Roosevelt was elected to a second term by a 14% margin. With the Hoover administration resisting proposals to directly address the economic crisis, Governor Roosevelt proposed an economic relief package and the establishment of the Temporary Emergency Relief Administration to distribute those funds. Led first by Jesse I. Straus and then by Harry Hopkins, the agency assisted well over one-third of New York's population between 1932 and 1938. Roosevelt also began an investigation into corruption in New York City among the judiciary, the police force, and organized crime, prompting the creation of the Seabury Commission. Many public officials were removed from office as a result.
He opened the 1932 Winter Olympics in Lake Placid, becoming the first American to open the Olympic Games as a government official.
As the 1932 presidential election approached, Roosevelt increasingly turned his attention to national politics. He established a campaign team led by Howe and Farley and a "brain trust" of policy advisers. With the economy ailing, many Democrats hoped that the 1932 elections would result in the election of the first Democratic president since Woodrow Wilson. Roosevelt's re-election as governor had established him as the front-runner for the 1932 Democratic presidential nomination. Roosevelt rallied the progressive supporters of the Wilson administration while also appealing to many conservatives, establishing himself as the leading candidate in the South and West. The chief opposition to Roosevelt's candidacy came from Northeastern conservatives such as Al Smith, the 1928 Democratic presidential nominee. Smith hoped to deny Roosevelt the two-thirds support necessary to win the party's presidential nomination at the 1932 Democratic National Convention in Chicago, and then emerge as the nominee after multiple rounds of balloting. Roosevelt entered the convention with a delegate lead due to his success in the 1932 Democratic primaries, but most delegates entered the convention unbound to any particular candidate. On the first presidential ballot of the convention, Roosevelt received the votes of more than half but less than two-thirds of the delegates, with Smith finishing in a distant second place. Speaker of the House John Nance Garner, who controlled the votes of Texas and California, threw his support behind Roosevelt after the third ballot, and Roosevelt clinched the nomination on the fourth ballot. With little input from Roosevelt, Garner won the vice-presidential nomination. Roosevelt flew in from New York after learning that he had won the nomination, becoming the first major-party presidential nominee to accept the nomination in person.
In his acceptance speech, Roosevelt declared, "I pledge you, I pledge myself to a new deal for the American people... This is more than a political campaign. It is a call to arms." Roosevelt promised securities regulation, tariff reduction, farm relief, government-funded public works, and other government actions to address the Great Depression. Reflecting changing public opinion, the Democratic platform included a call for the repeal of Prohibition; Roosevelt himself had not taken a public stand on the issue prior to the convention but promised to uphold the party platform. After the convention, Roosevelt won endorsements from several progressive Republicans, including George W. Norris, Hiram Johnson, and Robert La Follette Jr. He also reconciled with the party's conservative wing, and even Al Smith was persuaded to support the Democratic ticket. Hoover's handling of the Bonus Army further damaged the incumbent's popularity, as newspapers across the country criticized the use of force to disperse assembled veterans.
Roosevelt won 57% of the popular vote and carried all but six states. Historians and political scientists consider the 1932–36 elections to be a political realignment. Roosevelt's victory was enabled by the creation of the New Deal coalition, small farmers, the Southern whites, Catholics, big city political machines, labor unions, northern African Americans (southern ones were still disfranchised), Jews, intellectuals, and political liberals. The creation of the New Deal coalition transformed American politics and started what political scientists call the "New Deal Party System" or the Fifth Party System. Between the Civil War and 1929, Democrats had rarely controlled both houses of Congress and had won just four of seventeen presidential elections; from 1932 to 1979, Democrats won eight of twelve presidential elections and generally controlled both houses of Congress.
Roosevelt was elected in November 1932 but, like his predecessors, did not take office until the following March. After the election, President Hoover sought to convince Roosevelt to renounce much of his campaign platform and to endorse the Hoover administration's policies. Roosevelt refused Hoover's request to develop a joint program to stop the downward economic spiral, claiming that it would tie his hands and that Hoover had all the power to act if necessary. The economy spiraled downward until the banking system began a complete nationwide shutdown as Hoover's term ended. Roosevelt used the transition period to select the personnel for his incoming administration, and he chose Howe as his chief of staff, Farley as Postmaster General, and Frances Perkins as Secretary of Labor. William H. Woodin, a Republican industrialist close to Roosevelt, was the choice for Secretary of the Treasury, while Roosevelt chose Senator Cordell Hull of Tennessee as Secretary of State. Harold L. Ickes and Henry A. Wallace, two progressive Republicans, were selected for the roles of Secretary of the Interior and Secretary of Agriculture, respectively. In February 1933, Roosevelt escaped an assassination attempt by Giuseppe Zangara, who expressed a "hate for all rulers." Attempting to shoot Roosevelt, Zangara instead mortally wounded Chicago Mayor Anton Cermak, who was sitting alongside Roosevelt.
Roosevelt appointed powerful men to top positions but made all the major decisions, regardless of delays, inefficiency or resentment. Analyzing the president's administrative style, historian James MacGregor Burns concludes:
When Roosevelt was inaugurated on March 4, 1933, the U.S. was at the nadir of the worst depression in its history. A quarter of the workforce was unemployed. Farmers were in deep trouble as prices had fallen by 60%. Industrial production had fallen by more than half since 1929. Two million people were homeless. By the evening of March 4, 32 of the 48 states – as well as the District of Columbia – had closed their banks.
Historians categorized Roosevelt's program as "relief, recovery, and reform." Relief was urgently needed by tens of millions of unemployed. Recovery meant boosting the economy back to normal. Reform meant long-term fixes of what was wrong, especially with the financial and banking systems. Through Roosevelt's series of radio talks, known as fireside chats, he presented his proposals directly to the American public. Energized by his personal victory over his paralytic illness, Roosevelt relied on his persistent optimism and activism to renew the national spirit.
On his second day in office, Roosevelt declared a four-day national "bank holiday" and called for a special session of Congress to start March 9, on which date Congress passed the Emergency Banking Act. The act, which was based on a plan developed by the Hoover administration and Wall Street bankers, gave the president the power to determine the opening and closing of banks and authorized the Federal Reserve Banks to issue banknotes. The ensuing "First 100 Days" of the 73rd United States Congress saw an unprecedented amount of legislation and set a benchmark against which future presidents would be compared. When the banks reopened on Monday, March 15, stock prices rose by 15 percent and bank deposits exceeded withdrawals, thus ending the bank panic. On March 22, Roosevelt signed the Cullen–Harrison Act, which effectively ended federal Prohibition.
Roosevelt presided over the establishment of several agencies and measures designed to provide relief for the unemployed and others in need. The Federal Emergency Relief Administration (FERA), under the leadership of Harry Hopkins, was designed to distribute relief to state governments. The Public Works Administration (PWA), under the leadership of Secretary of the Interior Harold Ickes, was created to oversee the construction of large-scale public works such as dams, bridges, and schools. The most popular of all New Deal agencies – and Roosevelt's favorite – was the Civilian Conservation Corps (CCC), which hired 250,000 unemployed young men to work on local rural projects. Roosevelt also expanded a Hoover agency, the Reconstruction Finance Corporation, making it a major source of financing for railroads and industry. Congress gave the Federal Trade Commission broad new regulatory powers and provided mortgage relief to millions of farmers and homeowners. Roosevelt also made agricultural relief a high priority and set up the Agricultural Adjustment Administration (AAA). The AAA tried to force higher prices for commodities by paying farmers to leave land uncultivated and to cut herds.
Reform of the economy was the goal of the National Industrial Recovery Act (NIRA) of 1933. It sought to end cutthroat competition by forcing industries to establish rules of operation for all firms within specific industries, such as minimum prices, agreements not to compete, and production restrictions. Industry leaders negotiated the rules which were approved by NIRA officials. Industry needed to raise wages as a condition for approval. Provisions encouraged unions and suspended antitrust laws. NIRA was found to be unconstitutional by unanimous decision of the Supreme Court in May 1935; Roosevelt strongly protested the decision. Roosevelt reformed the financial regulatory structure of the nation with the Glass–Steagall Act, creating the Federal Deposit Insurance Corporation (FDIC) to underwrite savings deposits. The act also sought to curb speculation by limiting affiliations between commercial banks and securities firms. In 1934, the Securities and Exchange Commission was created to regulate the trading of securities, while the Federal Communications Commission was established to regulate telecommunications.
Recovery was pursued through federal spending. The NIRA included $3.3 billion (equivalent to $ billion in ) of spending through the Public Works Administration. Roosevelt worked with Senator Norris to create the largest government-owned industrial enterprise in American history — the Tennessee Valley Authority (TVA) — which built dams and power stations, controlled floods, and modernized agriculture and home conditions in the poverty-stricken Tennessee Valley. Executive Order 6102 declared that all privately held gold of American citizens was to be sold to the U.S. Treasury and the price raised from $20 to $35 per ounce. The goal was to counter the deflation which was paralyzing the economy.
Roosevelt tried to keep his campaign promise by cutting the federal budget — including a reduction in military spending from $752 million in 1932 to $531 million in 1934 and a 40% cut in spending on veterans benefits — by removing 500,000 veterans and widows from the pension rolls and reducing benefits for the remainder, as well as cutting the salaries of federal employees and reducing spending on research and education. But the veterans were well organized and strongly protested, and most benefits were restored or increased by 1934. Veterans groups such as the American Legion and the Veterans of Foreign Wars won their campaign to transform their benefits from payments due in 1945 to immediate cash when Congress overrode the President's veto and passed the Bonus Act in January 1936. It pumped sums equal to 2% of the GDP into the consumer economy and had a major stimulus effect.
Roosevelt expected that his party would lose several races in the 1934 Congressional elections, as the president's party had done in most previous midterm elections, but the Democrats picked up seats in both houses of Congress. Empowered by the public's apparent vote of confidence in his administration, the first item on Roosevelt's agenda in the 74th Congress was the creation of a social insurance program. The Social Security Act established Social Security and promised economic security for the elderly, the poor and the sick. Roosevelt insisted that it should be funded by payroll taxes rather than from the general fund, saying, "We put those payroll contributions there so as to give the contributors a legal, moral, and political right to collect their pensions and unemployment benefits. With those taxes in there, no damn politician can ever scrap my social security program." Compared with the social security systems in western European countries, the Social Security Act of 1935 was rather conservative. But for the first time, the federal government took responsibility for the economic security of the aged, the temporarily unemployed, dependent children, and the handicapped. Against Roosevelt's original intention for universal coverage, the act only applied to roughly sixty percent of the labor force, as farmers, domestic workers, and other groups were excluded.
Roosevelt consolidated the various relief organizations, though some, like the PWA, continued to exist. After winning Congressional authorization for further funding of relief efforts, Roosevelt established the Works Progress Administration (WPA). Under the leadership of Harry Hopkins, the WPA employed over three million people in its first year of existence. The WPA undertook numerous construction projects and provided funding to the National Youth Administration and arts organizations.
Senator Robert Wagner wrote the National Labor Relations Act, which guaranteed workers the right to collective bargaining through unions of their own choice. The act also established the National Labor Relations Board (NLRB) to facilitate wage agreements and to suppress the repeated labor disturbances. The Wagner Act did not compel employers to reach an agreement with their employees, but it opened possibilities for American labor. The result was a tremendous growth of membership in the labor unions, especially in the mass-production sector. When the Flint sit-down strike threatened the production of General Motors, Roosevelt broke with the precedent set by many former presidents and refused to intervene; the strike ultimately led to the unionization of both General Motors and its rivals in the American automobile industry.
While the First New Deal of 1933 had broad support from most sectors, the Second New Deal challenged the business community. Conservative Democrats, led by Al Smith, fought back with the American Liberty League, savagely attacking Roosevelt and equating him with Karl Marx and Vladimir Lenin. But Smith overplayed his hand, and his boisterous rhetoric let Roosevelt isolate his opponents and identify them with the wealthy vested interests that opposed the New Deal, strengthening Roosevelt for the 1936 landslide. By contrast, labor unions, energized by the Wagner Act, signed up millions of new members and became a major backer of Roosevelt's reelections in 1936, 1940 and 1944.
Biographer James M. Burns suggests that Roosevelt's policy decisions were guided more by pragmatism than ideology and that he "was like the general of a guerrilla army whose columns, fighting blindly in the mountains through dense ravines and thickets, suddenly converge, half by plan and half by coincidence, and debouch into the plain below." Roosevelt argued that such apparently haphazard methodology was necessary. "The country needs and, unless I mistake its temper, the country demands bold, persistent experimentation," he wrote. "It is common sense to take a method and try it; if it fails, admit it frankly and try another. But above all, try something."
Though eight million workers remained unemployed in 1936, economic conditions had improved since 1932 and Roosevelt was widely popular. An attempt by Louisiana Senator Huey Long and other individuals to organize a left-wing alternative to the Democratic Party collapsed after Long's assassination in 1935. Roosevelt won re-nomination with little opposition at the 1936 Democratic National Convention, while his allies overcame Southern resistance to permanently abolish the long-established rule that had required Democratic presidential candidates to win the votes of two-thirds of the delegates rather than a simple majority. The Republicans nominated Kansas Governor Alf Landon, a well-respected but bland candidate whose chances were damaged by the public re-emergence of the still-unpopular Herbert Hoover. While Roosevelt campaigned on his New Deal programs and continued to attack Hoover, Landon sought to win voters who approved of the goals of the New Deal but disagreed with its implementation.
In the election against Landon and a third-party candidate, Roosevelt won 60.8% of the vote and carried every state except Maine and Vermont. The Democratic ticket won the highest proportion of the popular vote. Democrats also expanded their majorities in Congress, winning control of over three-quarters of the seats in each house. The election also saw the consolidation of the New Deal coalition; while the Democrats lost some of their traditional allies in big business, they were replaced by groups such as organized labor and African Americans, the latter of whom voted Democratic for the first time since the Civil War. Roosevelt lost high income voters, especially businessmen and professionals, but made major gains among the poor and minorities. He won 86 percent of the Jewish vote, 81 percent of Catholics, 80 percent of union members, 76 percent of Southerners, 76 percent of blacks in northern cities, and 75 percent of people on relief. Roosevelt carried 102 of the country's 106 cities with a population of 100,000 or more.
The Supreme Court became Roosevelt's primary domestic focus during his second term after the court overturned many of his programs, including NIRA. The more conservative members of the court upheld the principles of the Lochner era, which saw numerous economic regulations struck down on the basis of freedom of contract. Roosevelt proposed the Judicial Procedures Reform Bill of 1937, which would have allowed him to appoint an additional Justice for each incumbent Justice over the age of 70; in 1937, there were six Supreme Court Justices over the age of 70. The size of the Court had been set at nine since the passage of the Judiciary Act of 1869, and Congress had altered the number of Justices six other times throughout U.S. history. Roosevelt's "court packing" plan ran into intense political opposition from his own party, led by Vice President Garner, since it upset the separation of powers. A bipartisan coalition of liberals and conservatives of both parties opposed the bill, and Chief Justice Charles Evans Hughes broke with precedent by publicly advocating defeat of the bill. Any chance of passing the bill ended with the death of Senate Majority Leader Joseph Taylor Robinson in July 1937.
Starting with the 1937 case of "West Coast Hotel Co. v. Parrish", the court began to take a more favorable view of economic regulations. That same year, Roosevelt appointed a Supreme Court Justice for the first time, and by 1941, seven of the nine Justices had been appointed by Roosevelt. After "Parish", the Court shifted its focus from judicial review of economic regulations to the protection of civil liberties. Four of Roosevelt's Supreme Court appointees, Felix Frankfurter, Robert H. Jackson,
Hugo Black, and William O. Douglas, would be particularly influential in re-shaping the jurisprudence of the Court.
With Roosevelt's influence on the wane following the failure of the Judicial Procedures Reform Bill of 1937, conservative Democrats joined with Republicans to block the implementation of further New Deal programs. Roosevelt did manage to pass some legislation, including the Housing Act of 1937, a second Agricultural Adjustment Act, and the Fair Labor Standards Act (FLSA) of 1938, which was the last major piece of New Deal legislation. The FLSA outlawed child labor, established a federal minimum wage, and required overtime pay for certain employees who work in excess of forty-hours per week. He also won passage of the Reorganization Act of 1939 and subsequently created the Executive Office of the President, making it "the nerve center of the federal administrative system." When the economy began to deteriorate again in late 1937, Roosevelt asked Congress for $5 billion (equivalent to $ billion in ) in relief and public works funding. This managed to eventually create as many as 3.3 million WPA jobs by 1938. Projects accomplished under the WPA ranged from new federal courthouses and post offices to facilities and infrastructure for national parks, bridges and other infrastructure across the country, and architectural surveys and archaeological excavations — investments to construct facilities and preserve important resources. Beyond this, however, Roosevelt recommended to a special congressional session only a permanent national farm act, administrative reorganization, and regional planning measures, all of which were leftovers from a regular session. According to Burns, this attempt illustrated Roosevelt's inability to decide on a basic economic program.
Determined to overcome the opposition of conservative Democrats in Congress, Roosevelt became involved in the 1938 Democratic primaries, actively campaigning for challengers who were more supportive of New Deal reform. Roosevelt failed badly, managing to defeat only one target, a conservative Democrat from New York City. In the November 1938 elections, Democrats lost six Senate seats and 71 House seats, with losses concentrated among pro-New Deal Democrats. When Congress reconvened in 1939, Republicans under Senator Robert Taft formed a Conservative coalition with Southern Democrats, virtually ending Roosevelt's ability to enact his domestic proposals. Despite their opposition to Roosevelt's domestic policies, many of these conservative Congressmen would provide crucial support for Roosevelt's foreign policy before and during World War II.
Roosevelt had a lifelong interest in the environment and conservation starting with his youthful interest in forestry on his family estate. Although Roosevelt was never an outdoorsman or sportsman on Theodore Roosevelt's scale, his growth of the national systems were comparable. Roosevelt was active in expanding, funding, and promoting the National Park and National Forest systems. Under Roosevelt, their popularity soared, from three million visitors a year at the start of the decade to 15.5 million in 1939. The Civilian Conservation Corps enrolled 3.4 million young men and built of trails, planted two billion trees, and upgraded of dirt roads. Every state had its own state parks, and Roosevelt made sure that WPA and CCC projects were set up to upgrade them as well as the national systems.
Government spending increased from 8.0% of gross national product (GNP) under Hoover in 1932 to 10.2% of the GNP in 1936. The national debt as a percentage of the GNP had more than doubled under Hoover from 16% to 40% of the GNP in early 1933. It held steady at close to 40% as late as fall 1941, then grew rapidly during the war. The GNP was 34% higher in 1936 than in 1932 and 58% higher in 1940 on the eve of war. That is, the economy grew 58% from 1932 to 1940 in eight years of peacetime, and then grew 56% from 1940 to 1945 in five years of wartime. Unemployment fell dramatically during Roosevelt's first term. It increased in 1938 ("a depression within a depression") but continually declined after 1938. Total employment during Roosevelt's term expanded by 18.31 million jobs, with an average annual increase in jobs during his administration of 5.3%.
The main foreign policy initiative of Roosevelt's first term was the Good Neighbor Policy, which was a re-evaluation of U.S. policy toward Latin America. The United States had frequently intervened in Latin America following the promulgation of the Monroe Doctrine in 1823, and the United States had occupied several Latin American nations in the Banana Wars that had occurred following the Spanish–American War of 1898. After Roosevelt took office, he withdrew U.S. forces from Haiti and reached new treaties with Cuba and Panama, ending their status as U.S. protectorates. In December 1933, Roosevelt signed the Montevideo Convention on the Rights and Duties of States, renouncing the right to intervene unilaterally in the affairs of Latin American countries. Roosevelt also normalized relations with the Soviet Union, which the United States had refused to recognize since the 1920s. He hoped to renegotiate the Russian debt from World War I and open trade relations, but no progress was made on either issue, and "both nations were soon disillusioned by the accord."
The rejection of the Treaty of Versailles in 1919–1920 marked the dominance of isolationism in American foreign policy. Despite Roosevelt's Wilsonian background, he and Secretary of State Cordell Hull acted with great care not to provoke isolationist sentiment. The isolationist movement was bolstered in the early to mid-1930s by Senator Gerald Nye and others who succeeded in their effort to stop the "merchants of death" in the U.S. from selling arms abroad. This effort took the form of the Neutrality Acts; the president asked for, but was refused, a provision to give him the discretion to allow the sale of arms to victims of aggression. Focused on domestic policy, Roosevelt largely acquiesced to Congress's non-interventionist policies in the early-to-mid 1930s. In the interim, Fascist Italy under Benito Mussolini proceeded to overcome Ethiopia, and the Italians joined Nazi Germany under Adolf Hitler in supporting General Francisco Franco and the Nationalist cause in the Spanish Civil War. As that conflict drew to a close in early 1939, Roosevelt expressed regret in not aiding the Spanish Republicans. When Japan invaded China in 1937, isolationism limited Roosevelt's ability to aid China, despite atrocities like the Nanking Massacre and the USS Panay incident.
Germany annexed Austria in 1938, and soon turned its attention to its eastern neighbors. Roosevelt made it clear that, in the event of German aggression against Czechoslovakia, the U.S. would remain neutral. After completion of the Munich Agreement and the execution of Kristallnacht, American public opinion turned against Germany, and Roosevelt began preparing for a possible war with Germany. Relying on an interventionist political coalition of Southern Democrats and business-oriented Republicans, Roosevelt oversaw the expansion U.S. airpower and war production capacity.
When World War II began in September 1939 with Germany's invasion of Poland and Britain and France's subsequent declaration of war upon Germany, Roosevelt sought ways to assist Britain and France militarily. Isolationist leaders like Charles Lindbergh and Senator William Borah successfully mobilized opposition to Roosevelt's proposed repeal of the Neutrality Act, but Roosevelt won Congressional approval of the sale of arms on a cash-and-carry basis. He also began a regular secret correspondence with Britain's First Lord of the Admiralty, Winston Churchill, in September 1939 — the first of 1,700 letters and telegrams between them. Roosevelt forged a close personal relationship with Churchill, who became Prime Minister of the United Kingdom in May 1940.
The Fall of France in June 1940 shocked the American public, and isolationist sentiment declined. In July 1940, Roosevelt appointed two interventionist Republican leaders, Henry L. Stimson and Frank Knox, as Secretaries of War and the Navy, respectively. Both parties gave support to his plans for a rapid build-up of the American military, but the isolationists warned that Roosevelt would get the nation into an unnecessary war with Germany. In July 1940, a group of Congressmen introduced a bill that would authorize the nation's first peacetime draft, and with the support of the Roosevelt administration the Selective Training and Service Act of 1940 passed in September. The size of the army would increase from 189,000 men at the end of 1939 to 1.4 million men in mid-1941. In September 1940, Roosevelt openly defied the Neutrality Acts by reaching the Destroyers for Bases Agreement, which, in exchange for military base rights in the British Caribbean Islands, gave 50 WWI American destroyers to Britain.
In the months prior to the July 1940 Democratic National Convention, there was much speculation as to whether Roosevelt would run for an unprecedented third term. The two-term tradition, although not yet enshrined in the Constitution, had been established by George Washington when he refused to run for a third term in the 1796 presidential election. Roosevelt refused to give a definitive statement as to his willingness to be a candidate again, and he even indicated to some ambitious Democrats, such as James Farley, that he would not run for a third term and that they could seek the Democratic nomination. However, as Germany swept through Western Europe and menaced Britain in mid-1940, Roosevelt decided that only he had the necessary experience and skills to see the nation safely through the Nazi threat. He was aided by the party's political bosses, who feared that no Democrat except Roosevelt could defeat Wendell Willkie, the popular Republican nominee.
At the July 1940 Democratic Convention in Chicago, Roosevelt easily swept aside challenges from Farley and Vice President Garner, who had turned against Roosevelt in his second term because of his liberal economic and social policies. To replace Garner on the ticket, Roosevelt turned to Secretary of Agriculture Henry Wallace of Iowa, a former Republican who strongly supported the New Deal and was popular in farm states. The choice was strenuously opposed by many of the party's conservatives, who felt Wallace was too radical and "eccentric" in his private life to be an effective running mate. But Roosevelt insisted that without Wallace on the ticket he would decline re-nomination, and Wallace won the vice-presidential nomination, defeating Speaker of the House William B. Bankhead and other candidates.
A late August poll taken by Gallup found the race to be essentially tied, but Roosevelt's popularity surged in September following the announcement of the Destroyers for Bases Agreement. Willkie supported much of the New Deal as well as rearmament and aid to Britain, but warned that Roosevelt would drag the country into another European war. Responding to Willkie's attacks, Roosevelt promised to keep the country out of the war. Roosevelt won the 1940 election with 55% of the popular vote, 38 of the 48 states, and almost 85% of the electoral vote.
The world war dominated FDR's attention, with far more time devoted to world affairs than ever before. Domestic politics and relations with Congress were largely shaped by his efforts to achieve total mobilization of the nation's economic, financial, and institutional resources for the war effort. Even relationships with Latin America and Canada were structured by wartime demands. Roosevelt maintained close personal control of all major diplomatic and military decisions, working closely with his generals and admirals, the war and Navy departments, the British, and even with the Soviet Union. His key advisors on diplomacy were Harry Hopkins (who was based in the White House), Sumner Welles (based in the State Department), and Henry Morgenthau Jr. at Treasury. In military affairs, FDR worked most closely with Secretary Henry L. Stimson at the War Department, Army Chief of Staff George Marshall, and Admiral William D. Leahy.
By late 1940, re-armament was in high gear, partly to expand and re-equip the Army and Navy and partly to become the "Arsenal of Democracy" for Britain and other countries. With his famous Four Freedoms speech in January 1941, Roosevelt laid out the case for an Allied battle for basic rights throughout the world. Assisted by Willkie, Roosevelt won Congressional approval of the Lend-Lease program, which directed massive military and economic aid to Britain, and China. In sharp contrast to the loans of World War I, there would be no repayment after the war. As Roosevelt took a firmer stance against Japan, Germany, and Italy, American isolationists such as Charles Lindbergh and the America First Committee vehemently attacked Roosevelt as an irresponsible warmonger. When Germany invaded the Soviet Union in June 1941, Roosevelt agreed to extend Lend-Lease to the Soviets. Thus, Roosevelt had committed the U.S. to the Allied side with a policy of "all aid short of war." By July 1941, Roosevelt authorized the creation of the Office of the Coordinator of Inter-American Affairs (OCIAA) to counter perceived propaganda efforts in Latin America by Germany and Italy.
In August 1941, Roosevelt and Churchill conducted a highly secret bilateral meeting in which they drafted the Atlantic Charter, conceptually outlining global wartime and postwar goals. This would be the first of several wartime conferences; Churchill and Roosevelt would meet ten more times in person. Though Churchill pressed for an American declaration of war against Germany, Roosevelt believed that Congress would reject any attempt to bring the United States into the war. In September, a German submarine fired on the U.S. destroyer "Greer," and Roosevelt declared that the U.S. Navy would assume an escort role for Allied convoys in the Atlantic as far east as Great Britain and would fire upon German ships or submarines (U-boats) of the Kriegsmarine if they entered the U.S. Navy zone. This "shoot on sight" policy effectively declared naval war on Germany and was favored by Americans by a margin of 2-to-1.
After the German invasion of Poland, the primary concern of both Roosevelt and his top military staff was on the war in Europe, but Japan also presented foreign policy challenges. Relations with Japan had continually deteriorated since its invasion of Manchuria in 1931, and they had further worsened with Roosevelt's support of China. With the war in Europe occupying the attention of the major colonial powers, Japanese leaders eyed vulnerable colonies such as the Dutch East Indies, French Indochina, and British Malaya. After Roosevelt announced a $100 million loan (equivalent to $ billion in ) to China in reaction to Japan's occupation of northern French Indochina, Japan signed the Tripartite Pact with Germany and Italy. The pact bound each country to defend the others against attack, and Germany, Japan, and Italy became known as the Axis powers. Overcoming those who favored invading the Soviet Union, the Japanese Army high command successfully advocated for the conquest of Southeast Asia to ensure continued access to raw materials. In July 1941, after Japan occupied the remainder of French Indochina, Roosevelt cut off the sale of oil to Japan, depriving Japan of more than 95 percent of its oil supply. He also placed the Philippine military under American command and reinstated General Douglas MacArthur into active duty to command U.S. forces in the Philippines.
The Japanese were incensed by the embargo and Japanese leaders became determined to attack the United States unless it lifted the embargo. The Roosevelt administration was unwilling to reverse policy, and Secretary of State Hull blocked a potential summit between Roosevelt and Prime Minister Fumimaro Konoe. After diplomatic efforts to end the embargo failed, the Privy Council of Japan authorized a strike against the United States. The Japanese believed that the destruction of the United States Asiatic Fleet (stationed in the Philippines) and the United States Pacific Fleet (stationed at Pearl Harbor in Hawaii) was vital to the conquest of Southeast Asia. On the morning of December 7, 1941, the Japanese struck the U.S. naval base at Pearl Harbor with a surprise attack, knocking out the main American battleship fleet and killing 2,403 American servicemen and civilians. At the same time, separate Japanese task forces attacked Thailand, British Hong Kong, the Philippines, and other targets. Roosevelt called for war in his famous "Infamy Speech" to Congress, in which he said: "Yesterday, December 7, 1941 — a date which will live in infamy — the United States of America was suddenly and deliberately attacked by naval and air forces of the Empire of Japan." In a nearly unanimous vote, Congress declared war on Japan. After the Japanese attack at Pearl Harbor, antiwar sentiment in the United States largely evaporated overnight. On December 11, 1941, Hitler and Mussolini declared war on the United States, which responded in kind.
A majority of scholars have rejected the conspiracy theories that Roosevelt, or any other high government officials, knew in advance about the Japanese attack on Pearl Harbor. The Japanese had kept their secrets closely guarded. Senior American officials were aware that war was imminent, but they did not expect an attack on Pearl Harbor. Roosevelt had expected that the Japanese would attack either the Dutch East Indies or Thailand.
In late December 1941 Churchill and Roosevelt met at the Arcadia Conference, which established a joint strategy between the U.S. and Britain.
Both agreed on a Europe first strategy that prioritized the defeat of Germany before Japan. The U.S. and Britain established the Combined Chiefs of Staff to coordinate military policy and the Combined Munitions Assignments Board to coordinate the allocation of supplies. An agreement was also reached to establish a centralized command in the Pacific theater called ABDA, named for the American, British, Dutch, and Australian forces in the theater. On January 1, 1942, the United States, Britain, China, the Soviet Union, and twenty-two other countries (the Allied Powers) issued the Declaration by United Nations, in which each nation pledged to defeat the Axis powers.
In 1942, Roosevelt formed a new body, the Joint Chiefs of Staff, which made the final decisions on American military strategy. Admiral Ernest J. King as Chief of Naval Operations commanded the Navy and Marines, while General George C. Marshall led the Army and was in nominal control of the Air Force, which in practice was commanded by General Hap Arnold. The Joint Chiefs were chaired by Admiral William D. Leahy, the most senior officer in the military. Roosevelt avoided micromanaging the war and let his top military officers make most decisions. Roosevelt's civilian appointees handled the draft and procurement of men and equipment, but no civilians – not even the secretaries of War or Navy – had a voice in strategy. Roosevelt avoided the State Department and conducted high-level diplomacy through his aides, especially Harry Hopkins, whose influence was bolstered by his control of the Lend Lease funds.
In August 1939, Leo Szilard and Albert Einstein sent the Einstein–Szilárd letter to Roosevelt, warning of the possibility of a German project to develop nuclear weapons. Szilard realized that the recently discovered process of nuclear fission could be used to create a nuclear chain reaction that could be used as a weapon of mass destruction. Roosevelt feared the consequences of allowing Germany to have sole possession of the technology and authorized preliminary research into nuclear weapons. After the attack on Pearl Harbor, the Roosevelt administration secured the funds needed to continue research and selected General Leslie Groves to oversee the Manhattan Project, which was charged with developing the first nuclear weapons. Roosevelt and Churchill agreed to jointly pursue the project, and Roosevelt helped ensure that American scientists cooperated with their British counterparts.
Roosevelt coined the term "Four Policemen" to refer to the "Big Four" Allied powers of World War II, the United States, the United Kingdom, the Soviet Union, and China. The "Big Three" of Roosevelt, Churchill, and Soviet leader Joseph Stalin, together with Chinese Generalissimo Chiang Kai-shek, cooperated informally on a plan in which American and British troops concentrated in the West; Soviet troops fought on the Eastern front; and Chinese, British and American troops fought in Asia and the Pacific. The United States also continued to send aid via the Lend-Lease program to the Soviet Union and other countries. The Allies formulated strategy in a series of high-profile conferences as well as by contact through diplomatic and military channels. Beginning in May 1942, the Soviets urged an Anglo-American invasion of German-occupied France in order to divert troops from the Eastern front. Concerned that their forces were not yet ready for an invasion of France, Churchill and Roosevelt decided to delay such an invasion until at least 1943 and instead focus on a landing in North Africa, known as Operation Torch.
In November 1943, Roosevelt, Churchill, and Stalin met to discuss strategy and post-war plans at the Tehran Conference, where Roosevelt met Stalin for the first time. At the conference, Britain and the United States committed to opening a second front against Germany in 1944, while Stalin committed to entering the war against Japan at an unspecified date. Subsequent conferences at Bretton Woods and Dumbarton Oaks established the framework for the post-war international monetary system and the United Nations, an intergovernmental organization similar to Wilson's failed League of Nations.
Roosevelt, Churchill, and Stalin met for a second time at the February 1945 Yalta Conference in Crimea. With the end of the war in Europe approaching, Roosevelt's primary focus was on convincing Stalin to enter the war against Japan; the Joint Chiefs had estimated that an American invasion of Japan would cause as many as one million American casualties. In return for the Soviet Union's entrance into the war against Japan, the Soviet Union was promised control of Asian territories such as Sakhalin Island. The three leaders agreed to hold a conference in 1945 to establish the United Nations, and they also agreed on the structure of the United Nations Security Council, which would be charged with ensuring international peace and security. Roosevelt did not push for the immediate evacuation of Soviet soldiers from Poland, but he won the issuance of the Declaration on Liberated Europe, which promised free elections in countries that had been occupied by Germany. Germany itself would not be dismembered, but would be jointly occupied by the United States, France, Britain, and the Soviet Union. Against Soviet pressure, Roosevelt and Churchill refused to consent to imposing huge reparations and deindustrialization on Germany after the war. Roosevelt's role in the Yalta Conference has been controversial; critics charge that he naively trusted the Soviet Union to allow free elections in Eastern Europe, while supporters argue that there was little more that Roosevelt could have done for the Eastern European countries given the Soviet occupation and the need for cooperation with the Soviet Union during and after the war.
The Allies invaded French North Africa in November 1942, securing the surrender of Vichy French forces within days of landing. At the January 1943 Casablanca Conference, the Allies agreed to defeat Axis forces in North Africa and then launch an invasion of Sicily, with an attack on France to take place in 1944. At the conference, Roosevelt also announced that he would only accept the unconditional surrender of Germany, Japan, and Italy. In February 1943, the Soviet Union won a major victory at the Battle of Stalingrad, and in May 1943, the Allies secured the surrender of over 250,000 German and Italian soldiers in North Africa, ending the North African Campaign. The Allies launched an invasion of Sicily in July 1943, capturing the island by the end of the following month. In September 1943, the Allies secured an armistice from Italian Prime Minister Pietro Badoglio, but Germany quickly restored Mussolini to power. The Allied invasion of mainland Italy commenced in September 1943, but the Italian Campaign continued until 1945 as German and Italian troops resisted the Allied advance.
To command the invasion of France, Roosevelt chose General Dwight D. Eisenhower, who had successfully commanded a multinational coalition in North Africa and Sicily. Eisenhower chose to launch Operation Overlord on June 6, 1944. Supported by 12,000 aircraft and the largest naval force ever assembled, the Allies successfully established a beachhead in Normandy and then advanced further into France. Though reluctant to back an unelected government, Roosevelt recognized Charles de Gaulle's Provisional Government of the French Republic as the de facto government of France in July 1944. After most of France had been liberated from German occupation, Roosevelt granted formal recognition to de Gaulle's government in October 1944. Over the following months, the Allies liberated more territory from Nazi occupation and began the invasion of Germany. By April 1945, Nazi resistance was crumbling in the face of advances by both the Western Allies and the Soviet Union.
In the opening weeks of the war, Japan conquered the Philippines and the British and Dutch colonies in Southeast Asia. The Japanese advance reached its maximum extent by June 1942, when the U.S. Navy scored a decisive victory at the Battle of Midway. American and Australian forces then began a slow and costly strategy called island hopping or leapfrogging through the Pacific Islands, with the objective of gaining bases from which strategic airpower could be brought to bear on Japan and from which Japan could ultimately be invaded. In contrast to Hitler, Roosevelt took no direct part in the tactical naval operations, though he approved strategic decisions. Roosevelt gave way in part to insistent demands from the public and Congress that more effort be devoted against Japan, but he always insisted on Germany first. The strength of the Japanese navy was decimated in the Battle of Leyte Gulf, and by April 1945 the Allies had re-captured much of their lost territory in the Pacific.
The home front was subject to dynamic social changes throughout the war, though domestic issues were no longer Roosevelt's most urgent policy concern. The military buildup spurred economic growth. Unemployment fell in half from 7.7 million in spring 1940 to 3.4 million in fall 1941 and fell in half again to 1.5 million in fall 1942, out of a labor force of 54 million. There was a growing labor shortage, accelerating the second wave of the Great Migration of African Americans, farmers and rural populations to manufacturing centers. African Americans from the South went to California and other West Coast states for new jobs in the defense industry. To pay for increased government spending, in 1941 Roosevelt proposed that Congress enact an income tax rate of 99.5% on all income over $100,000; when the proposal failed, he issued an executive order imposing an income tax of 100% on income over $25,000, which Congress rescinded. The Revenue Act of 1942 instituted top tax rates as high as 94% (after accounting for the excess profits tax), greatly increased the tax base, and instituted the first federal withholding tax. In 1944, Roosevelt requested that Congress enact legislation which would tax all "unreasonable" profits, both corporate and individual, and thereby support his declared need for over $10 billion in revenue for the war and other government measures. Congress overrode Roosevelt's veto to pass a smaller revenue bill raising $2 billion.
In 1942, with the United States now in the conflict, war production increased dramatically, but fell short of the goals established by the president, due in part to manpower shortages. The effort was also hindered by numerous strikes, especially among union workers in the coal mining and railroad industries, which lasted well into 1944. Nonetheless, between 1941 and 1945, the United States produced 2.4 million trucks, 300,000 military aircraft, 88,400 tanks, and 40 billion rounds of ammunition. The production capacity of the United States dwarfed that of other countries; for example, in 1944, the United States produced more military aircraft than the combined production of Germany, Japan, Britain, and the Soviet Union. The White House became the ultimate site for labor mediation, conciliation or arbitration. One particular battle royale occurred between Vice President Wallace, who headed the Board of Economic Warfare, and Jesse H. Jones, in charge of the Reconstruction Finance Corporation; both agencies assumed responsibility for acquisition of rubber supplies and came to loggerheads over funding. Roosevelt resolved the dispute by dissolving both agencies. In 1943, Roosevelt established the Office of War Mobilization to oversee the home front; the agency was led by James F. Byrnes, who came to be known as the "assistant president" due to his influence.
Roosevelt's 1944 State of the Union Address advocated that Americans should think of basic economic rights as a Second Bill of Rights. He stated that all Americans should have the right to "adequate medical care", "a good education", "a decent home", and a "useful and remunerative job". In the most ambitious domestic proposal of his third term, Roosevelt proposed the G.I. Bill, which would create a massive benefits program for returning soldiers. Benefits included post-secondary education, medical care, unemployment insurance, job counseling, and low-cost loans for homes and businesses. The G.I. Bill passed unanimously in both houses of Congress and was signed into law in June 1944. Of the fifteen million Americans who served in World War II, more than half benefitted from the educational opportunities provided for in the G.I. Bill.
Roosevelt, a chain-smoker throughout his entire adult life, had been in declining physical health since at least 1940. In March 1944, shortly after his 62nd birthday, he underwent testing at Bethesda Hospital and was found to have high blood pressure, atherosclerosis, coronary artery disease causing angina pectoris, and congestive heart failure.
Hospital physicians and two outside specialists ordered Roosevelt to rest. His personal physician, Admiral Ross McIntire, created a daily schedule that banned business guests for lunch and incorporated two hours of rest each day. During the 1944 re-election campaign, McIntire denied several times that Roosevelt's health was poor; on October 12, for example, he announced that "The President's health is perfectly OK. There are absolutely no organic difficulties at all." Roosevelt realized that his declining health could eventually make it impossible for him to continue as president, and in 1945 he told a confidant that he might resign from the presidency following the end of the war.
While some Democrats had opposed Roosevelt's nomination in 1940, the president faced little difficulty in securing his re-nomination at the 1944 Democratic National Convention. Roosevelt made it clear before the convention that he was seeking another term, and on the lone presidential ballot of the convention, Roosevelt won the vast majority of delegates, although a minority of Southern Democrats voted for Harry F. Byrd. Party leaders prevailed upon Roosevelt to drop Vice President Wallace from the ticket, believing him to be an electoral liability and a poor potential successor in case of Roosevelt's death. Roosevelt preferred Byrnes as Wallace's replacement but was convinced to support Senator Harry S. Truman of Missouri, who had earned renown for his investigation of war production inefficiency and was acceptable to the various factions of the party. On the second vice presidential ballot of the convention, Truman defeated Wallace to win the nomination.
The Republicans nominated Thomas E. Dewey, the governor of New York, who had a reputation as a liberal in his party. The opposition accused Roosevelt and his administration of domestic corruption, bureaucratic inefficiency, tolerance of Communism, and military blunders. Labor unions, which had grown rapidly in the war, fully supported Roosevelt. Roosevelt and Truman won the 1944 election by a comfortable margin, defeating Dewey and his running mate John W. Bricker with 53.4% of the popular vote and 432 out of the 531 electoral votes. The president campaigned in favor of a strong United Nations, so his victory symbolized support for the nation's future participation in the international community.
When Roosevelt returned to the United States from the Yalta Conference, many were shocked to see how old, thin and frail he looked. He spoke while seated in the well of the House, an unprecedented concession to his physical incapacity. During March 1945, he sent strongly worded messages to Stalin accusing him of breaking his Yalta commitments over Poland, Germany, prisoners of war and other issues. When Stalin accused the western Allies of plotting behind his back a separate peace with Hitler, Roosevelt replied: "I cannot avoid a feeling of bitter resentment towards your informers, whoever they are, for such vile misrepresentations of my actions or those of my trusted subordinates."
On March 29, 1945, Roosevelt went to the Little White House at Warm Springs, Georgia, to rest before his anticipated appearance at the founding conference of the United Nations.
In the afternoon of April 12, 1945, in Warm Springs, Georgia, while sitting for a portrait, Roosevelt said "I have a terrific headache." He then slumped forward in his chair, unconscious, and was carried into his bedroom. The president's attending cardiologist, Dr. Howard Bruenn, diagnosed the medical emergency as a massive intracerebral hemorrhage. At 3:35 p.m. that day, Roosevelt died at the age of 63.
On the morning of April 13, Roosevelt's body was placed in a flag-draped coffin and loaded onto the presidential train for the trip back to Washington. Along the route, thousands flocked to the tracks to pay their respects. After a White House funeral on April 14, Roosevelt was transported by train from Washington, D.C., to his place of birth at Hyde Park. As was his wish, Roosevelt was buried on April 15 in the Rose Garden of his Springwood estate.
Roosevelt's declining physical health had been kept secret from the general public. His death was met with shock and grief across the U.S. and around the world. After Germany surrendered the following month, newly-sworn in President Truman dedicated Victory in Europe Day and its celebrations to Roosevelt's memory, and kept the flags across the U.S. at half-staff for the remainder of the 30-day mourning period, saying that his only wish was "that Franklin D. Roosevelt had lived to witness this day". World War II finally ended with the signed surrender of Japan in September following the atomic bombings of Hiroshima and Nagasaki and the very late Soviet entry into the war against the Japanese. Truman would preside over the demobilization of the war effort and the establishment of the United Nations and other postwar institutions envisioned during Roosevelt's presidency.
Roosevelt was viewed as a hero by many African Americans, Catholics, and Jews, and he was highly successful in attracting large majorities of these voters into his New Deal coalition. He won strong support from Chinese Americans and Filipino Americans, but not Japanese Americans, as he presided over their internment in concentration camps during the war. African Americans and Native Americans fared well in two New Deal relief programs, the Civilian Conservation Corps and the Indian Reorganization Act, respectively. Sitkoff reports that the WPA "provided an economic floor for the whole black community in the 1930s, rivaling both agriculture and domestic service as the chief source" of income.
Roosevelt did not join NAACP leaders in pushing for federal anti-lynching legislation, as he believed that such legislation was unlikely to pass and that his support for it would alienate Southern congressmen. He did, however, appoint a "Black Cabinet" of African American advisers to advise on race relations and African American issues, and he publicly denounced lynching as "murder." First Lady Eleanor Roosevelt vocally supported efforts designed to aid the African American community, including the Fair Labor Standards Act, which helped boost wages for nonwhite workers in the South. In 1941, Roosevelt established the Fair Employment Practices Committee (FEPC) to implement Executive Order 8802, which prohibited racial and religious discrimination in employment among defense contractors. The FEPC was the first national program directed against employment discrimination, and it played a major role in opening up new employment opportunities to non-white workers. During World War II, the proportion of African American men employed in manufacturing positions rose significantly. In response to Roosevelt's policies, African Americans increasingly defected from the Republican Party during the 1930s and 1940s, becoming an important Democratic voting bloc in several Northern states.
The attack on Pearl Harbor raised concerns in the public regarding the possibility of sabotage by Japanese Americans. This suspicion was fed by long-standing racism against Japanese immigrants, as well as the findings of the Roberts Commission, which concluded that the attack on Pearl Harbor had been assisted by Japanese spies. On February 19, 1942, President Roosevelt signed Executive Order 9066, which relocated hundreds of thousands of the Japanese-American citizens and immigrants. They were forced to liquidate their properties and businesses and interned in hastily built camps in interior, harsh locations. Distracted by other issues, Roosevelt had delegated the decision for internment to Secretary of War Stimson, who in turn relied on the judgment of Assistant Secretary of War John J. McCloy. The Supreme Court upheld the constitutionality of the executive order in the 1944 case of "Korematsu v. United States". Many German and Italian citizens were also arrested or placed into internment camps.
After Kristallnacht in 1938, Roosevelt helped expedite Jewish immigration from Germany and Austria, and allowed German citizens already in the United States to stay indefinitely. However, he was prevented from accepting further Jewish immigrants, practically refugees, by the restrictive Immigration Act of 1924, and antisemitism among voters. Hitler chose to implement the "Final Solution" — the extermination of the European Jewish population — by January 1942, and American officials learned of the scale of the Nazi extermination campaign in the following months. Against the objections of the State Department, Roosevelt convinced the other Allied leaders to jointly issue the Joint Declaration by Members of the United Nations, which condemned the ongoing Holocaust and warned to try its perpetrators as war criminals. In January 1944, Roosevelt established the War Refugee Board to aid Jews and other victims of Axis atrocities. Aside from these actions, Roosevelt believed that the best way to help the persecuted populations of Europe was to end the war as quickly as possible. Top military leaders and War Department leaders rejected any campaign to bomb the extermination camps or the rail lines leading to the camps, fearing it would be a diversion from the war effort. According to biographer Jean Edward Smith, there is no evidence that anyone ever proposed such a campaign to Roosevelt.
Roosevelt is widely considered to be one of the most important figures in the history of the United States, as well as one of the most influential figures of the 20th century. Historians and political scientists consistently rank Roosevelt, George Washington, and Abraham Lincoln as the three greatest presidents. Reflecting on Roosevelt's presidency, "which brought the United States through the Great Depression and World War II to a prosperous future", said FDR biographer Jean Edward Smith in 2007, "He lifted himself from a wheelchair to lift the nation from its knees."
The rapid expansion of government programs that occurred during Roosevelt's term redefined the role of the government in the United States, and Roosevelt's advocacy of government social programs was instrumental in redefining liberalism for coming generations. Roosevelt firmly established the United States' leadership role on the world stage, with his role in shaping and financing World War II. His isolationist critics faded away, and even the Republicans joined in his overall policies. He also created a new understanding of the presidency, permanently increasing the power of the president at the expense of Congress.
His Second Bill of Rights became, according to historian Joshua Zeitz, "the basis of the Democratic Party's aspirations for the better part of four decades." After his death, his widow, Eleanor, continued to be a forceful presence in U.S. and world politics, serving as delegate to the conference which established the United Nations and championing civil rights and liberalism generally. Many members of his administration played leading roles in the administrations of Truman, Kennedy and Johnson, each of whom embraced Roosevelt's political legacy.
During his presidency, and continuing to a lesser extent afterwards, there has been much criticism of Roosevelt, some of it intense. Critics have questioned not only his policies, positions, and the consolidation of power that occurred due to his responses to the crises of the Depression and World War II but also his breaking with tradition by running for a third term as president. Long after his death, new lines of attack criticized Roosevelt's policies regarding helping the Jews of Europe, incarcerating the Japanese on the West Coast, and opposing anti-lynching legislation.
Roosevelt's home in Hyde Park is now a National Historic Site and home to his Presidential library. Washington D.C., hosts two memorials to the former president. The largest, the Roosevelt Memorial, is located next to the Jefferson Memorial on the Tidal Basin. A more modest memorial, a block of marble in front of the National Archives building suggested by Roosevelt himself, was erected in 1965. Roosevelt's leadership in the March of Dimes is one reason he is commemorated on the American dime. Roosevelt has also appeared on several U.S. Postage stamps.

</doc>
<doc id="10980" url="https://en.wikipedia.org/wiki?curid=10980" title="Four Freedoms">
Four Freedoms

The Four Freedoms were goals articulated by United States President Franklin D. Roosevelt on Monday, January 6, 1941. In an address known as the (technically the 1941 State of the Union address), he proposed four fundamental freedoms that people "everywhere in the world" ought to enjoy:
Roosevelt delivered his speech 11 months before the surprise Japanese attack on U.S. forces in Pearl Harbor, Hawaii that caused the United States to declare war on Japan, December 8, 1941. The State of the Union speech before Congress was largely about the national security of the United States and the threat to other democracies from world war that was being waged across the continents in the eastern hemisphere. In the speech, he made a break with the tradition of United States non-interventionism that had long been held in the United States. He outlined the U.S. role in helping allies already engaged in warfare.
In that context, he summarized the values of democracy behind the bipartisan consensus on international involvement that existed at the time. A famous quote from the speech prefaces those values: "As men do not live by bread alone, they do not fight by armaments alone." In the second half of the speech, he lists the benefits of democracy, which include economic opportunity, employment, social security, and the promise of "adequate health care". The first two freedoms, of speech and religion, are protected by the First Amendment in the United States Constitution. His inclusion of the latter two freedoms went beyond the traditional Constitutional values protected by the U.S. Bill of Rights. Roosevelt endorsed a broader human right to economic security and anticipated what would become known decades later as the "human security" paradigm in social science and economic development. He also included the "freedom from fear" against national aggression and took it to the new United Nations he was setting up.
In the 1930s many Americans, arguing that the involvement in World War I had been a mistake, were adamantly against continued intervention in European affairs. With the Neutrality Acts established after 1935, U.S. law banned the sale of armaments to countries that were at war and placed restrictions on travel with belligerent vessels.
When World War II began in September 1939, the neutrality laws were still in effect and ensured that no substantial support could be given to Britain and France. With the revision of the Neutrality Act in 1939, Roosevelt adopted a "methods-short-of-war policy" whereby supplies and armaments could be given to European Allies, provided no declaration of war could be made and no troops committed. By December 1940, Europe was largely at the mercy of Adolf Hitler and Germany's Nazi regime. With Germany's defeat of France in June 1940, Britain and its overseas Empire stood alone against the military alliance of Germany, Italy, and Japan. Winston Churchill, as Prime Minister of Britain, called for Roosevelt and the United States to supply them with armaments in order to continue with the war effort.
The 1939 New York World's Fair had celebrated Four Freedoms - religion, speech, press, and assembly - and commissioned Leo Friedlander to create sculptures representing them. Mayor of New York City Fiorello La Guardia described the resulting statues as the "heart of the fair". Later Roosevelt would declare his own "Four Essential Freedoms" and call on Walter Russell to create a "Four Freedoms Monument" that was eventually dedicated at Madison Square Garden in New York City.
They also appeared on the reverse of the AM-lira, the Allied Military Currency note issue that was issued in Italy during WWII, by the Americans, that was in effect occupation currency, guaranteed by the American dollar.
The Four Freedoms Speech was given on January 6, 1941. Roosevelt's hope was to provide a rationale for why the United States should abandon the isolationist policies that emerged from World War I. In the address, Roosevelt critiqued Isolationism, saying: "No realistic American can expect from a dictator's peace international generosity, or return of true independence, or world disarmament, or freedom of expression, or freedom of religion–or even good business. Such a peace would bring no security for us or for our neighbors. "Those, who would give up essential liberty to purchase a little temporary safety, deserve neither liberty nor safety."
The speech coincided with the introduction of the Lend-Lease Act, which promoted Roosevelt's plan to become the "arsenal of democracy" and support the Allies (mainly the British) with much-needed supplies. Furthermore, the speech established what would become the ideological basis for America's involvement in World War II, all framed in terms of individual rights and liberties that are the hallmark of American politics.
The speech delivered by President Roosevelt incorporated the following text, known as the "Four Freedoms":
The declaration of the Four Freedoms as a justification for war would resonate through the remainder of the war, and for decades longer as a frame of remembrance. The Freedoms became the staple of America's war aims and the center of all attempts to rally public support for the war. With the creation of the Office of War Information (1942), as well as the famous paintings by Norman Rockwell, the Freedoms were advertised as values central to American life and examples of American exceptionalism.
The Four Freedoms Speech was popular, and the goals were influential in postwar politics. However, in 1941 the speech received heavy criticism from anti-war elements. Critics argued that the Four Freedoms were simply a charter for Roosevelt's New Deal, social reforms that had already created sharp divisions within Congress. Conservatives who opposed social programs and increased government intervention argued against Roosevelt's attempt to justify and depict the war as necessary for the defense of lofty goals.
While the Freedoms did become a forceful aspect of American thought on the war, they were never the exclusive justification for the war. Polls and surveys conducted by the United States Office of War Information (OWI) revealed that "self-defense", and vengeance for the attack on Pearl Harbor were still the most prevalent reasons for war.
In a 1942 radio address, President Roosevelt declared the Four Freedoms embodied "rights of men of every creed and every race, wherever they live."
On February 19, 1942, President Roosevelt authorized Japanese American internment and internment of Italian Americans with Executive Order 9066, which allowed local military commanders to designate "military areas" as "exclusion zones," from which "any or all persons may be excluded." This power was used to declare that all people of Japanese ancestry were excluded from the entire Pacific coast, including all of California and much of Oregon, Washington, and Arizona, except for those in internment camps. By 1946, the United States had incarcerated 120,000 individuals of Japanese descent, of whom about 80,000 had been born in the United States.
The concept of the Four Freedoms became part of the personal mission undertaken by First Lady Eleanor Roosevelt regarding her inspiration behind the United Nations Declaration of Human Rights, General Assembly Resolution 217A. Indeed, these Four Freedoms were explicitly incorporated into the preamble to the Universal Declaration of Human Rights which reads, ""Whereas" disregard and contempt for human rights have resulted in barbarous acts which have outraged the conscience of mankind, and the advent of a world in which human beings shall enjoy the freedom of speech and belief and freedom from fear and want has been proclaimed the highest aspiration of the common people..."
FDR called for "a world-wide reduction of armaments" as a goal for "the future days, which we seek to make secure" but one that was "attainable in our own time and generation." More immediately, though, he called for a massive build-up of U.S. arms production:
The Franklin D. Roosevelt Four Freedoms Park is a park designed by the architect Louis Kahn for the south point of Roosevelt Island. The Park celebrates the famous speech, and text from the speech is inscribed on a granite wall in the final design of the Park.
The Roosevelt Institute honors outstanding individuals who have demonstrated a lifelong commitment to these ideals. The Four Freedoms Award medals are awarded at ceremonies at Hyde Park, New York and Middelburg, Netherlands during alternate years. The awards were first presented in 1982 on the centenary of President Roosevelt's birth as well as the bicentenary of diplomatic relations between the United States and the Netherlands.
Among the laureates have been:
Roosevelt's speech inspired a set of four paintings by Norman Rockwell.
The members of the set, known collectively as "The Four Freedoms", were published in four consecutive issues of "The Saturday Evening Post". The four paintings subsequently were displayed around the US by the United States Department of the Treasury.
Each painting was published with a matching essay on that particular "Freedom":
Rockwell's "Four Freedoms" paintings were reproduced as postage stamps by the United States Post Office in 1943, in 1946, and in 1994, the centenary of Rockwell's birth.

</doc>
<doc id="10983" url="https://en.wikipedia.org/wiki?curid=10983" title="First-order logic">
First-order logic

First-order logic—also known as predicate logic, quantificational logic, and first-order predicate calculus—is a collection of formal systems used in mathematics, philosophy, linguistics, and computer science. First-order logic uses quantified variables over non-logical objects, and allows the use of sentences that contain variables, so that rather than propositions such as "Socrates is a man", one can have expressions in the form "there exists x such that x is Socrates and x is a man", where "there exists""" is a quantifier, while "x" is a variable. This distinguishes it from propositional logic, which does not use quantifiers or relations; in this sense, propositional logic is the foundation of first-order logic.
A theory about a topic is usually a first-order logic together with a specified domain of discourse (over which the quantified variables range), finitely many functions from that domain to itself, finitely many predicates defined on that domain, and a set of axioms believed to hold about them. Sometimes, "theory" is understood in a more formal sense, which is just a set of sentences in first-order logic.
The adjective "first-order" distinguishes first-order logic from higher-order logic, in which there are predicates having predicates or functions as arguments, or in which one or both of predicate quantifiers or function quantifiers are permitted. In first-order theories, predicates are often associated with sets. In interpreted higher-order theories, predicates may be interpreted as sets of sets.
There are many deductive systems for first-order logic which are both sound (i.e., all provable statements are true in all models) and complete (i.e. all statements which are true in all models are provable). Although the logical consequence relation is only semidecidable, much progress has been made in automated theorem proving in first-order logic. First-order logic also satisfies several metalogical theorems that make it amenable to analysis in proof theory, such as the Löwenheim–Skolem theorem and the compactness theorem.
First-order logic is the standard for the formalization of mathematics into axioms, and is studied in the foundations of mathematics.
Peano arithmetic and Zermelo–Fraenkel set theory are axiomatizations of number theory and set theory, respectively, into first-order logic.
No first-order theory, however, has the strength to uniquely describe a structure with an infinite domain, such as the natural numbers or the real line. Axiom systems that do fully describe these two structures (that is, categorical axiom systems) can be obtained in stronger logics such as second-order logic.
The foundations of first-order logic were developed independently by Gottlob Frege and Charles Sanders Peirce. For a history of first-order logic and how it came to dominate formal logic, see José Ferreirós (2001).
While propositional logic deals with simple declarative propositions, first-order logic additionally covers predicates and quantification.
A predicate takes an entity or entities in the domain of discourse as input while outputs are either True or False. Consider the two sentences "Socrates is a philosopher" and "Plato is a philosopher". In propositional logic, these sentences are viewed as being unrelated, and might be denoted, for example, by variables such as "p" and "q". The predicate "is a philosopher" occurs in both sentences, which have a common structure of ""a" is a philosopher". The variable "a" is instantiated as "Socrates" in the first sentence, and is instantiated as "Plato" in the second sentence. While first-order logic allows for the use of predicates, such as "is a philosopher" in this example, propositional logic does not.
Relationships between predicates can be stated using logical connectives. Consider, for example, the first-order formula "if "a" is a philosopher, then "a" is a scholar". This formula is a conditional statement with ""a" is a philosopher" as its hypothesis, and ""a" is a scholar" as its conclusion. The truth of this formula depends on which object is denoted by "a", and on the interpretations of the predicates "is a philosopher" and "is a scholar".
Quantifiers can be applied to variables in a formula. The variable "a" in the previous formula can be universally quantified, for instance, with the first-order sentence "For every "a", if "a" is a philosopher, then "a" is a scholar". The universal quantifier "for every" in this sentence expresses the idea that the claim "if "a" is a philosopher, then "a" is a scholar" holds for "all" choices of "a".
The "negation" of the sentence "For every "a", if "a" is a philosopher, then "a" is a scholar" is logically equivalent to the sentence "There exists "a" such that "a" is a philosopher and "a" is not a scholar". The existential quantifier "there exists" expresses the idea that the claim ""a" is a philosopher and "a" is not a scholar" holds for "some" choice of "a".
The predicates "is a philosopher" and "is a scholar" each take a single variable. In general, predicates can take several variables. In the first-order sentence "Socrates is the teacher of Plato", the predicate "is the teacher of" takes two variables.
An interpretation (or model) of a first-order formula specifies what each predicate means, and the entities that can instantiate the variables. These entities form the domain of discourse or universe, which is usually required to be a nonempty set. For example, in an interpretation with the domain of discourse consisting of all human beings and the predicate "is a philosopher" understood as "was the author of the "Republic"", the sentence "There exists "a" such that "a" is a philosopher" is seen as being true, as witnessed by Plato.
There are two key parts of first-order logic. The syntax determines which finite sequences of symbols are well-formed expressions in first-order logic, while the semantics determines the meanings behind these expressions.
Unlike natural languages, such as English, the language of first-order logic is completely formal, so that it can be mechanically determined whether a given expression is well formed. There are two key types of well-formed expressions: terms, which intuitively represent objects, and formulas, which intuitively express predicates that can be true or false. The terms and formulas of first-order logic are strings of symbols, where all the symbols together form the alphabet of the language. As with all formal languages, the nature of the symbols themselves is outside the scope of formal logic; they are often regarded simply as letters and punctuation symbols.
It is common to divide the symbols of the alphabet into logical symbols, which always have the same meaning, and non-logical symbols, whose meaning varies by interpretation. For example, the logical symbol formula_1 always represents "and"; it is never interpreted as "or", which is represented by the logical symbol formula_2. On the other hand, a non-logical predicate symbol such as Phil("x") could be interpreted to mean ""x" is a philosopher", ""x" is a man named Philip", or any other unary predicate depending on the interpretation at hand.
There are several logical symbols in the alphabet, which vary by author but usually include:
Not all of these symbols are required–only one of the quantifiers, negation and conjunction, variables, brackets and equality suffice. There are numerous minor variations that may define additional logical symbols:
The non-logical symbols represent predicates (relations), functions and constants on the domain of discourse. It used to be standard practice to use a fixed, infinite set of non-logical symbols for all purposes. A more recent practice is to use different non-logical symbols according to the application one has in mind. Therefore, it has become necessary to name the set of all non-logical symbols used in a particular application. This choice is made via a signature.
The traditional approach is to have only one, infinite, set of non-logical symbols (one signature) for all applications. Consequently, under the traditional approach there is only one language of first-order logic. This approach is still common, especially in philosophically oriented books.
In contemporary mathematical logic, the signature varies by application. Typical signatures in mathematics are {1, ×} or just {×} for groups, or {0, 1, +, ×, <} for ordered fields. There are no restrictions on the number of non-logical symbols. The signature can be empty, finite, or infinite, even uncountable. Uncountable signatures occur for example in modern proofs of the Löwenheim–Skolem theorem.
In this approach, every non-logical symbol is of one of the following types.
The traditional approach can be recovered in the modern approach, by simply specifying the "custom" signature to consist of the traditional sequences of non-logical symbols.
The formation rules define the terms and formulas of first-order logic. When terms and formulas are represented as strings of symbols, these rules can be used to write a formal grammar for terms and formulas. These rules are generally context-free (each production has a single symbol on the left side), except that the set of symbols may be allowed to be infinite and there may be many start symbols, for example the variables in the case of terms.
The set of terms is inductively defined by the following rules:
Only expressions which can be obtained by finitely many applications of rules 1 and 2 are terms. For example, no expression involving a predicate symbol is a term.
The set of formulas (also called well-formed formulas or WFFs) is inductively defined by the following rules:
Only expressions which can be obtained by finitely many applications of rules 1–5 are formulas. The formulas obtained from the first two rules are said to be atomic formulas.
For example,
is a formula, if "f" is a unary function symbol, "P" a unary predicate symbol, and Q a ternary predicate symbol. On the other hand, formula_11 is not a formula, although it is a string of symbols from the alphabet.
The role of the parentheses in the definition is to ensure that any formula can only be obtained in one way—by following the inductive definition (i.e., there is a unique parse tree for each formula). This property is known as unique readability of formulas. There are many conventions for where parentheses are used in formulas. For example, some authors use colons or full stops instead of parentheses, or change the places in which parentheses are inserted. Each author's particular definition must be accompanied by a proof of unique readability.
This definition of a formula does not support defining an if-then-else function ite(c, a, b), where "c" is a condition expressed as a formula, that would return "a" if c is true, and "b" if it is false. This is because both predicates and functions can only accept terms as parameters, but the first parameter is a formula. Some languages built on first-order logic, such as SMT-LIB 2.0, add this.
For convenience, conventions have been developed about the precedence of the logical operators, to avoid the need to write parentheses in some cases. These rules are similar to the order of operations in arithmetic. A common convention is:
Moreover, extra punctuation not required by the definition may be inserted—to make formulas easier to read. Thus the formula
might be written as
In some fields, it is common to use infix notation for binary relations and functions, instead of the prefix notation defined above. For example, in arithmetic, one typically writes "2 + 2 = 4" instead of "=(+(2,2),4)". It is common to regard formulas in infix notation as abbreviations for the corresponding formulas in prefix notation, cf. also term structure vs. representation.
The definitions above use infix notation for binary connectives such as formula_15. A less common convention is Polish notation, in which one writes formula_4, formula_20 and so on in front of their arguments rather than between them. This convention is advantageous in that it allows all punctuation symbols to be discarded. As such, Polish notation is compact and elegant, but rarely used in practice because it is hard for humans to read. In Polish notation, the formula
becomes 
In a formula, a variable may occur free or bound (or both). Intuitively, a variable occurrence is free in a formula if it is not quantified: in ∀"y" "P"("x", "y"), the sole occurrence of variable "x" is free while that of "y" is bound. The free and bound variable occurrences in a formula are defined inductively as follows.
For example, in ∀"x" ∀"y" ("P"("x") → "Q"("x","f"("x"),"z")), "x" and "y" occur only bound, "z" occurs only free, and "w" is neither because it does not occur in the formula.
Free and bound variables of a formula need not be disjoint sets: in the formula "P"("x") → ∀"x" "Q"("x"), the first occurrence of "x", as argument of "P", is free while the second one, as argument of "Q", is bound.
A formula in first-order logic with no free variable occurrences is called a first-order sentence. These are the formulas that will have well-defined truth values under an interpretation. For example, whether a formula such as Phil("x") is true must depend on what "x" represents. But the sentence ∃"x" Phil("x") will be either true or false in a given interpretation.
In mathematics, the language of ordered abelian groups has one constant symbol 0, one unary function symbol −, one binary function symbol +, and one binary relation symbol ≤. Then:
The axioms for ordered abelian groups can be expressed as a set of sentences in the language. For example, the axiom stating that the group is commutative is usually written formula_24
An interpretation of a first-order language assigns a denotation to each non-logical symbol in that language. It also determines a domain of discourse that specifies the range of the quantifiers. The result is that each term is assigned an object that it represents, each predicate is assigned a property of objects, and each sentence is assigned a truth value. In this way, an interpretation provides semantic meaning to the terms, the predicates, and formulas of the language. The study of the interpretations of formal languages is called formal semantics. What follows is a description of the standard or Tarskian semantics for first-order logic. (It is also possible to define game semantics for first-order logic, but aside from requiring the axiom of choice, game semantics agree with Tarskian semantics for first-order logic, so game semantics will not be elaborated herein.)
The domain of discourse "D" is a nonempty set of "objects" of some kind. Intuitively, a first-order formula is a statement about these objects; for example, formula_25 states the existence of an object "x" such that the predicate "P" is true where referred to it. The domain of discourse is the set of considered objects. For example, one can take formula_26 to be the set of integer numbers.
The interpretation of a function symbol is a function. For example, if the domain of discourse consists of integers, a function symbol "f" of arity 2 can be interpreted as the function that gives the sum of its arguments. In other words, the symbol "f" is associated with the function "I(f)" which, in this interpretation, is addition.
The interpretation of a constant symbol is a function from the one-element set "D" to "D", which can be simply identified with an object in "D". For example, an interpretation may assign the value formula_27 to the constant symbol formula_28.
The interpretation of an "n"-ary predicate symbol is a set of "n"-tuples of elements of the domain of discourse. This means that, given an interpretation, a predicate symbol, and "n" elements of the domain of discourse, one can tell whether the predicate is true of those elements according to the given interpretation. For example, an interpretation "I(P)" of a binary predicate symbol "P" may be the set of pairs of integers such that the first one is less than the second. According to this interpretation, the predicate "P" would be true if its first argument is less than the second.
The most common way of specifying an interpretation (especially in mathematics) is to specify a structure (also called a model; see below). The structure consists of a nonempty set "D" that forms the domain of discourse and an interpretation "I" of the non-logical terms of the signature. This interpretation is itself a function:
A formula evaluates to true or false given an interpretation, and a variable assignment μ that associates an element of the domain of discourse with each variable. The reason that a variable assignment is required is to give meanings to formulas with free variables, such as formula_34. The truth value of this formula changes depending on whether "x" and "y" denote the same individual.
First, the variable assignment μ can be extended to all terms of the language, with the result that each term maps to a single element of the domain of discourse. The following rules are used to make this assignment:
Next, each formula is assigned a truth value. The inductive definition used to make this assignment is called the T-schema.
If a formula does not contain free variables, and so is a sentence, then the initial variable assignment does not affect its truth value. In other words, a sentence is true according to "M" and formula_52 if and only if it is true according to "M" and every other variable assignment formula_53.
There is a second common approach to defining truth values that does not rely on variable assignment functions. Instead, given an interpretation "M", one first adds to the signature a collection of constant symbols, one for each element of the domain of discourse in "M"; say that for each "d" in the domain the constant symbol "c" is fixed. The interpretation is extended so that each new constant symbol is assigned to its corresponding element of the domain. One now defines truth for quantified formulas syntactically, as follows:
This alternate approach gives exactly the same truth values to all sentences as the approach via variable assignments.
If a sentence φ evaluates to True under a given interpretation "M", one says that "M" satisfies φ; this is denoted formula_69. A sentence is satisfiable if there is some interpretation under which it is true.
Satisfiability of formulas with free variables is more complicated, because an interpretation on its own does not determine the truth value of such a formula. The most common convention is that a formula with free variables is said to be satisfied by an interpretation if the formula remains true regardless which individuals from the domain of discourse are assigned to its free variables. This has the same effect as saying that a formula is satisfied if and only if its universal closure is satisfied.
A formula is logically valid (or simply valid) if it is true in every interpretation. These formulas play a role similar to tautologies in propositional logic.
A formula φ is a logical consequence of a formula ψ if every interpretation that makes ψ true also makes φ true. In this case one says that φ is logically implied by ψ.
An alternate approach to the semantics of first-order logic proceeds via abstract algebra. This approach generalizes the Lindenbaum–Tarski algebras of propositional logic. There are three ways of eliminating quantified variables from first-order logic that do not involve replacing quantifiers with other variable binding term operators:
These algebras are all lattices that properly extend the two-element Boolean algebra.
Tarski and Givant (1987) showed that the fragment of first-order logic that has no atomic sentence lying in the scope of more than three quantifiers has the same expressive power as relation algebra. This fragment is of great interest because it suffices for Peano arithmetic and most axiomatic set theory, including the canonical ZFC. They also prove that first-order logic with a primitive ordered pair is equivalent to a relation algebra with two ordered pair projection functions.
A first-order theory of a particular signature is a set of axioms, which are sentences consisting of symbols from that signature. The set of axioms is often finite or recursively enumerable, in which case the theory is called effective. Some authors require theories to also include all logical consequences of the axioms. The axioms are considered to hold within the theory and from them other sentences that hold within the theory can be derived.
A first-order structure that satisfies all sentences in a given theory is said to be a model of the theory. An elementary class is the set of all structures satisfying a particular theory. These classes are a main subject of study in model theory.
Many theories have an intended interpretation, a certain model that is kept in mind when studying the theory. For example, the intended interpretation of Peano arithmetic consists of the usual natural numbers with their usual operations. However, the Löwenheim–Skolem theorem shows that most first-order theories will also have other, nonstandard models.
A theory is consistent if it is not possible to prove a contradiction from the axioms of the theory. A theory is complete if, for every formula in its signature, either that formula or its negation is a logical consequence of the axioms of the theory. Gödel's incompleteness theorem shows that effective first-order theories that include a sufficient portion of the theory of the natural numbers can never be both consistent and complete.
For more information on this subject see List of first-order theories and Theory (mathematical logic)
The definition above requires that the domain of discourse of any interpretation must be nonempty. There are settings, such as inclusive logic, where empty domains are permitted. Moreover, if a class of algebraic structures includes an empty structure (for example, there is an empty poset), that class can only be an elementary class in first-order logic if empty domains are permitted or the empty structure is removed from the class.
There are several difficulties with empty domains, however:
Thus, when the empty domain is permitted, it must often be treated as a special case. Most authors, however, simply exclude the empty domain by definition.
A deductive system is used to demonstrate, on a purely syntactic basis, that one formula is a logical consequence of another formula. There are many such systems for first-order logic, including Hilbert-style deductive systems, natural deduction, the sequent calculus, the tableaux method, and resolution. These share the common property that a deduction is a finite syntactic object; the format of this object, and the way it is constructed, vary widely. These finite deductions themselves are often called derivations in proof theory. They are also often called proofs, but are completely formalized unlike natural-language mathematical proofs.
A deductive system is sound if any formula that can be derived in the system is logically valid. Conversely, a deductive system is complete if every logically valid formula is derivable. All of the systems discussed in this article are both sound and complete. They also share the property that it is possible to effectively verify that a purportedly valid deduction is actually a deduction; such deduction systems are called effective.
A key property of deductive systems is that they are purely syntactic, so that derivations can be verified without considering any interpretation. Thus a sound argument is correct in every possible interpretation of the language, regardless whether that interpretation is about mathematics, economics, or some other area.
In general, logical consequence in first-order logic is only semidecidable: if a sentence A logically implies a sentence B then this can be discovered (for example, by searching for a proof until one is found, using some effective, sound, complete proof system). However, if A does not logically imply B, this does not mean that A logically implies the negation of B. There is no effective procedure that, given formulas A and B, always correctly decides whether A logically implies B.
A rule of inference states that, given a particular formula (or set of formulas) with a certain property as a hypothesis, another specific formula (or set of formulas) can be derived as a conclusion. The rule is sound (or truth-preserving) if it preserves validity in the sense that whenever any interpretation satisfies the hypothesis, that interpretation also satisfies the conclusion.
For example, one common rule of inference is the rule of substitution. If "t" is a term and φ is a formula possibly containing the variable "x", then φ["t"/"x"] is the result of replacing all free instances of "x" by "t" in φ. The substitution rule states that for any φ and any term "t", one can conclude φ["t"/"x"] from φ provided that no free variable of "t" becomes bound during the substitution process. (If some free variable of "t" becomes bound, then to substitute "t" for "x" it is first necessary to change the bound variables of φ to differ from the free variables of "t".)
To see why the restriction on bound variables is necessary, consider the logically valid formula φ given by formula_73, in the signature of (0,1,+,×,=) of arithmetic. If "t" is the term "x + 1", the formula φ["t"/"y"] is formula_74, which will be false in many interpretations. The problem is that the free variable "x" of "t" became bound during the substitution. The intended replacement can be obtained by renaming the bound variable "x" of φ to something else, say "z", so that the formula after substitution is formula_75, which is again logically valid.
The substitution rule demonstrates several common aspects of rules of inference. It is entirely syntactical; one can tell whether it was correctly applied without appeal to any interpretation. It has (syntactically defined) limitations on when it can be applied, which must be respected to preserve the correctness of derivations. Moreover, as is often the case, these limitations are necessary because of interactions between free and bound variables that occur during syntactic manipulations of the formulas involved in the inference rule.
A deduction in a Hilbert-style deductive system is a list of formulas, each of which is a logical axiom, a hypothesis that has been assumed for the derivation at hand, or follows from previous formulas via a rule of inference. The logical axioms consist of several axiom schemas of logically valid formulas; these encompass a significant amount of propositional logic. The rules of inference enable the manipulation of quantifiers. Typical Hilbert-style systems have a small number of rules of inference, along with several infinite schemas of logical axioms. It is common to have only modus ponens and universal generalization as rules of inference.
Natural deduction systems resemble Hilbert-style systems in that a deduction is a finite list of formulas. However, natural deduction systems have no logical axioms; they compensate by adding additional rules of inference that can be used to manipulate the logical connectives in formulas in the proof.
The sequent calculus was developed to study the properties of natural deduction systems. Instead of working with one formula at a time, it uses sequents, which are expressions of the form
where A, ..., A, B, ..., B are formulas and the turnstile symbol formula_77 is used as punctuation to separate the two halves. Intuitively, a sequent expresses the idea that formula_78 implies formula_79.
Unlike the methods just described, the derivations in the tableaux method are not lists of formulas. Instead, a derivation is a tree of formulas. To show that a formula A is provable, the tableaux method attempts to demonstrate that the negation of A is unsatisfiable. The tree of the derivation has formula_80 at its root; the tree branches in a way that reflects the structure of the formula. For example, to show that formula_81 is unsatisfiable requires showing that C and D are each unsatisfiable; this corresponds to a branching point in the tree with parent formula_81 and children C and D.
The resolution rule is a single rule of inference that, together with unification, is sound and complete for first-order logic. As with the tableaux method, a formula is proved by showing that the negation of the formula is unsatisfiable. Resolution is commonly used in automated theorem proving.
The resolution method works only with formulas that are disjunctions of atomic formulas; arbitrary formulas must first be converted to this form through Skolemization. The resolution rule states that from the hypotheses formula_83 and formula_84, the conclusion formula_85 can be obtained.
Many identities can be proved, which establish equivalences between particular formulas. These identities allow for rearranging formulas by moving quantifiers across other connectives, and are useful for putting formulas in prenex normal form. Some provable identities include:
There are several different conventions for using equality (or identity) in first-order logic. The most common convention, known as first-order logic with equality, includes the equality symbol as a primitive logical symbol which is always interpreted as the real equality relation between members of the domain of discourse, such that the "two" given members are the same member. This approach also adds certain axioms about equality to the deductive system employed. These equality axioms are:
These are axiom schemas, each of which specifies an infinite set of axioms. The third schema is known as Leibniz's law, "the principle of substitutivity", "the indiscernibility of identicals", or "the replacement property". The second schema, involving the function symbol "f", is (equivalent to) a special case of the third schema, using the formula
Many other properties of equality are consequences of the axioms above, for example:
An alternate approach considers the equality relation to be a non-logical symbol. This convention is known as first-order logic without equality. If an equality relation is included in the signature, the axioms of equality must now be added to the theories under consideration, if desired, instead of being considered rules of logic. The main difference between this method and first-order logic with equality is that an interpretation may now interpret two distinct individuals as "equal" (although, by Leibniz's law, these will satisfy exactly the same formulas under any interpretation). That is, the equality relation may now be interpreted by an arbitrary equivalence relation on the domain of discourse that is congruent with respect to the functions and relations of the interpretation.
When this second convention is followed, the term normal model is used to refer to an interpretation where no distinct individuals "a" and "b" satisfy "a" = "b". In first-order logic with equality, only normal models are considered, and so there is no term for a model other than a normal model. When first-order logic without equality is studied, it is necessary to amend the statements of results such as the Löwenheim–Skolem theorem so that only normal models are considered.
First-order logic without equality is often employed in the context of second-order arithmetic and other higher-order theories of arithmetic, where the equality relation between sets of natural numbers is usually omitted.
If a theory has a binary formula "A"("x","y") which satisfies reflexivity and Leibniz's law, the theory is said to have equality, or to be a theory with equality. The theory may not have all instances of the above schemas as axioms, but rather as derivable theorems. For example, in theories with no function symbols and a finite number of relations, it is possible to define equality in terms of the relations, by defining the two terms "s" and "t" to be equal if any relation is unchanged by changing "s" to "t" in any argument.
Some theories allow other "ad hoc" definitions of equality:
One motivation for the use of first-order logic, rather than higher-order logic, is that first-order logic has many metalogical properties that stronger logics do not have. These results concern general properties of first-order logic itself, rather than properties of individual theories. They provide fundamental tools for the construction of models of first-order theories.
Gödel's completeness theorem, proved by Kurt Gödel in 1929, establishes that there are sound, complete, effective deductive systems for first-order logic, and thus the first-order logical consequence relation is captured by finite provability. Naively, the statement that a formula φ logically implies a formula ψ depends on every model of φ; these models will in general be of arbitrarily large cardinality, and so logical consequence cannot be effectively verified by checking every model. However, it is possible to enumerate all finite derivations and search for a derivation of ψ from φ. If ψ is logically implied by φ, such a derivation will eventually be found. Thus first-order logical consequence is semidecidable: it is possible to make an effective enumeration of all pairs of sentences (φ,ψ) such that ψ is a logical consequence of φ.
Unlike propositional logic, first-order logic is undecidable (although semidecidable), provided that the language has at least one predicate of arity at least 2 (other than equality). This means that there is no decision procedure that determines whether arbitrary formulas are logically valid. This result was established independently by Alonzo Church and Alan Turing in 1936 and 1937, respectively, giving a negative answer to the Entscheidungsproblem posed by David Hilbert and Wilhelm Ackermann in 1928. Their proofs demonstrate a connection between the unsolvability of the decision problem for first-order logic and the unsolvability of the halting problem.
There are systems weaker than full first-order logic for which the logical consequence relation is decidable. These include propositional logic and monadic predicate logic, which is first-order logic restricted to unary predicate symbols and no function symbols. Other logics with no function symbols which are decidable are the guarded fragment of first-order logic, as well as two-variable logic. The Bernays–Schönfinkel class of first-order formulas is also decidable. Decidable subsets of first-order logic are also studied in the framework of description logics.
The Löwenheim–Skolem theorem shows that if a first-order theory of cardinality λ has an infinite model, then it has models of every infinite cardinality greater than or equal to λ. One of the earliest results in model theory, it implies that it is not possible to characterize countability or uncountability in a first-order language with a countable signature. That is, there is no first-order formula φ("x") such that an arbitrary structure M satisfies φ if and only if the domain of discourse of M is countable (or, in the second case, uncountable).
The Löwenheim–Skolem theorem implies that infinite structures cannot be categorically axiomatized in first-order logic. For example, there is no first-order theory whose only model is the real line: any first-order theory with an infinite model also has a model of cardinality larger than the continuum. Since the real line is infinite, any theory satisfied by the real line is also satisfied by some nonstandard models. When the Löwenheim–Skolem theorem is applied to first-order set theories, the nonintuitive consequences are known as Skolem's paradox.
The compactness theorem states that a set of first-order sentences has a model if and only if every finite subset of it has a model. This implies that if a formula is a logical consequence of an infinite set of first-order axioms, then it is a logical consequence of some finite number of those axioms. This theorem was proved first by Kurt Gödel as a consequence of the completeness theorem, but many additional proofs have been obtained over time. It is a central tool in model theory, providing a fundamental method for constructing models.
The compactness theorem has a limiting effect on which collections of first-order structures are elementary classes. For example, the compactness theorem implies that any theory that has arbitrarily large finite models has an infinite model. Thus the class of all finite graphs is not an elementary class (the same holds for many other algebraic structures).
There are also more subtle limitations of first-order logic that are implied by the compactness theorem. For example, in computer science, many situations can be modeled as a directed graph of states (nodes) and connections (directed edges). Validating such a system may require showing that no "bad" state can be reached from any "good" state. Thus one seeks to determine if the good and bad states are in different connected components of the graph. However, the compactness theorem can be used to show that connected graphs are not an elementary class in first-order logic, and there is no formula φ("x","y") of first-order logic, in the logic of graphs, that expresses the idea that there is a path from "x" to "y". Connectedness can be expressed in second-order logic, however, but not with only existential set quantifiers, as formula_100 also enjoys compactness.
Per Lindström showed that the metalogical properties just discussed actually characterize first-order logic in the sense that no stronger logic can also have those properties (Ebbinghaus and Flum 1994, Chapter XIII). Lindström defined a class of abstract logical systems, and a rigorous definition of the relative strength of a member of this class. He established two theorems for systems of this type:
Although first-order logic is sufficient for formalizing much of mathematics, and is commonly used in computer science and other fields, it has certain limitations. These include limitations on its expressiveness and limitations of the fragments of natural languages that it can describe.
For instance, first-order logic is undecidable, meaning a sound, complete and terminating decision algorithm for provability is impossible. This has led to the study of interesting decidable fragments, such as C: first-order logic with two variables and the counting quantifiers formula_101 and formula_102.
The Löwenheim–Skolem theorem shows that if a first-order theory has any infinite model, then it has infinite models of every cardinality. In particular, no first-order theory with an infinite model can be categorical. Thus there is no first-order theory whose only model has the set of natural numbers as its domain, or whose only model has the set of real numbers as its domain. Many extensions of first-order logic, including infinitary logics and higher-order logics, are more expressive in the sense that they do permit categorical axiomatizations of the natural numbers or real numbers. This expressiveness comes at a metalogical cost, however: by Lindström's theorem, the compactness theorem and the downward Löwenheim–Skolem theorem cannot hold in any logic stronger than first-order.
First-order logic is able to formalize many simple quantifier constructions in natural language, such as "every person who lives in Perth lives in Australia". But there are many more complicated features of natural language that cannot be expressed in (single-sorted) first-order logic. "Any logical system which is appropriate as an instrument for the analysis of natural language needs a much richer structure than first-order predicate logic".
There are many variations of first-order logic. Some of these are inessential in the sense that they merely change notation without affecting the semantics. Others change the expressive power more significantly, by extending the semantics through additional quantifiers or other new logical symbols. For example, infinitary logics permit formulas of infinite size, and modal logics add symbols for possibility and necessity.
First-order logic can be studied in languages with fewer logical symbols than were described above.
Restrictions such as these are useful as a technique to reduce the number of inference rules or axiom schemas in deductive systems, which leads to shorter proofs of metalogical results. The cost of the restrictions is that it becomes more difficult to express natural-language statements in the formal system at hand, because the logical connectives used in the natural language statements must be replaced by their (longer) definitions in terms of the restricted collection of logical connectives. Similarly, derivations in the limited systems may be longer than derivations in systems that include additional connectives. There is thus a trade-off between the ease of working within the formal system and the ease of proving results about the formal system.
It is also possible to restrict the arities of function symbols and predicate symbols, in sufficiently expressive theories. One can in principle dispense entirely with functions of arity greater than 2 and predicates of arity greater than 1 in theories that include a pairing function. This is a function of arity 2 that takes pairs of elements of the domain and returns an ordered pair containing them. It is also sufficient to have two predicate symbols of arity 2 that define projection functions from an ordered pair to its components. In either case it is necessary that the natural axioms for a pairing function and its projections are satisfied.
Ordinary first-order interpretations have a single domain of discourse over which all quantifiers range. Many-sorted first-order logic allows variables to have different sorts, which have different domains. This is also called typed first-order logic, and the sorts called types (as in data type), but it is not the same as first-order type theory. Many-sorted first-order logic is often used in the study of second-order arithmetic.
When there are only finitely many sorts in a theory, many-sorted first-order logic can be reduced to single-sorted first-order logic.
One introduces into the single-sorted theory a unary predicate symbol for each sort in the many-sorted theory, and adds an axiom saying that these unary predicates partition the domain of discourse. For example, if there are two sorts, one adds predicate symbols formula_129 and formula_130 and the axiom
Then the elements satisfying formula_132 are thought of as elements of the first sort, and elements satisfying formula_133 as elements of the second sort. One can quantify over each sort by using the corresponding predicate symbol to limit the range of quantification. For example, to say there is an element of the first sort satisfying formula φ("x"), one writes
Additional quantifiers can be added to first-order logic.
Infinitary logic allows infinitely long sentences. For example, one may allow a conjunction or disjunction of infinitely many formulas, or quantification over infinitely many variables. Infinitely long sentences arise in areas of mathematics including topology and model theory.
Infinitary logic generalizes first-order logic to allow formulas of infinite length. The most common way in which formulas can become infinite is through infinite conjunctions and disjunctions. However, it is also possible to admit generalized signatures in which function and relation symbols are allowed to have infinite arities, or in which quantifiers can bind infinitely many variables. Because an infinite formula cannot be represented by a finite string, it is necessary to choose some other representation of formulas; the usual representation in this context is a tree. Thus formulas are, essentially, identified with their parse trees, rather than with the strings being parsed.
The most commonly studied infinitary logics are denoted "L", where α and β are each either cardinal numbers or the symbol ∞. In this notation, ordinary first-order logic is "L".
In the logic "L", arbitrary conjunctions or disjunctions are allowed when building formulas, and there is an unlimited supply of variables. More generally, the logic that permits conjunctions or disjunctions with less than κ constituents is known as "L". For example, "L" permits countable conjunctions and disjunctions.
The set of free variables in a formula of "L" can have any cardinality strictly less than κ, yet only finitely many of them can be in the scope of any quantifier when a formula appears as a subformula of another. In other infinitary logics, a subformula may be in the scope of infinitely many quantifiers. For example, in "L", a single universal or existential quantifier may bind arbitrarily many variables simultaneously. Similarly, the logic "L" permits simultaneous quantification over fewer than λ variables, as well as conjunctions and disjunctions of size less than κ.
Fixpoint logic extends first-order logic by adding the closure under the least fixed points of positive operators.
The characteristic feature of first-order logic is that individuals can be quantified, but not predicates. Thus
is a legal first-order formula, but
is not, in most formalizations of first-order logic. Second-order logic extends first-order logic by adding the latter type of quantification. Other higher-order logics allow quantification over even higher types than second-order logic permits. These higher types include relations between relations, functions from relations to relations between relations, and other higher-type objects. Thus the "first" in first-order logic describes the type of objects that can be quantified.
Unlike first-order logic, for which only one semantics is studied, there are several possible semantics for second-order logic. The most commonly employed semantics for second-order and higher-order logic is known as full semantics. The combination of additional quantifiers and the full semantics for these quantifiers makes higher-order logic stronger than first-order logic. In particular, the (semantic) logical consequence relation for second-order and higher-order logic is not semidecidable; there is no effective deduction system for second-order logic that is sound and complete under full semantics.
Second-order logic with full semantics is more expressive than first-order logic. For example, it is possible to create axiom systems in second-order logic that uniquely characterize the natural numbers and the real line. The cost of this expressiveness is that second-order and higher-order logics have fewer attractive metalogical properties than first-order logic. For example, the Löwenheim–Skolem theorem and compactness theorem of first-order logic become false when generalized to higher-order logics with full semantics.
Automated theorem proving refers to the development of computer programs that search and find derivations (formal proofs) of mathematical theorems. Finding derivations is a difficult task because the search space can be very large; an exhaustive search of every possible derivation is theoretically possible but computationally infeasible for many systems of interest in mathematics. Thus complicated heuristic functions are developed to attempt to find a derivation in less time than a blind search.
The related area of automated proof verification uses computer programs to check that human-created proofs are correct. Unlike complicated automated theorem provers, verification systems may be small enough that their correctness can be checked both by hand and through automated software verification. This validation of the proof verifier is needed to give confidence that any derivation labeled as "correct" is actually correct.
Some proof verifiers, such as Metamath, insist on having a complete derivation as input. Others, such as Mizar and Isabelle, take a well-formatted proof sketch (which may still be very long and detailed) and fill in the missing pieces by doing simple proof searches or applying known decision procedures: the resulting derivation is then verified by a small, core "kernel". Many such systems are primarily intended for interactive use by human mathematicians: these are known as proof assistants. They may also use formal logics that are stronger than first-order logic, such as type theory. Because a full derivation of any nontrivial result in a first-order deductive system will be extremely long for a human to write, results are often formalized as a series of lemmas, for which derivations can be constructed separately.
Automated theorem provers are also used to implement formal verification in computer science. In this setting, theorem provers are used to verify the correctness of programs and of hardware such as processors with respect to a formal specification. Because such analysis is time-consuming and thus expensive, it is usually reserved for projects in which a malfunction would have grave human or financial consequences.
For the problem of model checking, efficient algorithms are known to decide whether an input finite structure satisfies a first-order formula, in addition to computational complexity bounds: see Model checking#First-order logic.

</doc>
<doc id="10987" url="https://en.wikipedia.org/wiki?curid=10987" title="Functor">
Functor

In mathematics, specifically category theory, a functor is a map between categories. Functors were first considered in algebraic topology, where algebraic objects (such as the fundamental group) are associated to topological spaces, and maps between these algebraic objects are associated to continuous maps between spaces. Nowadays, functors are used throughout modern mathematics to relate various categories. Thus, functors are important in all areas within mathematics to which category theory is applied.
The words "category" and "functor" were borrowed by mathematicians from the philosophers Aristotle and Rudolf Carnap, respectively. The latter used "functor" in a linguistic context;
see function word.
Let "C" and "D" be categories. A functor "F" from "C" to "D" is a mapping that
That is, functors must preserve identity morphisms and composition of morphisms.
There are many constructions in mathematics that would be functors but for the fact that they "turn morphisms around" and "reverse composition". We then define a contravariant functor "F" from "C" to "D" as a mapping that
Note that contravariant functors reverse the direction of composition.
Ordinary functors are also called covariant functors in order to distinguish them from contravariant ones. Note that one can also define a contravariant functor as a "covariant" functor on the opposite category formula_19. Some authors prefer to write all expressions covariantly. That is, instead of saying formula_20 is a contravariant functor, they simply write formula_21 (or sometimes formula_22) and call it a functor.
Contravariant functors are also occasionally called "cofunctors".
There is a convention which refers to "vectors"—i.e., vector fields, elements of the space of sections formula_23 of a tangent bundle formula_24—as "contravariant" and to "covectors"—i.e., 1-forms, elements of the space of sections formula_25 of a cotangent bundle formula_26—as "covariant". This terminology originates in physics, and its rationale has to do with the position of the indices ("upstairs" and "downstairs") in expressions such as formula_27 for formula_28 or formula_29 for formula_30 In this formalism it is observed that the coordinate transformation symbol formula_31 (representing the matrix formula_32) acts on the basis vectors "in the same way" as on the "covector coordinates": formula_33—whereas it acts "in the opposite way" on the "vector coordinates" (but "in the same way" as on the basis covectors: formula_34). This terminology is contrary to the one used in category theory because it is the covectors that have "pullbacks" in general and are thus "contravariant", whereas vectors in general are "covariant" since they can be "pushed forward". See also Covariance and contravariance of vectors.
Every functor formula_20 induces the opposite functor formula_36, where formula_19 and formula_38 are the opposite categories to formula_39 and formula_40. By definition, formula_41 maps objects and morphisms identically to formula_42. Since formula_19 does not coincide with formula_39 as a category, and similarly for formula_40, formula_41 is distinguished from formula_42. For example, when composing formula_48 with formula_49, one should use either formula_50 or formula_51. Note that, following the property of opposite category, formula_52.
A bifunctor (also known as a binary functor) is a functor whose domain is a product category. For example, the Hom functor is of the type . It can be seen as a functor in "two" arguments. The Hom functor is a natural example; it is contravariant in one argument, covariant in the other.
A multifunctor is a generalization of the functor concept to "n" variables. So, for example, a bifunctor is a multifunctor with .
Diagram: For categories "C" and "J", a diagram of type "J" in "C" is a covariant functor formula_53.
(Category theoretical) presheaf: For categories "C" and "J", a "J"-presheaf on "C" is a contravariant functor formula_54.
Presheaves: If "X" is a topological space, then the open sets in "X" form a partially ordered set Open("X") under inclusion. Like every partially ordered set, Open("X") forms a small category by adding a single arrow if and only if formula_55. Contravariant functors on Open("X") are called "presheaves" on "X". For instance, by assigning to every open set "U" the associative algebra of real-valued continuous functions on "U", one obtains a presheaf of algebras on "X".
Constant functor: The functor which maps every object of "C" to a fixed object "X" in "D" and every morphism in "C" to the identity morphism on "X". Such a functor is called a "constant" or "selection" functor.
Endofunctor: A functor that maps a category to that same category; e.g., polynomial functor.
Identity functor: in category "C", written 1 or id, maps an object to itself and a morphism to itself. The identity functor is an endofunctor.
Diagonal functor: The diagonal functor is defined as the functor from "D" to the functor category "D" which sends each object in "D" to the constant functor at that object.
Limit functor: For a fixed index category "J", if every functor has a limit (for instance if "C" is complete), then the limit functor assigns to each functor its limit. The existence of this functor can be proved by realizing that it is the right-adjoint to the diagonal functor and invoking the Freyd adjoint functor theorem. This requires a suitable version of the axiom of choice. Similar remarks apply to the colimit functor (which assigns to every functor its colimit, and is covariant).
Power sets: The power set functor maps each set to its power set and each function formula_56 to the map which sends formula_57 to its image formula_58. One can also consider the contravariant power set functor which sends formula_59 to the map which
sends formula_60 to its inverse image formula_61
For example, if formula_62 then formula_63. Suppose formula_64 and formula_65. Then formula_66 is the function which sends any subset formula_67 of formula_1 to its image formula_69, which in this case means
formula_70, where formula_71 denotes the mapping under formula_66, so this could also be written as formula_73. For the other values, formula_74 Note that formula_75 consequently generates the trivial topology on formula_1. Also note that although the function formula_77 in this example mapped to the power set of formula_1, that need not be the case in general.
Fundamental group: Consider the category of pointed topological spaces, i.e. topological spaces with distinguished points. The objects are pairs , where "X" is a topological space and "x" is a point in "X". A morphism from to is given by a continuous map with .
To every topological space "X" with distinguished point "x", one can define the fundamental group based at "x", denoted . This is the group of homotopy classes of loops based at "x". If is a morphism of pointed spaces, then every loop in "X" with base point "x" can be composed with "f" to yield a loop in "Y" with base point "y". This operation is compatible with the homotopy equivalence relation and the composition of loops, and we get a group homomorphism from to . We thus obtain a functor from the category of pointed topological spaces to the category of groups.
In the category of topological spaces (without distinguished point), one considers homotopy classes of generic curves, but they cannot be composed unless they share an endpoint. Thus one has the fundamental groupoid instead of the fundamental group, and this construction is functorial.
Algebra of continuous functions: a contravariant functor from the category of topological spaces (with continuous maps as morphisms) to the category of real associative algebras is given by assigning to every topological space "X" the algebra C("X") of all real-valued continuous functions on that space. Every continuous map induces an algebra homomorphism by the rule for every "φ" in C("Y").
Tangent and cotangent bundles: The map which sends every differentiable manifold to its tangent bundle and every smooth map to its derivative is a covariant functor from the category of differentiable manifolds to the category of vector bundles.
Doing this constructions pointwise gives the tangent space, a covariant functor from the category of pointed differentiable manifolds to the category of real vector spaces. Likewise, cotangent space is a contravariant functor, essentially the composition of the tangent space with the dual space above.
Group actions/representations: Every group "G" can be considered as a category with a single object whose morphisms are the elements of "G". A functor from "G" to Set is then nothing but a group action of "G" on a particular set, i.e. a "G"-set. Likewise, a functor from "G" to the category of vector spaces, Vect, is a linear representation of "G". In general, a functor can be considered as an "action" of "G" on an object in the category "C". If "C" is a group, then this action is a group homomorphism.
Lie algebras: Assigning to every real (complex) Lie group its real (complex) Lie algebra defines a functor.
Tensor products: If "C" denotes the category of vector spaces over a fixed field, with linear maps as morphisms, then the tensor product formula_79 defines a functor which is covariant in both arguments.
Forgetful functors: The functor which maps a group to its underlying set and a group homomorphism to its underlying function of sets is a functor. Functors like these, which "forget" some structure, are termed "forgetful functors". Another example is the functor which maps a ring to its underlying additive abelian group. Morphisms in Rng (ring homomorphisms) become morphisms in Ab (abelian group homomorphisms).
Free functors: Going in the opposite direction of forgetful functors are free functors. The free functor sends every set "X" to the free group generated by "X". Functions get mapped to group homomorphisms between free groups. Free constructions exist for many categories based on structured sets. See free object.
Homomorphism groups: To every pair "A", "B" of abelian groups one can assign the abelian group Hom("A", "B") consisting of all group homomorphisms from "A" to "B". This is a functor which is contravariant in the first and covariant in the second argument, i.e. it is a functor (where Ab denotes the category of abelian groups with group homomorphisms). If and are morphisms in Ab, then the group homomorphism : is given by . See Hom functor.
Representable functors: We can generalize the previous example to any category "C". To every pair "X", "Y" of objects in "C" one can assign the set of morphisms from "X" to "Y". This defines a functor to Set which is contravariant in the first argument and covariant in the second, i.e. it is a functor . If and are morphisms in "C", then the map is given by .
Functors like these are called representable functors. An important goal in many settings is to determine whether a given functor is representable.
Two important consequences of the functor axioms are:
One can compose functors, i.e. if "F" is a functor from "A" to "B" and "G" is a functor from "B" to "C" then one can form the composite functor from "A" to "C". Composition of functors is associative where defined. Identity of composition of functors is the identity functor. This shows that functors can be considered as morphisms in categories of categories, for example in the category of small categories.
A small category with a single object is the same thing as a monoid: the morphisms of a one-object category can be thought of as elements of the monoid, and composition in the category is thought of as the monoid operation. Functors between one-object categories correspond to monoid homomorphisms. So in a sense, functors between arbitrary categories are a kind of generalization of monoid homomorphisms to categories with more than one object.
Let "C" and "D" be categories. The collection of all functors from "C" to "D" forms the objects of a category: the functor category. Morphisms in this category are natural transformations between functors.
Functors are often defined by universal properties; examples are the tensor product, the direct sum and direct product of groups or vector spaces, construction of free groups and modules, direct and inverse limits. The concepts of limit and colimit generalize several of the above.
Universal constructions often give rise to pairs of adjoint functors.
Functors sometimes appear in functional programming. For instance, the programming language Haskell has a class codice_1 where codice_2 is a polytypic function used to map functions ("morphisms" on "Hask", the category of Haskell types) between existing types to functions between some new types.

</doc>
<doc id="10989" url="https://en.wikipedia.org/wiki?curid=10989" title="Felix Hausdorff">
Felix Hausdorff

Felix Hausdorff (November 8, 1868 – January 26, 1942) was a German mathematician who is considered to be one of the founders of modern topology and who contributed significantly to set theory, descriptive set theory, measure theory, and functional analysis.
Life became difficult for Hausdorff and his family after Kristallnacht in 1938. The next year he initiated efforts to emigrate to the United States, but was unable to make arrangements to receive a research fellowship. On 26 January 1942, Felix Hausdorff, along with his wife and his sister-in-law, committed suicide by taking an overdose of veronal, rather than comply with German orders to move to the Endenich camp, and there suffer the likely implications, about which he held no illusions.
Hausdorff's father, the Jewish merchant Louis Hausdorff (1843–1896), moved in the autumn of 1870 with his young family to Leipzig and worked over time at various companies, including a linen-and cotton goods factory. He was an educated man and had become a Morenu at the age of 14. There are several treatises from his pen, including a long work on the Aramaic translations of the Bible from the perspective of Talmudic law.
Hausdorff's mother, Hedwig (1848–1902), who is also referred to in various documents as Johanna, came from the Jewish Tietz family. From another branch of this family came Hermann Tietz, founder of the first department store, and later co-owner of the department store chain called "Hermann Tietz". During the period of Nazi dictatorship the name was "Aryanised" to Hertie.
From 1878 to 1887 Felix Hausdorff attended the Nicolai School in Leipzig, a facility that had a reputation as a hotbed of humanistic education. He was an excellent student, class leader for many years and often recited self-written Latin or German poems at school celebrations. In his graduation in 1887 (with two Oberprimen), he was the only one who reached the highest grade.
The choice of subject was not easy for Hausdorff. Magda Dierkesmann, who was often a guest in the home of Hausdorff as a student in Bonn in the years 1926–1932, reported in 1967 that:
The decision was made to study the sciences in high school.
From summer term 1887 to summer semester 1891 Hausdorff studied mathematics and astronomy, mainly in his native city of Leipzig, interrupted by one semester in Freiburg (summer semester 1888) and Berlin (winter semester 1888/1889). The surviving testimony of other students show him as extremely versatile interested young man, who, in addition to the mathematical and astronomical lectures, attended lectures in physics, chemistry and geography, and also lectures on philosophy and history of philosophy as well as on issues of language, literature and social sciences. In Leipzig he heard lectures on the history of music from musicologist Paul. His early love of music lasted a lifetime; in Hausdorff's house there were impressive musical evenings with the landlord at the piano, according to witness statements made by various participants. Even as a student in Leipzig, he was an admirer and connoisseur of the music of Richard Wagner.
In later semesters of his studies, Hausdorff was close to Heinrich Bruns (1848–1919). 
Bruns was professor of astronomy and director of the observatory at the University of Leipzig. Under him, Hausdorff graduated in 1891 with a work on the theory of astronomical refraction of light in the atmosphere. Two publications on the same subject followed, and in 1895 his habilitation also followed with a thesis on the absorbance of light in the atmosphere. These early astronomical works of Hausdorff have—despite their excellent mathematical working through—not gained importance.
Firstly, the underlying idea of Bruns has not proved viable (there were needs for refraction observations near the astronomical horizon, which—as Julius Bauschinger could show a little later—in principle can not be obtained with the required accuracy). On the other hand, the progress in the direct measurement of atmospheric data (weather balloon ascents) has since made the painstaking accuracy of this data from refraction observations unnecessary. In the time between PhD and habilitation Hausdorff completed the yearlong-volunteer military requirement and worked for two years as a human computer at the observatory in Leipzig.
With his habilitation, Hausdorff became a lecturer at the University of Leipzig and began an extensive teaching in a variety of mathematical areas. In addition to teaching and research in mathematics, he went with his literary and philosophical inclinations. A man of varied interests, educated, highly sensitive and sophisticated in thinking, feeling and experiencing, he frequented in his Leipzig period with a number of famous writers, artists and publishers such as Hermann Conradi, Richard Dehmel, Otto Erich Hartleben, Gustav Kirstein, Max Klinger, Max Reger and Frank Wedekind. The years 1897 to about 1904 mark the high point of his literary and philosophical creativity, during which time 18 of his 22 pseudonymous works were published, including a book of poetry, a play, an epistemological book and a volume of aphorisms.
Hausdorff married Charlotte Goldschmidt in 1899, daughter of Jewish doctor Siegismund Goldschmidt. Her stepmother was the famous suffragist and preschool teacher Henriette Goldschmidt. Hausdorff's only child, daughter Lenore (Nora), was born in 1900; she survived the era of National Socialism and enjoyed a long life, dying in Bonn in 1991.
In December 1901 Hausdorff was appointed as adjunct associate professor at the University of Leipzig. The often repeated assertion that Hausdorff got a call from Göttingen and rejected it cannot be verified and is probably wrong. When applying in Leipzig, Dean Kirchner had been led to very positive vote of his colleagues, written by Heinrich Bruns, still accompanied by the following words:
This quote emphasizes the undisguised anti-Semitism present, which especially took a sharp upturn after the Gründerkrach in 1873 throughout the German Reich. Leipzig was a center of anti-Semitic movement, especially among the student body. This may well be the reason that Hausdorff did not feel at ease at Leipzig. Another reason was perhaps the stresses due to the hierarchical posturing of the Leipzig professors.
After his habilitation, Hausdorff wrote another work on optics, on non-Euclidean geometry, and on hypercomplex number systems, as well as two papers on probability theory. However, his main area of work soon became set theory, especially the theory of ordered sets. It was initially a philosophical interest, which led him around 1897 to study Georg Cantor's work. Already, in the summer semester of 1901, Hausdorff gave a lecture on set theory. This was one of the first lectures on set theory at all; Ernst Zermelo's lectures in Göttingen College during the winter semester of 1900/1901 were a little earlier. That year, he published his first paper on order types in which he examined a generalization of well-orderings called graded order types, where a linear order is graded if no two of its segments share the same order type. He generalized the Cantor–Bernstein theorem, which said the collection of countable order types has the cardinality of the continuum and showed that the collection of all graded types of an idempotent cardinality has a cardinality of 2.
For the summer semester 1910 Hausdorff was appointed as professor to the University of Bonn. In Bonn, he began a lecture on set theory, which he repeated in the summer semester 1912, substantially revised and expanded.
In the summer of 1912 he also began work on his magnum opus, the book "Basics of set theory". It was completed in Greifswald, where Hausdorff had been appointed for the summer semester as full professor in 1913, and was released in April 1914.
The University of Greifswald was the smallest of the Prussian universities. Also, the mathematical institute was small; in the summer semester 1916 and winter semester 1916/17 Hausdorff was the only mathematician in Greifswald. This brought with it that he was almost fully occupied in teaching the basic courses. It was a substantial improvement of his academic situation when Hausdorff was appointed in 1921 to Bonn. Here he could develop a thematically wide-spanned teaching and always lecture on the latest research. He gave a particularly noteworthy lecture on probability theory (NL Hausdorff: Capsule 21: Fasz 64) in the summer semester 1923, in which he grounded this theory in measure-theoretic axiomatic theory, and this occurred ten years before A. N. Kolmogorov's "Basic concepts of probability theory" (reprinted in full in the collected works, Volume V). In Bonn, Hausdorff had Eduard Study, and later with Otto Toeplitz, outstanding mathematicians as well as colleagues and friends.
The National Socialist party's state doctrine established anti-Semitism and the seizure of power. Hausdorff was not initially concerned by the "Law for the Restoration of the Professional Civil Service", adopted in 1933, because he had been a German official since before 1914. However, he was not completely spared, as one of his lectures was interrupted by Nazi students. He stopped his 1934/1935 winter semester Calculus III course from 20 November on. During that time, there was a working session of the National Socialist German Student Union (NSDStB) at the University of Bonn, which chose "Race and Ethnicity" as their theme for the semester. The assumption is that this event is related to the cancellation of Hausdorff's class, because otherwise he never, in his long career as a university teacher, stopped a class.
On March 31, 1935, after some going back and forth, Hausdorff was finally given emeritus status. No words of thanks were given for 40 years of successful work in the German higher education system. He worked tirelessly and published, in addition to the expanded edition of his work on set theory, seven works on topology and descriptive set theory, all published in Polish magazines: one in "Studia Mathematica", the others in "Fundamenta Mathematicae".
His Nachlass shows that Hausdorff was still working mathematically during these increasingly difficult times and following current developments of interest. He was selflessly supported at this time by Erich Bessel-Hagen, a loyal friend to the Hausdorff family who obtained books and magazines from the Library of the institute, which Hausdorff was no longer allowed to enter as a Jew.
About the humiliations to which Hausdorff and his family especially were exposed to after Kristallnacht in 1938, much is known and from many different sources, such as from the letters of Bessel-Hagen.
In vain, Hausdorff asked the mathematician Richard Courant in 1939 for a research fellowship to be able to emigrate into the USA.
In mid-1941, the Bonn Jews began to be deported to the Monastery "To Perpetual Adoration" in Endenich, from which the nuns had been expelled. The transports to the death camps in the east occurred later. After Felix Hausdorff, his wife and his wife's sister, Edith Pappenheim (who was living with them) were ordered in January 1942 to move to the Endenich camp, they committed suicide on 26 January 1942 by taking an overdose of veronal. Their final resting place is located on the Poppelsdorfer cemetery in Bonn. Between their placement in temporary camps and his suicide, he gave his handwritten "Nachlass" to the Egyptologist and presbyter Hans Bonnet, who saved as much of them as possible, despite the destruction of his house by a bomb.
Some of his fellow Jews may have had illusions about the camp Endenich, but not Hausdorff. E. Neuenschwander discovered in the estate of Bessel-Hagen the farewell letter that Hausdorff wrote to his Jewish lawyer Hans Wollstein. Here is the beginning and end of the letter:
After thanking friends and, in great composure, expressing his last wishes regarding his funeral and his will, Hausdorff writes:
Unfortunately, this desire was not fulfilled. Hausdorff's lawyer Wollstein was murdered in Auschwitz.
Hausdorff's library was sold by his son-in-law and sole heir, Arthur König. The handwritten "Nachlass" was adopted by a family friend, the Bonn Egyptologist Hans Bonnet, for storage. It is now in the University and State Library of Bonn. The "Nachlass" is catalogued.
Hausdorff's volume of aphorisms, published in 1897, was his first work published under the pseudonym Paul Mongré. It is entitled "Sant' Ilario. Thoughts from the landscape of Zarathustra". The subtitle of "Sant 'Ilario," "Thoughts from the landscape of Zarathustra," plays first on the fact that Hausdorff had completed his book during a recovery stay on the Ligurian coast by Genoa and that in this same area, Friedrich Nietzsche wrote the first two parts of Thus Spoke Zarathustra; he also alludes to his spiritual closeness to Nietzsche. In an article on Sant 'Ilario in the weekly paper Die Zukunft, Hausdorff acknowledged in his debt to Nietzsche.
Hausdorff was not trying to copy or even exceed Nietzsche. "Of Nietzsche imitation no trace", says a contemporary review. He follows Nietzsche in an attempt to liberate individual thinking, to take the liberty of questioning outdated standards. Hausdorff maintained critical distance to the late works of Nietzsche. In his essay on the book "The Will to Power" compiled from notes left in the Nietzsche Archive he says:
His critical standard he took from Nietzsche himself,
In 1898 appeared—also under the pseudonym Paul Mongré—Hausdorff's epistemological experiment "Chaos in cosmic selection". The critique of metaphysics put forward in this book had its starting point in Hausdorff's confrontation with Nietzsche's idea of eternal recurrence. It ultimately gets to destroying any kind of metaphysics. Of the world itself, from the transcendent world core—as Hausdorff expressed—we know nothing and we know nothing. We must assume "the world itself" as undetermined and undeterminable, as a mere chaos. The world of our experience, our cosmos is the result of the selection, the selection that we have always instinctively made according to our possibilities of understanding and make more. From that chaos would also be seen other orders, other Kosmoi, conceivably. At any rate, from the world of our cosmos you can not conclude the existence of a transcendent world.
In 1904, in the magazine The New Rundschau, Hausdorff's play appeared, the one-act play "The doctor in his honor". It is a crude satire on the duel and on the traditional concepts of honor and nobility of the Prussian officer corps, which in the developing bourgeois society were increasingly anachronistic. "The doctor in his honor" was Hausdorff's greatest literary success. In 1914–1918 there were numerous performances in more than thirty cities. Hausdorff later wrote an epilogue to the play, but it was not performed at that time. Only in 2006 did this epilogue have its premier at the annual meeting of the German Mathematical Society in Bonn.
Besides the works above mentioned Hausdorff wrote numerous essays that appeared in some of the leading literary magazines of the time, as well as a book of poems, "Ecstasy" (1900). Some of his poems were set to music by Austrian composer Joseph Marx.
Hausdorff's entry into a thorough study of ordered sets was prompted in part by Cantor's continuum problem: which place does the cardinal number formula_1 take in the series formula_2. In a letter to Hilbert on 29 September 1904, he speaks of this problem, "it has plagued almost like a monomania". He saw in the set formula_3 a new strategy to attack the problem. Cantor had suspected formula_4, but had only shown formula_5. formula_6 is the "number" of possible well-orderings of a countable set ; formula_7 had now emerged as the "number" of all possible orders of such an amount. It was natural, therefore, to study systems that are more special than general orders, but more general than well-orderings. Hausdorff did just that in his first volume of 1901 with the publication of theoretical studies of "graded sets". We know from the results of Kurt Gödel and Paul Cohen, that this strategy to solve the continuum problem is just as ineffectual as Cantor's strategy, which was aimed at generalizing the Cantor–Bendixson principle for closed sets to general uncountable sets.
In 1904 Hausdorff published the recursion named after him:
For each non-limit ordinal formula_8 we have formula_9
This formula was, together with the later notion of cofinality introduced by Hausdorff, the basis for all further results for Aleph exponentiation. Hausdorff' excellent knowledge of the problems of this type of sequence was also empowered by his efforts to uncover the error in Julius König's lecture at the International Congress of Mathematicians in 1904 in Heidelberg. There König had argued that the continuum cannot be well-ordered, so its cardinality is no Aleph, and thus caused a great stir. The assertion that it was Hausdorff who clarified the mistake has a special weight because a false image was drawn in the historical literature for more than 50 years of the events in Heidelberg.
In the years 1906–1909 Hausdorff did his fundamental work on ordered sets. Only a few points can be touched briefly. Of fundamental importance to the whole theory is the concept of cofinality that Hausdorff introduced. An ordinal is called regular if it is cofinal with any smaller ordinal; otherwise it is singular. Hausdorff's question whether there are regular numbers with index a limit ordinal, was the starting point for the theory of inaccessible cardinals. Hausdorff had already noticed that such numbers, if they exist, must be of "exorbitant size".
Of fundamental importance is the following theorem of Hausdorff: for each unbounded ordered dense set formula_10 there are two uniquely determined regular initial numbers formula_11 so that formula_10 is cofinal with formula_13 and coinitial with formula_14 (* Denotes the inverse order). This theorem provides, for example, a technique to characterize elements and gaps in ordered sets. Thus Hausdorff utilized the gap characters and element characters introduced by him.
If formula_15 is a predetermined set of characters (element and gap characters), the question arises whether there are ordered sets whose character set is exactly formula_15. One can easily find a necessary condition for formula_15. Hausdorff was able to show that this condition is also sufficient. For this one needs a rich reservoir of ordered sets; Hausdorff had created this with his theory of general products and powers. In this reservoir such interesting structures are found as the Hausdorff formula_18 normal-types, in connection with which Hausdorff first formulated the generalized continuum hypothesis. Hausdorff's formula_18-sets formed the starting point for the study of the important model theory of saturated structure.
Hausdorff's general products and powers of cardinalities had led him to the concept of partially ordered set. The question of whether any ordered subset of a partially ordered set is contained in a maximal ordered subset was answered in the positive by Hausdorff using the well-ordering theorem. This is the Hausdorff maximal principle. It follows not only from the well-ordering theorem (or from the (equivalent to this) axiom of choice), but it is, as it turned out, even to the axiom of choice are equivalent.
Already, in 1908, Arthur Moritz Schoenflies found in the second part of his report on set theory, that the newer theory of ordered sets (i.e., that which occurred after Cantor's extensions thereof) was almost exclusively due to Hausdorff.
According to former notions, set theory included not only the general set theory and the theory of sets of points, but also dimension and measure theory. Hausdorff's work was the first textbook which presented all of set theory in this broad sense, systematically and with full proofs. Hausdorff was aware of how easily the human mind can err while also seeking for rigor and truth. So he proposed in the preface of the work:
This book went far beyond its masterful portrayal of the known. It also contained a series of important original contributions of the author that can only be hinted at in the following.
The first six chapters deal with the basic concepts of the general set theory. At the beginning Hausdorff sets forth a detailed set algebra with some pioneering new concepts (differences chains, set rings and set fields, formula_20- and formula_21-systems). These introductory paragraphs on sets and their connections included, for example, the modern set-theoretic notion of functions. Next followed in Chapters 3 to 5 the classical theory of cardinal numbers, order types and ordinals. In the sixth chapter "Relations between ordered and well-ordered sets" Hausdorff presents, among other things, the most important results of his own research on ordered sets.
In the chapters on "point sets"—the topological chapters—Hausdorff developed for the first time, based on the known neighborhood axioms, a systematic theory of topological spaces, where in addition he added the separation axiom later named after him. This theory emerges from a comprehensive synthesis of earlier approaches of other mathematicians and Hausdorff's own reflections on the problem of space. The concepts and theorems of classical point set theory formula_22 are—as far as possible—transferred to the general case, and thus become part of the newly created general or set-theoretic topology. But Hausdorff not only performed this "translation work", but he developed also basic construction method of topology as nucleation (interior, dense-in-itself core) and shell formation (closure), and he works the fundamental importance of the concept of open set (called "area" by him) and of the compactness introduced by Fréchet. He also founded and developed the theory of the connected set, particularly through the introduction of the terms "component" and "quasi-component".
By the first and eventually the second Hausdorff countability axioms the considered spaces were gradually further specialized. A large class of spaces satisfying the countable first axiom are metric spaces. They were introduced in 1906 by Fréchet under the name "classes (E)". The term "metric space" comes from Hausdorff. In "Principles", he developed the theory of metric spaces and systematically enriched it through a series of new concepts: Hausdorff metric, complete, total boundedness, formula_23-connectivity, reducible sets. Fréchet's work had been little noticed; only through Hausdorff's "Principles" did metric spaces become the common property of the mathematician.
The chapter on illustrations and the final chapter of "Principles" on measure and integration theory are enriched by the generality of the material and the originality of presentation. Hausdorff's mention of the importance of measure theory for probability had great historical effect, despite its laconic brevity. One finds in this chapter the first correct proof of the strong law of large numbers of Émile Borel. Finally, the appendix contains the single most spectacular result of the whole book, namely Hausdorff's theorem that one cannot define a volume for all bounded subsets of formula_22 for formula_25. The proof is based on Hausdorff's paradoxical ball decomposition, whose production requires the axiom of choice.
During the 20th century, it became the standard to build mathematical theories on axiomatic set theory. The creation of axiomatically founded generalized theories, such as the general topology, served among other things to single out the common structural core for various specific cases or regions and then set up an abstract theory, which contained all these parts as special cases. This brought a great success in the form of simplification and harmonization and ultimately brought on economy of thought with itself. Hausdorff himself highlighted this aspect in the "Principles". The topological chapter the basic concepts are methodologically a pioneering effort, and they showed the way for the development of modern mathematics.
"Principles of set theory" appeared in an already tense time on the eve of the First World War. In August 1914, the war, which also dramatically affected the scientific life in Europe. Under these circumstances, could hardly be effective Hausdorff's book in the first five to six years after its appearance. After the war, a new generation of young researchers set forth to expand on the suggestions that were included in this work in such abundance, and with no doubt, the topology was the focus of attention. The journal "Fundamenta Mathematicae" played a special role in the reception of Hausdorff's ideas, founded in Poland in 1920. It was one of the first mathematical journals with special emphasis on set theory, topology, theory of real functions, measure and integration theory, functional analysis, logic and foundations of mathematics. In this spectrum, a special focus was the general topology. Hausdorff's "Principles" were present in Fundamenta Mathematicae from the first volume in a remarkable frequency. Of the 558 works (Hausdorff's own three works not calculated), which appeared in the first twenty volumes from 1920 to 1933, 88 cite "Principles". One even has to take into account that as Hausdorff's conceptions increasingly became commonplace, so they were also used in a number of works that did not mention them explicitly.
The Russian topological school, founded by Paul Alexandroff and Paul Urysohn, was based heavily on Hausdorff's "Principles". This is shown by the surviving correspondence in Hausdorff's Nachlass with Urysohn, and especially Alexandroff and Urysohn's "Mémoire sur les multiplicités Cantoriennes", a work the size of a book, in which Urysohn developed dimension theory and "Principles" is cited no fewer than 60 times.
Long after the Second World War there was a strong demand for Hausdorff's book, and there were three reprints at Chelsea from 1949, 1965 and 1978.
In 1916, Alexandroff and Hausdorff independently solved the continuum problem for Borel sets: Every Borel set in a complete separable metric space is either countable or has the cardinality of the continuum. This result generalizes the Cantor–Bendixson theorem that such a statement holds for the closed sets of formula_22. For linear formula_27 sets William Henry Young had proved the result in 1903, for formula_28 sets Hausdorff obtained a corresponding result in 1914 in the "Principles". The theorem of Alexandroff and Hausdorff was a strong impetus for further development of descriptive set theory.
Among the publications of Hausdorff in his time at Greifswald time the work "Dimension and outer measure" from 1919 is particularly outstanding. It has remained highly topical and in later years has been probably the most cited mathematical original work from the decade from 1910 to 1920. In this work, the concepts were introduced which are now known as Hausdorff measure and the Hausdorff dimension.
The concept of Hausdorff dimension is useful for the characterization and comparison of "highly rugged quantities". The concepts of "Dimension and outer measure" have experienced applications and further developments in many areas such as in the theory of dynamical systems, geometric measure theory, the theory of self-similar sets and fractals, the theory of stochastic processes, harmonic analysis, potential theory and number theory.
Significant analytical work of Hausdorff occurred in his second time at Bonn. In "Summation methods and moment sequences I" in 1921, he developed a whole class of summation methods for divergent series, which today are called Hausdorff methods. In Hardy's classic "Divergent Series", an entire chapter is devoted to the Hausdorff method. The classical methods of Hölder and Cesàro proved to be special Hausdorff method. Every Hausdorff method is given by a moment sequence; in this context Hausdorff gave an elegant solution of the moment problem for a finite interval, bypassing the theory of continued fractions. In "Moment problems for a finite interval" of 1923 he treated more special moment problems, such as those with certain restrictions for generating density formula_29, for instance formula_30. Criteria for solvability and determination of moment problems occupied Hausdorff for many years as hundreds of pages of studies in his Nachlass attest.
A significant contribution to the emerging functional analysis in the twenties was Hausdorff's extension of the Riesz-Fischer theorem toformula_31 spaces in his 1923 work "An extension of Parseval's theorem on Fourier series". He proved the inequalities now named after him and W.H. Young. The Hausdorff–Young inequalities became the starting point of major new developments.
Hausdorff's book "Set Theory" appeared in 1927. This was declared as a second Edition of "Principles", but it was actually a completely new book. Since the scale was significantly reduced due to its appearance in Goschen's teaching library, large parts of the theory of ordered sets and measures and integration theory were removed. "More than these deletions, the reader will perhaps regret" (said Hausdorff in the preface), "that I, to further save space in point set theory, have abandoned the topological point of view through which the first edition has apparently acquired many friends have limited myself to the easier theory of metric spaces".
In fact, this was an explicit regret of some reviewers of the work. As a kind of compensation Hausdorff showed for the first time the then current state of descriptive set theory. This fact assured the book almost as intense a reception as "Principles", especially in Fundamenta Mathematicae. As a textbook it was very popular. In 1935 there was an expanded edition published, and this was reprinted by Dover in 1944. An English translation appeared in 1957 with reprints in 1962 and 1967.
There was also a Russian edition (1937), although it was only partially a faithful translation, and partly a reworking by Alexandroff and Kolmogorov. In this translation the topological point of view again moved to the forefront. In 1928 a review of "Set Theory" appeared from the pen of Hans Hahn. Perhaps Hahn had the danger of German anti-Semitism in his mind as he closed this discussion with the following sentence:
In his last work Erweiterung einer stetigen Abbildung, Hausdorff showed in 1938 that a continuous function from a closed subset formula_32 of a metric space formula_33 can be extended to all of formula_33 (although the image may need to be extended). As a special case, every homeomorphism from formula_32 can be extended to a homeomorphism from formula_33. This work set forth results from earlier years. In 1919, in Über halbstetige Funktionen und deren Verallgemeinerung, Hausdorff had, among other things, given another proof of the Tietze extension theorem. In 1930, in "Erweiterung einer Homöomorphie" (Extending a Homeomorphism), he showed the following: Let formula_33 be a metric space, formula_38 a closed subset. If formula_32 is given a new metric without changing the topology, this metric can be extended to the entire space without changing the topology. The work "Gestufte Räume" appeared in 1935. Here Hausdorff discussed spaces which fulfilled the Kuratowski closure axioms up to just the axiom of idempotence. He named them "graded spaces" (often also called closure spaces) and used them in the study of the relationships between the Fréchet limit spaces and topological spaces.
The name Hausdorff is found throughout mathematics. Among others, these concepts were named after him:
In the universities of Bonn and Greifswald, these things were named in his honor:
Besides these, in Bonn there is the Hausdorffstraße (Hausdorff Street), where he first lived. (Haus-Nr. 61). In Greifswald there is a Felix-Hausdorff–Straße, where the Institutes for Biochemistry and Physics are located, among others. Since 2011, there is a "Hausdorffweg" (Hausdorff-Way) in the middle of Leipziger Ortsteil Gohlis.
The Asteroid 24947 Hausdorff was named after him.
Only a selection of the essays that appeared in text are shown here.
"Hausdorff on Ordered Sets". Trans. and Ed.: Jacob M. Plotkin, American Mathematical Society 2005.
The "Hausdorff-Edition", edited by E. Brieskorn (†), F. Hirzebruch (†), W. Purkert (all Bonn), R. Remmert (†) (Münster) and E. Scholz (Wuppertal) with the collaboration of over twenty mathematicians, historians, philosophers and scholars, is an ongoing project of the North Rhine-Westphalian Academy of Sciences, Humanities and the Arts to present the works of Hausdorff, with commentary and much additional material. The planned nine volumes are being published by Springer-Verlag, Heidelberg. As of 2019, eight volumes have been published with volume I being split up into volume IA and volume IB. See the website of the Hausdorff Project website of the Hausdorff Edition (German) for its current status and further information. The projected volumes are:

</doc>
<doc id="10990" url="https://en.wikipedia.org/wiki?curid=10990" title="Fimbulwinter">
Fimbulwinter

In Norse mythology, Fimbulvetr (or "fimbulvinter"), commonly rendered in English as Fimbulwinter, is the immediate prelude to the events of Ragnarök. It means "great winter". In Old English it is pronounced as "Fifelwinter".
Fimbulwinter is the harsh winter that precedes the end of the world and puts an end to all life on Earth. Fimbulwinter is three successive winters, when snow comes in from all directions, without any intervening summer. Then, there will be innumerable wars.
The event is described primarily in the "Poetic Edda". In the poem "Vafþrúðnismál", Odin poses the question to Vafþrúðnir as to who of mankind will survive the Fimbulwinter. Vafþrúðnir responds that Líf and Lífþrasir will survive and that they will live in the forest of Hoddmímis holt. 
The mythology might be related to the extreme weather events of 535–536, which resulted in a notable drop in temperature across northern Europe. There have also been several popular ideas about whether the particular piece of mythology has a connection to the climate change that occurred in the Nordic countries at the end of the Nordic Bronze Age from about 650 BC. Before that climate change, the Nordic countries were considerably warmer.
In Denmark, Norway, Sweden and other Nordic countries, the term "fimbulvinter" is still used to refer to an unusually cold and harsh winter.
Fimbulvetr comes from Old Norse, meaning "awful, great winter." The prefix "fimbul" means "the great/big" so the correct interpretation of the word is "the great winter."
The strategy videogame "Age of Mythology" gives the Fimbulwinter God Power to Norse players that chose Tyr as their minor god in the Mythic Age. This power summons groups of powerful Fimbulwinter Wolves in up to four enemy town centers, who will start to attack the buildings and units around them. While Fimbulwinter is active, no other God Powers can be used.
In the 2008 MMORPG "Wizard101", the Fimbulvetr was adapted to "Everwinter" for the 2011 Wintertusk expansion to Grizzleheim, a world heavily based on Nordic culture and mythology. In the game, players must stop the Coven from awaking Ymir and causing the end of the world.
In "Bayonetta 2", Fimbulventr is the name of a mountain which hides entrances to Inferno and Paradiso.
In the 2013 game "Tower of Saviors", Fimbulwinter is a title of the complete is all monster from "Norse Gods - Power Release" series.
In the tactical strategy RPG series "Fire Emblem", there is a magical tome that goes by Fimbulvetr within many of the games.
In the 2018 game "God of War", Fimbulwinter occurs a century earlier than it was supposed to.
In the 2018 game "Frostpunk", the citizens of the frozen wasteland have turned away from Christianity in favor of the old gods, believing the apocalypse to be Fimbulwinter proceeding Ragnarok.

</doc>
<doc id="10991" url="https://en.wikipedia.org/wiki?curid=10991" title="February 10">
February 10


</doc>
<doc id="10992" url="https://en.wikipedia.org/wiki?curid=10992" title="Frankfurt">
Frankfurt

Frankfurt (officially: Frankfurt am Main (; Hessian: "Frangford am Maa", "Frank ford on the Main")) is a metropolis and the largest city of the German state of Hesse. Its 763,380 inhabitants as of December 31, 2019 make it the fifth-largest city in Germany. On the River Main (a tributary of the Rhine), it forms a continuous conurbation with the neighbouring city of Offenbach am Main and its urban area has a population of 2.3 million. The city is at the centre of the larger Rhine-Main Metropolitan Region, which has a population of 5.5 million and is Germany's second-largest metropolitan region after the Rhine-Ruhr Region. Frankfurt's central business district lies about northwest of the geographic centre of the EU at Gadheim, Lower Franconia. Like France and Franconia, the city is named after the Franks. Frankfurt is the largest city in the Rhine Franconian dialect area.
Frankfurt was a city state, the Free City of Frankfurt, for nearly five centuries and was one of the most important cities of the Holy Roman Empire, as a site of imperial coronations; it lost its sovereignty upon the collapse of the empire in 1806, regained it in 1815 and then lost it again in 1866, when it was annexed (though neutral) by the Kingdom of Prussia. It has been part of the state of Hesse since 1945. Frankfurt is culturally, ethnically and religiously diverse, with half of its population, and a majority of its young people, having a migrant background. A quarter of the population consists of foreign nationals, including many expatriates.
Frankfurt is a global hub for commerce, culture, education, tourism and transportation, and rated as an "alpha world city" according to GaWC. It is the site of many global and European corporate headquarters. Frankfurt Airport is Germany's busiest. Frankfurt is one of the major financial centres of the European continent, with the headquarters of the European Central Bank, Deutsche Bundesbank, Frankfurt Stock Exchange, Deutsche Bank, DZ Bank, KfW, Commerzbank, several cloud and fintech startups and other institutes. Automotive, technology and research, services, consulting, media and creative industries complement the economic base. Frankfurt's DE-CIX is the world's largest internet exchange point. Messe Frankfurt is one of the world's largest trade fairs. Major fairs include the Music Fair and the Frankfurt Book Fair, the world's largest book fair.
Frankfurt is home to influential educational institutions, including the Goethe University, the UAS, the FUMPA and graduate schools like the Frankfurt School of Finance & Management. Its renowned cultural venues include the concert hall Alte Oper, Europe's largest English theatre and many museums (e.g. the Museumsufer ensemble with Städel and Liebieghaus, Senckenberg Natural Museum, Goethe House and the Schirn art venue at the old town). Frankfurt's skyline is shaped by some of Europe's tallest skyscrapers. The city is also characterised by various green areas and parks, including the central Wallanlagen, the City Forest, two major botanical gardens (the Palmengarten and the University's Botanical Garden) and the Frankfurt Zoo. In electronic music, Frankfurt has been a pioneering city since the 1980s, with renowned DJs including Sven Väth, Marc Trauner, Scot Project, Kai Tracid, and the clubs Dorian Gray, U60311, Omen and Cocoon. In sports, the city is known as the home of the top-tier football club Eintracht Frankfurt, the Löwen Frankfurt ice hockey team, the basketball club Frankfurt Skyliners, the Frankfurt Marathon and the venue of Ironman Germany. It was also one of the host cities of the 1974 and 2006 FIFA World Cups.
Frankfurt is the largest financial centre in continental Europe. It is home to the European Central Bank, Deutsche Bundesbank, Frankfurt Stock Exchange and several large commercial banks.
The Frankfurt Stock Exchange is one of the world's largest stock exchanges by market capitalization and accounts for more than 90 percent of the turnover in the German market.
In 2010, 63 national and 152 international banks had their registered offices in Frankfurt, including Germany's major banks, notably Deutsche Bank, DZ Bank, KfW and Commerzbank, as well as 41 representative offices of international banks.
Frankfurt is considered a global city (alpha world city) as listed by the GaWC group's 2012 inventory. Among global cities it was ranked 10th by the Global Power City Index 2011 and 11th by the Global City Competitiveness Index 2012. Among financial centres it was ranked 8th by the International Financial Centers Development Index 2013 and 9th in the 2013 Global Financial Centres Index.
Its central location within Germany and Europe makes Frankfurt a major air, rail and road transport hub. Frankfurt Airport is one of the world's busiest international airports by passenger traffic and the main hub for Germany's flag carrier Lufthansa. Frankfurt Central Station is one of the largest rail stations in Europe and the busiest junction operated by Deutsche Bahn, the German national railway company, with 342 trains a day to domestic and European destinations. Frankfurter Kreuz, the Autobahn interchange close to the airport, is the most heavily used interchange in the EU, used by 320,000 cars daily. In 2011 human-resource-consulting firm Mercer ranked Frankfurt as seventh in its annual 'Quality of Living' survey of cities around the world. According to "The Economist" cost-of-living survey, Frankfurt is Germany's most expensive city and the world's 10th most expensive.
Frankfurt has many high-rise buildings in the city centre, forming the Frankfurt skyline. It is one of the few cities in the European Union to have such a skyline, which is why Germans sometimes refer to Frankfurt as Mainhattan, a portmanteau of the local Main River and Manhattan. The other well known and obvious nickname is Bankfurt. Before World War II the city was globally noted for its unique old town with timber-framed buildings, the largest timber-framed old town in Europe. The Römer area was later rebuilt and is popular with visitors and for events such as Christmas markets. Other parts of the old town were reconstructed as part of the Dom-Römer Project from 2012 to 2018.
"Frankonovurd" (in Old High German) or "Vadum Francorum" (in Latin) were the first names mentioned in written records from 794. It transformed to "Frankenfort" during the Middle Ages and then to "Franckfort" and "Franckfurth" in the modern era. According to historian David Gans, the city was named 146 AD by its builder, a Frankish king named Zuna, who ruled over the province then known as Sicambri. He hoped thereby to perpetuate the name of his lineage. The name is derived from the "Franconofurd" of the Germanic tribe of the Franks; "Furt" (cf. English "ford") where the river was shallow enough to be crossed on foot.
By the 19th century, the name "Frankfurt" had been established as the official spelling. The older English spelling of "Frankfort" is now rarely seen in reference to Frankfurt am Main, although more than a dozen other towns and cities, mainly in the United States, use this spelling (e.g., Frankfort, Kentucky; Frankfort, New York; Frankfort, Illinois).
The suffix "am Main" has been used regularly since the 14th century. In English, the city's full name of "Frankfurt am Main" means "Frankfurt on the Main" (pronounced like English "mine" or German "mein"). Frankfurt is located on an ancient ford (German: "") on the Main River. As a part of early Franconia, the inhabitants were the early Franks, thus the city's name reveals its legacy as "the ford of the Franks on the Main".
Among English speakers, the city is commonly known simply as Frankfurt, but Germans occasionally call it by its full name to distinguish it from the other (significantly smaller) German city of Frankfurt an der Oder in the "Land" of Brandenburg on the Polish border.
The city district Bonames has a name probably dating back to Roman times, thought to be derived from "" (good table).
The common abbreviations for the city, primarily used in railway services and on road signs, are "Frankfurt (Main)", "Frankfurt (M)", "Frankfurt a. M.", "Frankfurt/Main" or "Frankfurt/M". The common abbreviation for the name of the city is "FFM". Also in use is "FRA", the IATA code for Frankfurt Airport.
Roman settlements were established in the area of the "Römer", probably in the first century. "Nida" (Heddernheim) was also a Roman civitas capital.
Alemanni and Franks lived there, and by 794, Charlemagne presided over an imperial assembly and church synod, at which "Franconofurd" (alternative spellings end with -furt and -) was first mentioned.
Frankfurt was one of the most important cities in the Holy Roman Empire. From 855, the German kings and emperors were elected and crowned in Aachen. From 1562, the kings and emperors were crowned in Frankfurt, initiated for Maximilian II. This tradition ended in 1792, when Franz II was elected. His coronation was deliberately held on Bastille Day, 14 July, the anniversary of the storming of the Bastille. The elections and coronations took place in St. Bartholomäus Cathedral, known as the "Kaiserdom" (Emperor's Cathedral), or its predecessors.
The "Frankfurter Messe" (Frankfurt Trade Fair) was first mentioned in 1150. In 1240, Emperor Friedrich II granted an imperial privilege to its visitors, meaning they would be protected by the empire. The fair became particularly important when similar fairs in French Beaucaire lost attraction around 1380. Book trade fairs began in 1478.
In 1372, Frankfurt became a "Reichsstadt" (Imperial Free City), i.e., directly subordinate to the Holy Roman Emperor and not to a regional ruler or a local nobleman.
In 1585, Frankfurt traders established a system of exchange rates for the various currencies that were circulating to prevent cheating and extortion. Therein lay the early roots for the Frankfurt Stock Exchange.
Frankfurt managed to remain neutral during the Thirty Years' War, but suffered from the bubonic plague that refugees brought to the city. After the war, Frankfurt regained its wealth. In the late 1770s the theatre principal Abel Seyler was based in Frankfurt, and established the city's theatrical life.
Following the French Revolution, Frankfurt was occupied or bombarded several times by French troops. It remained a free city until the collapse of the Holy Roman Empire in 1805/6. In 1806, it became part of the principality of Aschaffenburg under the "Fürstprimas" (Prince-Primate), Karl Theodor Anton Maria von Dalberg. This meant that Frankfurt was incorporated into the confederation of the Rhine. In 1810, Dalberg adopted the title of a Grand Duke of Frankfurt. Napoleon intended to make his adopted son Eugène de Beauharnais, already "Prince de Venise" ("prince of Venice", a newly established primogeniture in Italy), Grand Duke of Frankfurt after Dalberg's death (since the latter as a Catholic bishop had no legitimate heirs). The Grand Duchy remained a short episode lasting from 1810 to 1813, when the military tide turned in favour of the Anglo-Prussian lead allies that overturned the Napoleonic order. Dalberg abdicated in favour of Eugène de Beauharnais, which of course was only a symbolic action, as the latter effectively never ruled after the ruin of the French armies and Frankfurt's takeover by the allies.
After Napoleon's final defeat and abdication, the Congress of Vienna (1814–1815) dissolved the grand-duchy and Frankfurt became a fully sovereign city state with a republican form of government. Frankfurt entered the newly founded German Confederation (till 1866) as a free city, becoming the seat of its "Bundestag", the confederal parliament where the nominally presiding Habsburg Emperor of Austria was represented by an Austrian "presidential envoy".
After the ill-fated revolution of 1848, Frankfurt was the seat of the first democratically elected German parliament, the Frankfurt Parliament, which met in the Frankfurter Paulskirche (St. Paul's Church) and was opened on 18 May 1848. The institution failed in 1849 when the Prussian king, Frederick William IV, declared that he would not accept "a crown from the gutter". In the year of its existence, the assembly developed a common constitution for a unified Germany, with the Prussian king as its monarch.
Frankfurt lost its independence after the Austro-Prussian War in 1866 when Prussia annexed several smaller states, among them the Free City of Frankfurt. The Prussian administration incorporated Frankfurt into its province of Hesse-Nassau. The Prussian occupation and annexation was perceived as a great injustice in Frankfurt, which retained its distinct western European, urban and cosmopolitan character. The formerly independent towns of Bornheim and Bockenheim were incorporated in 1890.
In 1914, the citizens founded the University of Frankfurt, later named Goethe University Frankfurt. This marked the only civic foundation of a university in Germany; today it is one of Germany's largest.
From 6 April to 17 May 1920, following military intervention to put down the Ruhr uprising, Frankfurt was occupied by French troops. The French claimed that Articles 42 to 44 of the peace treaty of Versailles concerning the demilitarisation of the Rhineland had been broken. In 1924, Ludwig Landmann became the first Jewish mayor of the city, and led a significant expansion during the following years. During the Nazi era, the synagogues of the city were destroyed.
Frankfurt was severely bombed in World War II (1939–1945). About 5,500 residents were killed during the raids, and the once-famous medieval city centre, by that time the largest in Germany, was almost completely destroyed. It became a ground battlefield on 26 March 1945, when the Allied advance into Germany was forced to take the city in contested urban combat that included a river assault. The 5th Infantry Division and the 6th Armored Division of the United States Army captured Frankfurt after several days of intense fighting, and it was declared largely secure on 29 March 1945.
After the end of the war, Frankfurt became a part of the newly founded state of Hesse, consisting of the old Hesse-(Darmstadt) and the Prussian Hesse provinces. The city was part of the American Zone of Occupation of Germany. The Military Governor for the United States Zone (1945–1949) and the United States High Commissioner for Germany (HICOG) (1949–1952) had their headquarters in the IG Farben Building, intentionally left undamaged by the Allies' wartime bombardment.
Frankfurt was the original choice for the provisional capital city of the newly founded state of West Germany in 1949. The city constructed a parliament building that was never used for its intended purpose (it housed the radio studios of Hessischer Rundfunk). In the end, Konrad Adenauer, the first postwar Chancellor, preferred the town of Bonn, for the most part because it was close to his hometown, but also because many other prominent politicians opposed the choice of Frankfurt out of concern that Frankfurt would be accepted as the permanent capital, thereby weakening the West German population's support for a reunification with East Germany and the eventual return of the capital to Berlin.
Postwar reconstruction took place in a sometimes simple modern style, thus changing Frankfurt's architectural face. A few landmark buildings were reconstructed historically, albeit in a simplified manner (e.g., Römer, St. Paul's Church, and Goethe House). The collection of historically significant Cairo Genizah documents of the Municipal Library was destroyed by the bombing. According to Arabist and Genizah scholar S.D. Goitein, "not even handlists indicating its contents have survived."
The end of the war marked Frankfurt's comeback as Germany's leading financial centre, mainly because Berlin, now a city divided into four sectors, could no longer rival it. In 1948, the allies founded the Bank deutscher Länder, the forerunner of Deutsche Bundesbank. Following this decision, more financial institutions were re-established, e.g. Deutsche Bank and Dresdner Bank. In the 1950s, Frankfurt Stock Exchange regained its position as the country's leading stock exchange.
Frankfurt also re-emerged as Germany's transportation centre and Frankfurt Airport became Europe's second-busiest airport behind London Heathrow Airport in 1961.
During the 1970s, the city created one of Europe's most efficient underground transportation systems. That system includes a suburban rail system (S-Bahn) linking outlying communities with the city centre, and a deep underground light rail system with smaller coaches (U-Bahn) also capable of travelling above ground on rails.
In 1998, the European Central Bank was founded in Frankfurt, followed by the European Insurance and Occupational Pensions Authority and European Systemic Risk Board in 2011.
Frankfurt is the largest city in the federated state of Hesse in the south-western part of Germany.
Frankfurt is located on both sides of the Main River, south-east of the Taunus mountain range. The southern part of the city contains the Frankfurt City Forest, Germany's largest city forest. The city area is and extends over east to west and north to south. The city centre is north of the River Main in Altstadt district (the historical centre) and the surrounding Innenstadt district. The geographical centre is in Bockenheim district near Frankfurt West station.
Frankfurt is the centre of the densely populated Frankfurt Rhine-Main Metropolitan Region with a population of 5.5 million. Other important cities in the region are Wiesbaden (capital of Hesse), Mainz (capital of Rhineland-Palatinate), Darmstadt, Offenbach am Main, Hanau, Aschaffenburg, Bad Homburg vor der Höhe, Rüsselsheim, Wetzlar and Marburg.
The city is divided into 46 city districts ("Stadtteile"), which are in turn divided into 121 city boroughs ("Stadtbezirke") and 448 electoral districts ("Wahlbezirke"). The 46 city districts combine into 16 area districts ("Ortsbezirke"), which each have a district committee and chairperson.
The largest city district by population and area is Sachsenhausen, while the smallest is Altstadt, Frankfurt's historical center. Three larger city districts (Sachsenhausen, Westend and Nordend) are divided for administrative purposes into a northern ("-Nord") and a southern ("-Süd") part, respectively a western ("-West") and an eastern ("-Ost") part, but are generally considered as one city district (which is why often only 43 city districts are mentioned, even on the city's official website).
Some larger housing areas are often falsely called city districts, even by locals, like Nordweststadt (part of Niederursel, Heddernheim and Praunheim), Goldstein (part of Schwanheim), Riedberg (part of Kalbach-Riedberg) and Europaviertel (part of Gallus). The Bankenviertel ("banking district"), Frankfurt's financial district, is also not an administrative city district (it covers parts of the western Innenstadt district, the southern Westend district and the eastern Bahnhofsviertel district).
Many city districts are incorporated suburbs ("Vororte"), or were previously independent cities, such as Höchst. Some like Nordend and Westend arose during the rapid growth of the city in the Gründerzeit following the Unification of Germany, while others were formed from territory which previously belonged to other city , such as Dornbusch and Riederwald.
Until the year 1877 the city's territory consisted of the present-day inner-city districts of Altstadt, Innenstadt, Bahnhofsviertel, Gutleutviertel, Gallus, Westend, Nordend, Ostend and Sachsenhausen.
Bornheim was part of an administrative district called "Landkreis Frankfurt", before becoming part of the city on 1 January 1877, followed by Bockenheim on 1 April 1895. Seckbach, Niederrad and Oberrad followed on 1 July 1900. The "Landkreis Frankfurt" was finally dispersed on 1 April 1910, and therefore Berkersheim, Bonames, Eckenheim, Eschersheim, Ginnheim, Hausen, Heddernheim, Niederursel, Praunheim, Preungesheim and Rödelheim joined the city. In the same year a new city district, Riederwald, was created on territory that had formerly belonged to Seckbach and Ostend.
On 1 April 1928 the City of Höchst became part of Frankfurt, as well as its city districts Sindlingen, Unterliederbach and Zeilsheim. Simultaneously the "Landkreis Höchst" was dispersed with its member cities either joining Frankfurt (Fechenheim, Griesheim, Nied, Schwanheim, Sossenheim) or joining the newly established "Landkreis" of Main-Taunus-Kreis.
Dornbusch became a city district in 1946. It was created on territory that had formerly belonged to Eckenheim and Ginnheim.
On 1 August 1972 Hesse's smaller suburbs of Harheim, Kalbach, Nieder-Erlenbach, and Nieder-Eschbach became districts while other neighbouring suburbs chose to join the Main-Taunus-Kreis, the Landkreis Offenbach, the Kreis Groß-Gerau, the Hochtaunuskreis, the Main-Kinzig-Kreis or the Wetteraukreis.
Bergen-Enkheim was the last suburb to become part of Frankfurt on 1 January 1977.
Flughafen became an official city district in 1979. It covers the area of Frankfurt Airport that had belonged to Sachsenhausen and the neighbouring city of Mörfelden-Walldorf.
Frankfurt's youngest city district is Frankfurter Berg. It was part of Bonames until 1996.
Kalbach was officially renamed Kalbach-Riedberg in 2006 because of the large residential housing development in the area known as Riedberg.
To the west Frankfurt borders the administrative district ("Landkreis") of Main-Taunus-Kreis with towns such as Hattersheim am Main, Kriftel, Hofheim am Taunus, Kelkheim, Liederbach am Taunus, Sulzbach, Schwalbach am Taunus and Eschborn; to the northwest the Hochtaunuskreis with Steinbach, Oberursel (Taunus) and Bad Homburg vor der Höhe; to the north the Wetteraukreis with Karben and Bad Vilbel; to the northeast the Main-Kinzig-Kreis with Niederdorfelden and Maintal; to the southeast the city of Offenbach am Main; to the south the Kreis Offenbach with Neu-Isenburg and to the southwest the Kreis Groß-Gerau with Mörfelden-Walldorf, Rüsselsheim and Kelsterbach.
Together with these towns (and some larger nearby towns, e.g., Hanau, Rodgau, Dreieich, Langen) Frankfurt forms a contiguous built-up urban area called "Stadtregion Frankfurt" which is not an official administrative district. The urban area had an estimated population of 2.3 million in 2010 and is therefore the 13th largest urban area in the European Union.
Frankfurt has a temperate-oceanic climate (Köppen: "Cfb"). Its average annual temperature is , with monthly mean temperatures ranging from in January to in July.
With a population of 732,688 (2015) within its administrative boundaries and of 2,300,000 in the actual urban area, Frankfurt is the fifth largest city in Germany after Berlin, Hamburg, Munich and Cologne. 
Central Frankfurt has been a "Großstadt" (a city with at least 100,000 residents by definition) since 1875. With 414,576 residents in 1910, it was the ninth largest city in Germany and the number of inhabitants grew to 553,464 before World War II. After the war, at the end of the year 1945, the number had dropped to 358,000. In the following years, the population grew again and reached an all-time-high of 691,257 in 1963. It dropped again to 592,411 in 1986 but has increased since then. According to the demographic forecasts for central Frankfurt, the city will have a population up to 825,000 within its administrative boundaries in 2020 and more than 2.5 million inhabitants in its urban area.
During the 1970s, the state government of Hesse wanted to include the entire urban area into its administrative boundaries. This would have made Frankfurt officially the second largest city in Germany after Berlin with up to 3 million inhabitants. However, because local authorities did not agree the administrative territory is still much smaller than its actual urban area.
According to data from the city register of residents, 51.2% of the population had a "migration background" as of 2015, which means that a person or at least one or both of their parents was born with foreign citizenship. For the first time, a majority of the city residents had an at least part non-German background. Moreover, three of four children in the city under the age of six had immigrant backgrounds. and 27.7% of residents had a foreign citizenship.
According to statistics, 46.7% of immigrants in Frankfurt come from other countries in The European Union, 24.5% of immigrants in Frankfurt come from European countries that are not part of the EU, 15.7% of immigrants in Frankfurt come from Asia (including Western Asia and South Asia), 7.3% of immigrants in Frankfurt come from Africa, 3.4% of immigrants in Frankfurt come from North America (Including the Caribbean and Central America), 0.2% of immigrants in Frankfurt come from Australia and Zealandia, 2.3% of immigrants in Frankfurt come from South America and 1.1% of immigrants in Frankfurt come from The Pacific Island Nations. Because of this, the city is often considered to be a multicultural city and has been compared to New York City and London.
Frankfurt was historically a Protestant-dominated city. However, during the 19th century an increasing number of Catholics moved there. The Jewish community has a history dating back to Medieval times and has always ranked among the largest in Germany. Two synagogues operate there. Due to the growing immigration of people from Muslim countries beginning in the 1960s, Frankfurt has a large Muslim community. The Ahmadiyya Noor Mosque, constructed in 1959, is the city's largest mosque and the third largest in Germany.
, the largest Christian denominations were Catholicism (22.7% of the population) and Protestantism, especially Lutheranism (19.4%). Estimations put the share of Muslim inhabitants at approximately 12% (2006). According to calculations based on census data for 21 countries of origin, the number of Muslim migrants in Frankfurt amounted to about 84,000 in 2011, making up 12.6 percent of the population. A large part of them was from Turkey and Morocco. Over 7,000 inhabitants were affiliated with the Jewish community, amounting to approximately 1% of the population.
Frankfurt is one of five independent district-free cities ("kreisfreie Städte") in Hesse, which means that it does not form part of another general-purpose local government entity, in this case it is not part of a "Landkreis". The other four cities are the second to fifth largest cities in Hesse: Wiesbaden, Kassel, Darmstadt and Offenbach am Main. A "kreisfreie Stadt" has territorial sovereignty within its defined city limits.
In 1995 Petra Roth of the Christian Democratic Union (CDU) became Lord Mayor ("Oberbürgermeisterin"), Frankfurt's municipal leader. In 2012, Peter Feldmann (SPD) succeeded Roth as Lord Mayor. The CDU and the Alliance '90/The Greens ("Bündnis '90/Die Grünen") formed the government.
Frankfurt is twinned with:
Partnerships and city friendships are a weaker form of cooperation than the sister city relationship, acting more like a fixed-term cooperation or limited to certain projects. Frankfurt has partnerships with the following cities:
Römer
Römer, the German word for Roman, is a complex of nine houses that form the Frankfurt city hall ("Rathaus"). The houses were acquired by the city council in 1405 from a wealthy merchant family. The middle house became the city hall and was later connected with its neighbours. The "Kaisersaal" ("Emperor's Hall") is located on the upper floor and is where the newly crowned emperors held their banquets. The Römer was partially destroyed in World War II and later rebuilt. The surrounding square, the Römerberg, is named after the city hall.
The former Altstadt (old town) quarter between the Römer and the Frankfurt Cathedral was redeveloped as the Dom-Römer Quarter from 2012 to 2018, including 15 reconstructions of historical buildings that were destroyed during World War II.
Frankfurt Cathedral
Frankfurt Cathedral (Frankfurter Dom) is not a cathedral, but the main Catholic church, dedicated to St. Bartholomew. The Gothic building was constructed in the 14th and 15th centuries on the foundation of an earlier church from the Merovingian time. From 1356 onwards, kings of the Holy Roman Empire were elected in this church, and from 1562 to 1792, Roman-German emperors were crowned there.
Since the 18th century, St. Bartholomew's has been called "Dom", although it was never a bishop's seat. In 1867 it was destroyed by fire and rebuilt in its present style. It was again partially destroyed in World War II and rebuilt in the 1950s. Its height is 95 meters. The cathedral tower has a viewing platform open to the public at a height of 66 meters, accessed through a narrow spiral staircase with 386 steps.
St. Paul's Church
St. Paul's Church ("Paulskirche") is a national historic monument in Germany because it was the seat of the first democratically elected parliament in 1848. It was established in 1789 as a Protestant church, but was not completed until 1833. Its importance has its roots in the Frankfurt Parliament, which met in the church during the revolutionary years of 1848/49 in order to write a constitution for a united Germany. The attempt failed because the monarchs of Prussia and Austria did not want to lose power. In 1849 Prussian troops ended the democratic experiment by force and the parliament dissolved. Afterwards, the building was used for church services again.
St. Paul's was partially destroyed in World War II, particularly its interior, which now has a modern appearance. It was quickly and symbolically rebuilt after the war; today it is used mainly for exhibitions and events.
Archäologischer Garten Frankfurt
The Archaeological Garden contains small parts of the oldest recovered buildings: an ancient Roman settlement and the Frankfurt Royal Palace ("Kaiserpfalz Frankfurt") from the 6th century. The garden is located between the Römerberg and the cathedral. It was discovered after World War II when the area was heavily bombed and later partly rebuilt. The remains were preserved and are now open to the public. There are plans underway to construct a building on top of the garden but anyhow it is decided that the garden will stay open to the public.
Haus Wertheim
Wertheim House is the only timbered house in the Altstadt district that survived the heavy bombings of World War II undamaged. It is located on the Römerberg next to the Historical Museum.
Saalhof
The Saalhof is the oldest conserved building in the Altstadt district and dates to the 12th century. It was used as an exhibition hall by Dutch clothiers when trade fairs were held during the 14th and 15th century. The Saalhof was partly destroyed in World War II and later rebuilt. Today it serves as a part of the Historical Museum.
Eiserner Steg
The Eiserner Steg (Iron Bridge) is a pedestrian-only bridge across the Main that connects Römerberg and Sachsenhausen. It was built in 1868 and was the second bridge to cross the river. After World War II, when it was blown up by the Wehrmacht, it was quickly rebuilt in 1946. Today some 10,000 people cross the bridge on a daily basis.
Alte Oper
The Alte Oper is a former opera house, hence the name "Old Opera". The opera house was built in 1880 by architect Richard Lucae. It was one of the major opera houses in Germany until it was heavily damaged in World War II. Until the late 1970s, it was a ruin, nicknamed "Germany's most beautiful ruin". Former Frankfurt Lord Mayor Rudi Arndt called for blowing it up in the 1960s, which earned him the nickname "Dynamite-Rudi". (Later on, Arndt said he never had meant his suggestion seriously.)
Public pressure led to its refurbishment and reopening in 1981. Today, it functions as a famous concert hall, while operas are performed at the "new" Frankfurt Opera. The inscription on the frieze of the Alte Oper says: "Dem Wahren, Schönen, Guten" ("To the true, the beautiful, the good").
Eschenheimer Turm
The Eschenheim Tower ("Eschenheimer Turm") was erected at the beginning of the 15th century and served as a city gate as part of late-medieval fortifications. It is the oldest and most unaltered building in the Innenstadt district.
St. Catherine's Church
St. Catherine's Church ("Katharinenkirche") is the largest Protestant church, dedicated to Catherine of Alexandria, a martyred early Christian saint. It is located in the city centre at the entrance to the Zeil, the central pedestrian shopping street.
Hauptwache
Although today Hauptwache is mostly associated with the inner-city underground train station of the same name, the name originates from a baroque building on the square above the station. The Hauptwache building was constructed in 1730 and was used as a prison, therefore the name that translates as "main guard-house". Today the square surrounding the building is also called "Hauptwache" (formal: "An der Hauptwache"). It is situated in the city centre opposite to St. Catherine's Church and houses a famous café.
Central Station
Frankfurt Central Station ("Frankfurt Hauptbahnhof"), which opened in 1888, was built as the central train station for Frankfurt to replace three smaller train stations in the city centre and to boost the needed capacity for travellers. It was constructed as a terminus station and was the largest train station in Europe by floor area until 1915 when Leipzig Central Station was opened. Its three main halls were constructed in a neorenaissance-style, while the later enlargement with two outer halls in 1924 was constructed in neoclassic-style.
Frankfurter Hof
The Frankfurter Hof is a landmarked hotel in the city centre at Kaiserplatz, built from 1872 to 1876. It is part of Steigenberger Hotels group and is considered the city's most prestigious.
St. Leonhard
St. Leonhard, on the Main close to the bridge Eiserner Steg, is a Catholic late Gothic hall church, derived from a Romanesque style basilica beginning in 1425. It is the only one of nine churches in the Old Town that survived World War II almost undamaged. The parish serves the English-speaking community. The church has been under restoration since 2011.
Frankfurt is one of the few European cities with a significant number of skyscrapers, (buildings at least tall). It hosts 17 out of Germany's 18 skyscrapers. Most skyscrapers and high-rise office buildings are located in the financial district (Bankenviertel) near the city centre, around the trade fair premises (Europaviertel) and at Mainzer Landstraße between Opernplatz and Platz der Republik, which connects the two areas.
The 17 skyscrapers are:
Other high-rise buildings include:
For centuries, St. Bartholomeus's Cathedral was the tallest structure. The first building to exceed the 95-metre-high cathedral was not an office building but a grain silo, the Henninger Turm, built from 1959 to 1961.
The first high-rise building boom came in the 1970s when Westend Gate (then called "Plaza Büro Center") and Silberturm were constructed and became the tallest buildings in Germany with a height of 159.3 metres and 166.3 metres, respectively. Around the same time, Frankfurter Büro Center and City-Haus (142.4 metres and 142.1 metres) were constructed at Mainzer Landstraße and Eurotower (148.0 metres) and Garden Tower (127.0 metres; then called "Helaba-Hochhaus") were constructed in the financial district.
None of the buildings constructed during the 1980s surpassed Silberturm. The most famous buildings from this decade are the Deutsche Bank Twin Towers at Taunusanlage, both 155.0 metres tall.
The 1990s featured a second wave. Messeturm, built on the trade fair site, reached a height of 256.5 metres and became the tallest building in Europe by 1991. It was overtaken by the Commerzbank Tower in 1997. Other tall buildings from this decade are Westendstrasse 1 (208.0 metres), Main Tower (200.0 metres) and Trianon (186.0 metres).
In 21st-century Frankfurt, more high-rise buildings and skyscrapers (e.g., Skyper, Opernturm, Tower 185, Seat of the European Central Bank, Taunusturm) emerged, but none have surpassed Commerzbank Tower.
With a large forest, many parks, the Main riverbanks and the two botanical gardens, Frankfurt is considered a "green city": More than 50 percent of the area within the city limits are protected green areas.
With more than 30 museums, Frankfurt has one of the largest variety of museums in Europe. 20 museums are part of the Museumsufer, located on the front row of both sides of the Main riverbank or nearby, which was created on an initiative by cultural politician Hilmar Hoffmann.
Ten museums are located on the southern riverbank in Sachsenhausen between the Eiserner Steg and the Friedensbrücke. The street itself, Schaumainkai, is partially closed to traffic on Saturdays for Frankfurt's largest flea market.
Two museums are located on the northern riverbank:
Not directly located on the northern riverbank in the Altstadt district are:
Another important museum is located in the Westend district:
Other museums are the Dialogmuseum (Dialogue Museum) in the Ostend district, Eintracht Frankfurt Museum at Commerzbank-Arena, Explora Museum+Wissenschaft+Technik (Explora Museum of Science and Engineering) in the Nordend district, the Frankfurter Feldbahnmuseum (Light Railway Museum Frankfurt) in the Gallus district, the Verkehrsmuseum Frankfurt (Transport Museum Frankfurt) in the Schwanheim district, the Hammer Museum in the Bahnhofsviertel district and the Geldmuseum der Deutschen Bundesbank (Money Museum of the German Federal Bank) in the Ginnheim district.
Eurodance and Trance music originated in Frankfurt. In 1989 German producers Michael Münzing and Luca Anzilotti (under the pseudonyms Benito Benites and John "Virgo" Garrett III) formed the Snap! project. Snap! songs combined Rap and Soul vocals adding rhythm by using computer technology and mixing electronic sounds, bass and drums. By doing so a new genre was born: Eurodance. In the early 1990s, DJs including Sven Väth and DJ DAG (of Dance 2 Trance) first played a harder, deeper style of acid house that became popular worldwide over the next decade as Trance music. Some of the early and most influential Eurodance, Trance and Techno acts, e.g., La Bouche, Jam and Spoon, Magic Affair, Culture Beat, Snap!, Dance 2 Trance, Oliver Lieb and Hardfloor, and record labels such as Harthouse and Eye Q, were based in the city in the early 1990s.
Frankfurt is home to two major botanical gardens.
Frankfurt offers a variety of restaurants, bars, pubs and clubs. Clubs concentrate in and around the city centre and in the Ostend district, mainly close to Hanauer Landstraße. Restaurants, bars and pubs concentrate in Sachsenhausen, Nordend, Bornheim and Bockenheim.
One of the main venues of the early Trance music sound was the (closed 1998). Another popular disco club of the 1980-1990s and a hotspot for Techno/Trance music was the Dorian Gray, located within Terminal 1 at Frankfurt Airport. Because of the location at the airport the club had no restrictions regarding opening hours. The club had to close at the end of 2000 because of stricter fire safety regulations. Also notable for its extraordinary design was Coocoon Club in Fechenheim which opened in 2004 was and voted "best techno club of the year" by music magazines "Groove" and "Raveline" in 2004, 2005, 2006 and 2007. It closed in 2012.
The city can be accessed from around the world via Frankfurt Airport ("Flughafen Frankfurt am Main") located southwest of the city centre. The airport has four runways and serves 265 non-stop destinations. Run by transport company Fraport it ranks among the world's busiest airports by passenger traffic and is the busiest airport by cargo traffic in Europe. The airport also serves as a hub for Condor and as the main hub for German flag carrier Lufthansa. It is the busiest airport in Europe in terms of cargo traffic, and the fourth busiest in Europe in terms of passenger traffic behind London Heathrow Airport, Paris Charles de Gaulle Airport and Amsterdam Airport Schiphol. Passenger traffic at Frankfurt Airport in 2018 was 69,510,269 passengers.
A third terminal is being constructed (planned to open in 2023). The third terminal will increase the capacity of the airport to over 90 million passengers per year.
The airport can be reached by car or bus and has two railway stations, one for regional and one for long-distance traffic. The S-Bahn lines S8 and S9 (direction "Offenbach Ost" or "Hanau Hbf") departing at the regional station take 10–15 minutes from the airport to Frankfurt Central Station and onwards to the city centre (Hauptwache station), the IC and ICE trains departing at the long-distance station take 10 minutes to Frankfurt Central Station.
Despite the name, Frankfurt Hahn Airport ("Flughafen Frankfurt-Hahn") is situated approximately from the city in Lautzenhausen (Rhineland-Palatinate). Hahn Airport is a major base for low-cost carrier Ryanair. This airport can only be reached by car or bus. An hourly bus service runs from Frankfurt Central Station, taking just over 2 hours. Passenger traffic at Hahn Airport in 2010 was 3.5 million.
Frankfurt Egelsbach Airport ("Flugplatz Frankfurt-Egelsbach") is a busy general aviation airport located south-east of Frankfurt Airport, near Egelsbach.
Frankfurt is a traffic hub for the German motorway ("Autobahn") system. The Frankfurter Kreuz is an Autobahn interchange close to the airport, where the Bundesautobahn 3 (A3), Cologne to Würzburg, and the Bundesautobahn 5 (A5), Basel to Hanover, meet. With approximately 320,000 cars passing through it every day it is Europe's most heavily used interchange. The Bundesautobahn 66 (A66) connects Frankfurt with Wiesbaden in the west and Fulda in the east. The Bundesautobahn 661 (A661) is mainly a commuter motorway which starts in the south (Egelsbach), runs through the eastern part and ends in the north (Oberursel). The Bundesautobahn 648 (A648) is a very short motorway in the western part which primarily serves as a fast connection between the A 66 and the Frankfurt Trade Fair. The A5 in the west, the A3 in the south and the A661 in the north-east form a ring road around the inner city districts and define a Low-emission zone ("Umweltzone"; established in 2008), meaning that vehicles have to meet certain emission criteria to enter the zone.
The streets of central Frankfurt are usually congested with cars during rush hour. Some areas, especially around the shopping streets Zeil, Goethestraße and Freßgass, are pedestrian-only streets. Car parks are located throughout the city and especially in the city centre.
Frankfurt Central Station ("Frankfurt Hauptbahnhof", often abbreviated as "Frankfurt (Main) Hbf" or "F-Hbf") is the largest railway station in Germany by railway traffic. By daily passenger volume, it ranks second together with Munich Central Station (350,000 each) after Hamburg Central Station (450,000). It is located between the Gallus, the Gutleutviertel and the Bahnhofsviertel district, not far away from the trade fair and the financial district. It serves as a major hub for long-distance trains (InterCity, ICE) and regional trains as well as for Frankfurt's public transport system. It is a stop for most of ICE high speed lines, making it Germany's most important ICE station. ICE Trains to London via the Channel Tunnel were planned for 2013. All Rhine-Main S-Bahn lines, two U-Bahn lines (U4, U5), several tram and bus lines stop there. Regional and local trains are integrated in the Public transport system Rhein-Main-Verkehrsverbund (RMV), the second largest integrated public transport systems in the world, after Verkehrsverbund Berlin-Brandenburg.
Frankfurt Airport can be accessed by two railway stations: Frankfurt Airport long-distance station ("Frankfurt Flughafen Fernbahnhof") is only for long-distance traffic and connects the airport to the main rail network, with most of the ICE services using the Cologne-Frankfurt high-speed rail line. The long-distance station is located outside the actual airport ground but has a connecting bridge for pedestrians to Terminal 1, concourse B. Frankfurt Airport regional station ("Frankfurt Flughafen Regionalbahnhof") is for local S-Bahn trains (lines S8, S9) and regional trains. The regional station is located within Terminal 1, concourse B.
Frankfurt's third long-distance station is Frankfurt South station ("Frankfurt Südbahnhof", often abbreviated as "Frankfurt (Main) Süd" or "F-Süd"), located in Sachsenhausen. It is an important destination for local trains and trams (lines 14 to 16, 19) and the terminal stop for four U-Bahn lines (U1, U2, U3, U8) and four S-Bahn lines (S3, S4, S5, S6).
The Frankfurt Trade Fair offers two railway stations: Messe station is for local S-Bahn trains (lines S3-S6) and is located at the centre of the trade fair premises while Festhalle/Messe station is served by U-Bahn line U4 and is located at the north-east corner of the premises.
Two other major railway stations in the city centre are Konstablerwache and Hauptwache, located on each end of the Zeil. They are the main stations to change from east-to-west-bound S-Bahn trains to north-to-south-bound U-Bahn trains. Konstablerwache station is the second-busiest railway station regarding daily passenger volume (191,000) after the central station. The third-busiest railway station is Hauptwache station (181,000).
There are three stations for intercity bus services in Frankfurt: one at the south side of the Central Station, one at the Terminal 2 of the airport and another one at Stephanstraße.
The city has two rapid transit systems: the U-Bahn and the S-Bahn, as well as an above-ground tram system. Information about the U- and S-Bahn can be found on the website of the RMV.
Nine S-Bahn lines (S1 to S9) connect Frankfurt with the densely populated Rhine Main Region. Most routes have at least 15-minute service during the day, either by one line running every 15 minutes, or by two lines servicing one route at a 30-minute interval. All lines, except line S7, run through the Frankfurt city tunnel and serve the stations Ostendstraße, Konstablerwache, Hauptwache, Taunusanlage and Frankfurt Central Station. When leaving the city the S-Bahn travels above ground. It provides access to the trade fair (S3, S4, S5, S6), the airport (S8, S9), the stadium (S7, S8, S9) and nearby cities such as Wiesbaden, Mainz, Darmstadt, Rüsselsheim, Hanau, Offenbach am Main, Oberursel, Bad Homburg, Kronberg, Friedberg and smaller towns that are on the way.
The S8/S9 runs 24/7.
The U-Bahn has nine lines (U1 to U9) serving Frankfurt and the larger suburbs of Bad Homburg and Oberursel in the north. The trains that run on the U-Bahn are in fact light rail ("Stadtbahn") as many lines travel along a track in the middle of the street instead of underground further from the city centre. The minimum service interval is 2.5 minutes, although the usual pattern is that each line runs at 7.5 to 10-minute intervals, which produce between 3 and 5-minute intervals on city centre tracks shared by more than one line.
Frankfurt has ten tram lines (11, 12, 14 to 21), with trams arriving usually every 10 minutes. Many sections are served by two lines, combining to run at 5-minute intervals during rush-hour. Trams only run above ground and serve more stops than the U-Bahn or the S-Bahn.
A number of bus lines complete the Frankfurt public transport system. Night buses replace U-Bahn and tram services between 1:30 am and 3:30 am. The central junction for the night bus service is Konstablerwache in the city centre, where all night bus lines start and end.
Taxicabs can usually be found outside the major S-Bahn and U-Bahn stations, at the central station, the south station, the airport, the trade fair and in the crowded inner-city shopping streets. The common way to obtain a taxi is to either call a taxi operator or to go to a taxi rank. However, although not the norm, one can hail a passing taxi on the street.
Uber ceased operations in Frankfurt on 9 November 2015 after operating in the city for 18 months.
Deutsche Bahn makes bicycles available for hire through their Call a Bike service. The bicycles are stationed all over the city, including at selected railway stations. They can easily be spotted because of their eye-catching silver-red colour. To rent a specific bike, riders either call a service number to get an unlock code or reserve the bike via the smartphone application. To return the bike, the rider locks it within a designated return area (and calls the service number, if not booked via the app).
Nextbike also makes bicycles available for hire in Frankfurt. They are stationed all over the city. These can be spotted with their blue color scheme.
Cycle rickshaws (velotaxis), a type of tricycle designed to carry passengers in addition to the driver, are also available. These are allowed to operate in pedestrian-only areas and are therefore practical for sightseeing.
Frankfurt has a network of cycle routes. Many long-distance bike routes into the city have cycle tracks that are separate from motor vehicle traffic. A number of roads in the city centre are "bicycle streets" where the cyclist has the right of way and where motorised vehicles are only allowed access if they do not disrupt the cycle users. In addition, cyclists are allowed to ride many cramped one-way streets in both directions. , 15 percent of citizens used bicycles.
Frankfurt is one of the world's most important financial centres and Germany's financial capital, followed by Munich. Frankfurt was ranked 8th at the International Financial Centers Development Index (2013), 8th at the Worldwide Centres of Commerce Index (2008), 9th at the Global Financial Centres Index (September 2013), 10th at the Global Power City Index (2011), 11th at the Global City Competitiveness Index (2012), 12th at the Innovation Cities Index (2011), 14th at the World City Survey (2011) and 23rd at the Global Cities Index (2012).
The city's importance as a financial centre has risen since the eurozone crisis. Indications are the establishment of two institutions of the European System of Financial Supervisors (European Insurance and Occupational Pensions Authority and European Systemic Risk Board) in 2011 and the Single Supervisory Mechanism by which the European Central Bank was to assume responsibility for specific supervisory tasks related to the financial stability of the biggest and most important Eurozone banks.
According to an annual study by Cushman & Wakefield, the European Cities Monitor (2010), Frankfurt has been one of the top three cities for international companies in Europe, after London and Paris, since the survey started in 1990. It is the only German city considered to be an alpha world city (category 3) as listed by the Loughborough University group's 2010 inventory, which was a promotion from the group's 2008 inventory when it was ranked as an alpha minus world city (category 4).
With over 922 jobs per 1,000 inhabitants, Frankfurt has the highest concentration of jobs in Germany. On work days and Saturdays one million people commute from all over the Rhein-Main-Area.
The city is expected to benefit from international banks relocating jobs from London to Frankfurt as a result of Brexit to retain access to the EU market. Thus far, Morgan Stanley, Citigroup Inc., Standard Chartered Plc and Nomura Holdings Inc. announced they would move their EU headquarters to Frankfurt.
Frankfurt is home to two important central banks: the German Bundesbank and the European Central Bank (ECB).
The European Central Bank ("Europäische Zentralbank") is one of the world's most important central banks. The ECB sets monetary policy for the Eurozone, consisting of 19 European Union member states that have adopted the Euro (€) as their common currency. From 1998 the ECB Headquarters have been located in Frankfurt, first in the Eurotower at Willy-Brandt-Platz and in two other nearby high-rises. The new Seat of the European Central Bank in the Ostend district, consisting of the former wholesale market hall ("Großmarkthalle") and a newly built 185-metre skyscraper, was completed in late 2014. The new building complex was designed to accommodate up to 2,300 ECB personnel. The location is a few kilometres away from the city centre and borders an industrial area as well as the Osthafen ("East Harbour"), It was primarily chosen because of its large premises which allows the ECB to install security arrangements without high fences.
The city honours the importance of the ECB by officially using the slogan "The City of the Euro" since 1998.
The Deutsche Bundesbank (German Federal Bank), located in Ginnheim, was established in 1957 as the central bank for the Federal Republic of Germany. Until the euro (€) was introduced in 1999, the Deutsche Bundesbank was responsible for the monetary policy of Germany and for the German currency, the Deutsche Mark (DM). The Bundesbank was greatly respected for its control of inflation through the second half of the 20th century. Today the Bundesbank is an integral part of the European System of Central Banks (ESCB) which is formed by all 28 European Union member states.
In 2010, 63 national and 152 international banks had a registered office, including the headquarters of the major German banks, as well as 41 offices of international banks. Frankfurt is therefore known as Bankenstadt ("City of the banks") and nicknamed "Mainhattan" (a portmanteau of the local Main river and Manhattan in New York City) or "Bankfurt". 73,200 people were employed at banks in 2010.
Other major German banks include Frankfurter Volksbank, the second-largest "Volksbank" in Germany, Frankfurter Sparkasse and old-established private banks such as Bankhaus Metzler, Hauck & Aufhäuser and Delbrück Bethmann Maffei.
Many international banks have a registered or a representative office, e.g., Credit Suisse, UBS, Bank of America, Morgan Stanley, Goldman Sachs, Merrill Lynch, JPMorgan Chase & Co., Bank of China, Banco do Brasil, Itaú Unibanco Société Générale, BNP Paribas, SEB, Royal Bank of Scotland and Barclays.
The Frankfurt Stock Exchange ("Frankfurter Wertpapierbörse") began in the 9th century. By the 16th century Frankfurt had developed into an important European hub for trade fairs and financial services. Today the Frankfurt Stock Exchange is by far the largest in Germany, with a turnover of more than 90 percent of the German stock market and is the third-largest in Europe after the London Stock Exchange and the European branch of the NYSE Euronext. The most important stock market index is the DAX, the index of the 30 largest German business companies listed at the stock exchange. The stock exchange is owned and operated by Deutsche Börse, which is itself listed in the DAX. Deutsche Börse also owns the European futures exchange Eurex and clearing company Clearstream. Trading takes place exclusively via the Xetra trading system, with redundant floor brokers taking on the role of market-makers on the new platform.
On 1 February 2012 European Commission blocked the proposed merger of Deutsche Börse and NYSE Euronext. "The merger between Deutsche Börse and NYSE Euronext would have led to a near-monopoly in European financial derivatives worldwide. These markets are at the heart of the financial system and it is crucial for the whole European economy that they remain competitive. We tried to find a solution, but the remedies offered fell far short of resolving the concerns." European competition commissioner, Joaquín Almunia, said.
It is located in the city centre at the Börsenplatz. Deutsche Börse's headquarters are formally registered in Frankfurt but it moved most of its employees to a high-rise called "The Cube" in Eschborn in 2010, primarily due to significantly lower local corporate taxes.
Frankfurt Trade Fair ("Messe Frankfurt") has the third-largest exhibition site in the world with a total of 578,000 square metres (6,221,540 square feet). The trade fair premises are located in the western part between Bockenheim, the Westend and the Gallus district. It houses ten exhibition halls with a total of 321,754 square meters (3,463,331 square feet) of space and 96,078 square metres (1,034,175 square feet) of outdoor space.
Hosted in Frankfurt are the Frankfurt Motor Show ("Internationale Automobil-Ausstellung – IAA"), the world's largest auto show, the Frankfurt Book Fair ("Frankfurter Buchmesse"), the world's largest book fair, the Ambiente Frankfurt, the world's largest consumer goods fair, the Achema, the world's largest plant engineering fair, and many more like Paperworld, Christmasworld, Beautyworld, Tendence Lifestyle or Light+Building.
Messe Frankfurt GmbH, the owner and operator company, organized 87 exhibitions in 2010, 51 thereof in foreign countries. It is one of the largest trade fair companies with commercial activities in over 150 countries.
A landmark building of the trade fair (and of the whole city) is the Messeturm (the name translates as "Fair Trade Tower"), which was the tallest building in Europe from 1991 to 1997. It is located on the north-east corner of the trade fair premises at the so-called city entrance. Despite the name it is not used for exhibition but serves as an office tower.
Frankfurt Airport is one of the busiest airports in the world and is also the single largest place of work in Germany with over 500 companies which employ 71,500 people (2010).
The largest company at Frankfurt Airport is Lufthansa, Germany's flag carrier and Europe's largest airline. Lufthansa employs 35,000 people in Frankfurt. The Lufthansa Aviation Center (LAC) is the main operation base of Lufthansa at Frankfurt Airport. The airport serves as Lufthansa's primary hub with 157 worldwide destinations (compared to 110 destinations at Munich Airport, Lufthansa's second-largest hub). Lufthansa Cargo is based in Frankfurt and operates its largest cargo center (LCC) at Frankfurt Airport. Lufthansa Flight Training is also based here.
Fraport is the owner and operator of Frankfurt Airport. It is the airport's second-largest employer (19,800 workers in 2010). Fraport also operates other airports worldwide, e.g., King Abdulaziz International Airport in Jeddah, Jorge Chávez International Airport in Lima and Antalya Airport.
Condor is a German airline and part of Thomas Cook Group, based at Frankfurt Airport.
Three of the four largest international accountancy and professional services firms "(Big Four)" are present.
PricewaterhouseCoopers (PwC) German headquarters are located at Tower 185. KPMG moved its European Headquarters (KPMG Europe LLP) to The Squaire. Deloitte Touche Tohmatsu are present, while Ernst & Young is located in Eschborn.
The three major international credit rating agencies – Standard & Poor's, Moody's and Fitch Ratings – have their German headquarters in Frankfurt.
DWS Investments is the largest investment trust company in Germany and manages €288 billion fund assets. It is one of the 10 largest investment trust companies in the world. Other large investment trust companies are Allianz Global Investors Europe (a division of Allianz SE, and a top-five global active investment manager with €1,933 billion assets under management globally), Union Investment and Deka Investmentfonds.
Many of the largest international management consultancies are represented, including Arthur D. Little, McKinsey & Company, Boston Consulting Group, Booz & Company, Oliver Wyman, Bearing Point, Capgemini, Bain & Company and Roland Berger Strategy Consultants.
Located in Frankfurt are the German headquarters of Jones Lang LaSalle and BNP Paribas Real Estate.
Frankfurt has the highest concentration of lawyers in Germany, with one lawyer per 97 inhabitants (followed by Düsseldorf with a ratio of 1/117 and Munich with 1/124) in 2005.
Most of the large international law firms maintain offices, among them Allen & Overy, Baker & McKenzie, Bird & Bird, Clifford Chance, Cleary Gottlieb Steen & Hamilton, Debevoise & Plimpton, DLA Piper, Freshfields Bruckhaus Deringer, Hogan Lovells, Jones Day, Latham & Watkins, Linklaters, Mayer Brown, Milbank, Tweed, Hadley & McCloy, Norton Rose, Shearman & Sterling, Sidley Austin, SJ Berwin, Skadden, Arps, Slate, Meagher & Flom, Sullivan & Cromwell, K&L Gates, Taylor Wessing and White & Case.
Although it is best known for its banks and financial institutions, Frankfurt is also a centre for media companies. Around 570 companies of the advertising industry and 270 public relations companies are there.
According to a ranking of German FOCUS magazine (November 2007) seven of the 48 largest advertising agencies in Germany are based in Frankfurt, including Havas, Dentsu, McCann-Erickson, Saatchi & Saatchi, JWT, and Publicis.
Frankfurt is home to the German headquarters of Nestlé, the world's largest food company, located in Niederrad. Other important food companies are Ferrero SpA (German headquarters) and Radeberger Gruppe KG, the largest private brewery group in Germany.
The South-Korean automobile manufacturer Kia Motors moved its European headquarters to Frankfurt in 2007. In the same year Italian manufacturer Fiat opened its new German headquarters. The automotive supplier Continental AG has the headquarters and a major manufacturing plant of its Chassis & Safety division (formerly ITT Automotive) located in Frankfurt Rödelheim.
Some of the largest German construction companies have offices, e.g., Bilfinger Berger, Hochtief, Züblin and BAM Deutschland.
Frankfurt has Germany's highest concentration of homeowners. This is partly attributed to the financial sector, but also to its cosmopolitan nature, with expatriates and immigrants representing one fourth of its population. For this reason Frankfurt's property market often operates differently than the rest of the country where the prices are generally flatter.
Frankfurt is home to companies from the chemical, the transportation, the telecommunication and the energy industry. Some of the larger companies are:
Within Frankfurt's urban area are several important companies.
The business centre of Eschborn is located right at Frankfurt's city limits in the west and attracts businesses with significantly lower corporate taxes compared to Frankfurt. Major companies in Eschborn include Ernst & Young, Vodafone Germany, Randstad Holding and VR Leasing. Deutsche Börse moved most of its employees to Eschborn in 2010.
Rüsselsheim is internationally known for its automobile manufacturer Opel, one of the biggest automobile manufacturers in Germany. With 20,000 employees in 2003, Opel was one of the five largest employers in Hesse.
Offenbach am Main is home to the European headquarters of automobile manufacturer Hyundai Motor Company, to the German headquarters of automobile manufacturer Honda, to Honeywell Germany and to Deutscher Wetterdienst, the central scientific agency that monitors weather and meteorological conditions over Germany.
Two DAX companies are located in Bad Homburg vor der Höhe, Fresenius SE & Co. KGaA and Fresenius Medical Care. Other major companies are Hewlett-Packard, Bridgestone, Deutsche Leasing and Basler Versicherungen.
Kronberg im Taunus is home of the German headquarters of automobile manufacturer Jaguar Cars as well as the German headquarters of Accenture.
Lufthansa Systems, a subsidiary of Lufthansa, is located in Kelsterbach.
LSG Sky Chefs, another subsidiary of Lufthansa, is located in Neu-Isenburg.
The German headquarters of Thomas Cook Group are based in Oberursel.
Langen is home to Deutsche Flugsicherung, the German air traffic control.
According to a ranking list (2001) produced by the University of Liverpool, Frankfurt is the richest city in Europe by GDP per capita, followed by Karlsruhe, Paris and Munich.
Frankfurt was voted the 7th in the Mercer Quality of Living Survey by the Mercer Quality of Living Survey (2012), seventh in the Mercer Quality of Living Survey (2010) and 18th at the Economist's World's Most Liveable Cities Survey (2011). According to an annual citizen survey (2010), arranged by the city council, 66 percent inhabitants are satisfied or highly satisfied with the city, while only 6 percent said that they are dissatisfied. Compared to the 1993's survey the number of satisfied inhabitants has grown about 22 percent while the number of dissatisfied inhabitants was reduced by 8 percent. 84 percent of the inhabitants like to live in Frankfurt, 13 percent would rather choose to live somewhere else. 37 percent are satisfied with the public safety (1993: only 9 percent), 22 percent are dissatisfied (1993: 64 percent).
Frankfurt consistently has the highest levels of crime per 100,000 inhabitants in Germany (15.976 crimes per annum in 2008) and is therefore dubbed the German "crime capital". However, this statistic is often criticized because it ignores major factors: It is calculated based on the administrative 680,000-inhabitant figure while the urban area has 2.5 M inhabitants and on weekdays adds another million people (not counting the 53 million passengers passing through the airport each year). The rate for personal safety-relevant crimes such as murder, manslaughter, rape or bodily harm, is 3.4 percent, placing Frankfurt twelfth in the ranking (related to the official 680,000-inhabitant figure) or number 21 (related to the one-million-figure). In 2018, the state of Hesse where Frankfurt lies ranked the third safest state in Germany.
The European Insurance and Occupational Pensions Authority (EIOPA) is an institution of the European Union and part of the European System of Financial Supervisors that was created in response to the financial crisis of 2007–2008. It was established on 1 January 2011.
Frankfurt is one of two locations of the German Federal Financial Supervisory Authority ("Bundesanstalt für Finanzdienstleistungsaufsicht", short: "BaFin"). The BaFin is an independent federal institution and acts as Germany's financial regulatory authority.
Frankfurt is home to the German office of the International Finance Corporation (IFC), which is part of the World Bank Group. The IFC promotes sustainable private sector investment in developing countries.
Frankfurt is one of two sites of the German National Library ("Deutsche Nationalbibliothek"), the other being Leipzig. The Deutsche Nationalbibliothek is the largest universal library in Germany. Its task, unique in Germany, is to collect, permanently archive, comprehensively document and record bibliographically all German and German-language publications from 1913 on, foreign publications about Germany, translations of German works and the works of German-speaking emigrants published abroad between 1933 and 1945, and to make them available to the public.
Frankfurt is home to multiple trade unions and associations, including:
trade associations include:
Frankfurt is one of Germany's leading tourist destinations. In addition to its infrastructure and economy, its diversity supports a vibrant cultural scene. This blend of attractions led 4.3 million tourists (2012) to visit Frankfurt. The Hotels in central Frankfurt offer 34,000 beds in 228 hotels, of which 13 are luxury hotels and 46 are first-class hotels.
Besides the tourist attractions in central Frankfurt many internationally famous sites are within 80 km (50 mi) of the city, such as:
As a profoundly international city, Frankfurt hosts 92 diplomatic missions (consulates and consulates-general). Worldwide, only New York City and Hamburg are non-capital cities with more foreign representation. The Consulate General of the United States in Eckenheim is the largest American consulate in the world.
Several courts are located in Frankfurt, including:
Two important daily newspapers are published. The conservative "Frankfurter Allgemeine Zeitung", also known as "FAZ", was founded in 1949 and is the German newspaper with the widest circulation outside of Germany, with its editors claiming to deliver the newspaper to 148 countries every day. The FAZ has a circulation of over 380,000 copies daily. The other important newspaper, the "Frankfurter Rundschau", was first published in 1945 and has a daily circulation of over 181,000.
Several magazines also originate from Frankfurt. The local "Journal Frankfurt" is the best-known magazine for events, parties, and "insider tips". "Öko-Test" is a consumer-oriented magazine that focuses on ecological topics. "Titanic" is a well-known and often criticized satirical magazine with a circulation of approximately 100,000.
Frankfurt's first radio station was the Südwestdeutsche Rundfunkdienst AG (Southwest German Broadcast Service), founded in 1924. Its successor service is the public broadcaster Hessischer Rundfunk (Hessian Broadcast Service). It is located at the "Funkhaus am Dornbusch" in the Dornbusch district and is one of the most important radio and television broadcasters in Hesse, with additional studios in Kassel, Darmstadt and Fulda.
Bloomberg TV and RTL Television have regional studios.
Other radio broadcasters include Main FM and Radio X.
From August 1945 to October 2004, the American Forces Network (AFN) had broadcast from Frankfurt. Due to troop reductions the AFN's location has been closed with AFN now broadcasting from Mannheim.
Frankfurt is home to the German office of Reuters, a global news agency.
Frankfurt hosts two universities and several specialist schools. The two business schools are Goethe University Frankfurt's Goethe Business School and Frankfurt School of Finance & Management.
The oldest and best-known university is the Johann Wolfgang Goethe University, with locations in Bockenheim, Westend, and Riedberg, and the university hospital in Niederrad. Goethe Business School is part of the University's House of Finance at Campus Westend. The Business School's Full Time MBA program has over 70% international students.
The Frankfurt University of Applied Sciences was created out of several older organisations in 1971, and offers over 38 study areas, in the arts, sciences, engineering and law. Some of the most important research projects: Planet Earth Simulator, FraLine-IT-School-Service, quantitative analysis of methane in human corpses with the help of a mass spectrometer, software engineering (e.g., fraDesk), analysis of qualitative and quantitative gas in human lungs, long-term studies on photovoltaic modules (to name only a few).
The city is also home to a business school, Frankfurt School of Finance & Management, formerly known as the "" (Institution of Higher Learning for Banking Economics), with its new campus near Deutsche Nationalbibliothek U-Bahn stop (recently moving from its previous location in the Ostend (Eastend) neighbourhood). In 2001, it became a specialist institution for Economics and Management, or FOM. Frankfurt School is consistently ranked among the best business schools in the world, attributed to its high research output and quality of undergraduate and graduate training.
Frankfurt has the State Institution of Higher Learning for Artistic Education known as the Städelschule, founded in 1817 by Johann Friedrich Städel. It was taken over by the city in 1942 and turned into a state art school.
Music institutions are the Frankfurt University of Music and Performing Arts, and the Hoch Conservatory (Dr. Hoch's Konservatorium) which was founded in 1878. The International Ensemble Modern Academy is a significant institution for the study of contemporary music.
The Sankt Georgen Graduate School of Philosophy and Theology (German:"Philosophisch-Theologische Hochschule Sankt Georgen"), a private institution with membership in the German Jesuit Association, has been located in Sachsenhausen since 1950.
The city is home to three Max Planck Society institutes: the Max Planck Institute for European History of Law (MPIeR), Max Planck Institute for Biophysics, and the Max Planck Institute for Brain Research.
The Frankfurt Institute for Advanced Studies, sponsored by several institutional and private sources, is involved in theoretical research in physics, chemistry, neuroscience, and computer science.
Frankfurt is host to the "Römisch-Germanische-Kommission" (RGK), the German Archaeological Institute branch for prehistoric archaeology in Germany and Europe. The RGK is involved in a variety of research projects. Its library, with over 130,000 volumes, is one of the largest specialised archaeological libraries in the world.
Frankfurt schools rank among the best equipped schools nationwide for availability of PCs and other media facilities. In order to assure maintenance and support of the school PCs, the city in cooperation with the University of Applied Sciences launched the project Fraline – IT-Schul-Service, an initiative employing students to provide basic school IT-support.
Frankfurt is home to several professional sports teams. Some of them have won German Championships. E.g. the Skyliners Frankfurt won the German Basketball Championship in 2004 and the German Cup in 2000. Women's side 1. FFC Frankfurt are Germany's record title holders; Eintracht Frankfurt are one-time German champions, five-times winners of the DFB-Pokal, and winners of the UEFA Cup in 1980.
Frankfurt hosts the following sports teams or clubs:
Frankfurt is host to the classic cycle race Eschborn-Frankfurt City Loop (known as "Rund um den Henninger-Turm" from 1961 to 2008). The city hosts also the annual Frankfurt Marathon and the Ironman Germany. In addition to the former, it is one of 13 global host locations to the J.P. Morgan Corporate Challenge , Germany's biggest corporate sports event. Rhein-Main Eissport Club forms the base of the German bandy community.

</doc>
<doc id="10993" url="https://en.wikipedia.org/wiki?curid=10993" title="Albert of Saxony">
Albert of Saxony

Albert (; 23 April 1828 – 19 June 1902) was the King of Saxony and a member of the House of Wettin.
He was the eldest son of Prince John (who succeeded his brother Frederick Augustus II on the Saxon throne as King John in 1854) by his wife Amalie Auguste of Bavaria.
Albert had a successful military career leading Saxon troops which participated in the First Schleswig War, the Austro-Prussian War, and the Franco-Prussian War.
Albert's education, as usual with German princes, concentrated to a great extent on military matters, but he attended lectures at the University of Bonn. His first experience of warfare came in 1849, when he served as a captain in the First War of Schleswig against Denmark.
When the Austro-Prussian War broke out in 1866, Albert, then Crown Prince (German: "Kronprinz"), took up the command of the Saxon forces opposing the Prussian Army of Prince Friedrich Karl of Prussia. No attempt was made to defend Saxony, and the Saxons fell back into Bohemia and effected a junction with the Austrians. They took a prominent part in the battles by which the Prussians forced the line of the Jizera and in the Battle of Jičín. The Crown Prince, however, succeeded in effecting the retreat in good order, and in the decisive Battle of Königgrätz (3 July 1866), he held the extreme left of the Austrian position. The Saxons maintained their post with great tenacity but were involved in the disastrous defeat of their allies.
During the operations, the Crown Prince won the reputation of a thorough soldier. After peace was made and Saxony had entered the North German Confederation, he gained the command of the Saxon army, which had now become the XII army corps of the North German army, and in that position, he carried out the necessary reorganisation. He proved a firm adherent of the Prussian alliance. On the outbreak of the Franco-Prussian War in 1870, he again commanded the Saxons, who were included in the 2nd army under Prince Friedrich Karl of Prussia, his old opponent. At the Battle of Gravelotte, they formed the extreme left of the German army, and with the Prussian Guard carried out the attack on St Privat, the final and decisive action in the battle.
In the reorganisation of the army which accompanied the march towards Paris the Crown Prince gained a separate command over the 4th army (Army of the Meuse) consisting of the Saxons, the Prussian Guard corps, and the IV (Prussian Saxony) corps. He was succeeded in command of the XII corps by his brother Prince George, who had served under him in Bohemia.
Albert took a leading part in the operations which preceded the battle of Sedan, the 4th army being the pivot on which the whole army wheeled round in pursuit of MacMahon; and the actions of Buzancy and Beaumont on 29 and 30 August 1870 were fought under his direction; in the Battle of Sedan itself (1 September 1870), with the troops under his orders, Albert carried out the envelopment of the French on the east and the north.
Albert's conduct in the engagements won for him the complete confidence of the army, and during the Siege of Paris, his troops formed the north-east section of the investing force. During the siege, he blocked French attempts to break out of the encirclement at Le Bourget and Villiers. After the conclusion of the Treaty of Frankfurt (1871), he was left in command of the German army of occupation, a position which he held till the fall of the Paris Commune. On the conclusion of peace, he was made an inspector-general of the army and a field marshal.
On the death of his father, King John on 29 October 1873, the Crown Prince succeeded to the throne as King Albert. His reign proved uneventful, and he took little public part in politics, devoting himself to military affairs, in which his advice and experience were of the greatest value, not only to the Saxon corps but also to the German army in general. During his reign, the Saxon monarchy became constitutional.
In the 1870s, Albert initiated the construction of a Dresden suburb, the Albertstadt. It was then the largest garrison in Germany. Near the former suburb other buildings and places still bear his name: the Albertbrücke, the Alberthafen, the Albertplatz and the Albertinum.
In 1879, he initiated the reconstruction of the Saint Afra School in Meissen. In 1897, he was appointed arbitrator between the claimants for the Principality of Lippe.
In Dresden on 18 June 1853, Albert married Princess Carola, daughter of Gustav, Prince of Vasa and granddaughter of Gustav IV Adolf, the second to last king of Sweden of the House of Holstein-Gottorp. The marriage was childless although Carola miscarried many times. They included:
Albert died at Sibyllenort on 19 June 1902 and was succeeded by his brother, who became King George. He was buried in Dresden on 23 June, among the mourners present were both the German Emperor Wilhelm II and the Austrian Emperor Franz Joseph I.
The King of Saxony bird-of-paradise was named in Albert's honour; the Queen Carola's parotia was named for his wife.

</doc>
<doc id="10996" url="https://en.wikipedia.org/wiki?curid=10996" title="François d'Aguilon">
François d'Aguilon

François d'Aguilon (also d'Aguillon or in Latin Franciscus Aguilonius) (4 January 1567 – 20 March 1617) was a Belgian Jesuit mathematician, physicist and architect.
D'Aguilon was born in Brussels; his father was a secretary to Philip II of Spain. He became a Jesuit in Tournai in 1586. In 1598 he moved to Antwerp, where he helped plan the construction of the Saint Carolus Borromeus church. In 1611, he started a special school of mathematics in Antwerp, fulfilling a dream of Christopher Clavius for a Jesuit mathematical school; in 1616, he was joined there by Grégoire de Saint-Vincent. The notable geometers educated at this school included Jean-Charles della Faille, André Tacquet, and Theodorus Moretus.
His book, "Opticorum Libri Sex philosophis juxta ac mathematicis utiles," or "Six Books of Optics," is useful for philosophers and mathematicians. It was published by Balthasar I Moretus in Antwerp in 1613 and illustrated by the famous painter Peter Paul Rubens. It included one of the first studies of binocular vision. It also gave the names we now use to stereographic projection and orthographic projection, although the projections themselves were likely known to Hipparchus. This book inspired the works of Desargues and Christiaan Huygens.
He died in Antwerp, aged 50.
Francois d'Aguilon's "Six Books Of Optics" concerns geometrical optics, which at the time in the Jesuit school was a subcategory of geometry. He taught logic, syntax, and theology while being charged with organizing the teaching of geometry and science which would be useful for geography, navigation, architecture and the military arts in Belgium. His superiors wanted him to synthesize the work of Euclid, Alhazen, Vitello, Roger Bacon and others. Although he died before completing the book, it still consists of six in-depth books, called "Opticorum Libri Sex."
D'Aguilon extensively studied stereographic projection, which he wanted to use a means to aid architects, cosmographers, navigators and artists. For centuries, artists and architects had sought formal laws of projection to place objects on a screen. Aguilon's "Opticorum libri sex" successfully treated projections and the errors in perception. D'Aguillon adopted Alhazen's theory that only light rays orthogonal to the cornea and lens surface are clearly registered. Aguilon was the first to use the term horopter, which is the line drawn through the focal point of both eyes and parallel to the line between the eyes. In other words, it describes how only objects on the horopter are seen in their true location. He then built an instrument to measure the spacing of double images in the horopter as he saw fit.
D'Aguilon expanded on the horopter by saying in his book: 
At first glance, it seems that Aguillon discovered the geometrical horopter more than 200 years before Prevost and Vieth and Muller. The horopter was then used by architect Girard Desargues, who in 1639 published a remarkable treatise on the conic sections, emphasizing the idea of projection.
In Aguilon's book there are elements of perspectivities as well as the stereographic projections of Ptolemy and Hipparchus. Unaware that Johannes Kepler had already published optical theories years before him, Aguilon decided to share his insights on geometric optics. At the age of 20, the Dutch poet Constantijn Huygens read Aguilon's and was enthralled by it. He later said that it was the best book he had ever read in geometrical optics, and he thought that Aguilon should be compared to Plato, Eudoxus and Archimedes. In fact the title of Constantijn Huygens' first publication imitated Aguilon's title (omitting letters p and c): Otiorum Libri Sex (1625).
In Aguilon's book the beginning of each section had works of the Flemish Baroque painter, Peter Paul Rubens. The frontispiece at the beginning of the book shows an eagle, referring to Aguilon's name and a variety of optical and geometrical images. On either side of the title stands Mercury holding the head of Argus with a hundred eyes, and Minerva holding a shield reflecting the head of Medusa. Then, at the beginning of each of six sections are Rubens' drawings describing Aguilon's experiments, one of which is the first known picture of a photometer This is one of six experiments drawn by Rubens and shows how intensity of light varies with the square of distance from the source. The experiment was later taken up by Mersenne and another Jesuit, Claude de Chales, and eventually led to Bouguer's more famous photometer. It is evident, from the detail that he put into his drawings, how enthused Rubens was about the subject matter, perspective geometry and optical rules.

</doc>
<doc id="10997" url="https://en.wikipedia.org/wiki?curid=10997" title="Freenet">
Freenet

Freenet is a peer-to-peer platform for censorship-resistant communication. It uses a decentralized distributed data store to keep and deliver information, and has a suite of free software for publishing and communicating on the Web without fear of censorship.<ref name="Peers in a Client/Server World, 2005">Taylor, Ian J. "From P2P to Web Services and Grids: Peers in a Client/Server World". London: Springer, 2005.</ref> Both Freenet and some of its associated tools were originally designed by Ian Clarke, who defined Freenet's goal as providing freedom of speech on the Internet with strong anonymity protection.
The distributed data store of Freenet is used by many third-party programs and plugins to provide microblogging and media sharing, anonymous and decentralised version tracking, blogging, a generic web of trust for decentralized spam resistance, Shoeshop for using Freenet over Sneakernet, and many more.
The origin of Freenet can be traced to Ian Clarke's student project at the University of Edinburgh, which he completed as a graduation requirement in the summer of 1999. Ian Clarke's resulting unpublished report "A distributed decentralized information storage and retrieval system" (1999) provided foundation for the seminal paper written in collaboration with other researchers, "Freenet: A Distributed Anonymous Information Storage and Retrieval System" (2001). According to CiteSeer, it became one of the most frequently cited computer science articles in 2002.
Researchers suggested that Freenet can provide anonymity on the Internet by storing small encrypted snippets of content distributed on the computers of its users and connecting only through intermediate computers which pass on requests for content and sending them back without knowing the contents of the full file, similar to how routers on the Internet route packets without knowing anything about files—except Freenet has caching, a layer of strong encryption, and no reliance on centralized structures. This allows users to publish anonymously or retrieve various kinds of information.
Freenet has been under continuous development since 2000.
Freenet 0.7, released on 8 May 2008, is a major re-write incorporating a number of fundamental changes. The most fundamental change is support for darknet operation. Version 0.7 offered two modes of operation: a mode in which it connects only to friends, and an opennet-mode in which it connects to any other Freenet user. Both modes can be run simultaneously. When a user switches to pure darknet operation, Freenet becomes very difficult to detect from the outside. The transport layer created for the darknet mode allows communication over restricted routes as commonly found in mesh networks, as long as these connections follow a small-world structure. Other modifications include switching from TCP to UDP, which allows UDP hole punching along with faster transmission of messages between peers in the network.
Freenet 0.7.5, released on 12 June 2009, offers a variety of improvements over 0.7. These include reduced memory usage, faster insert and retrieval of content, significant improvements to the FProxy web interface used for browsing freesites, and a large number of smaller bugfixes, performance enhancements, and usability improvements. Version 0.7.5 also shipped with a new version of the Windows installer.
As of build 1226, released on 30 July 2009, features that have been written include significant security improvements against both attackers acting on the network and physical seizure of the computer running the node.
As of build 1468, released on 11 July 2015, the Freenet core stopped using the db4o database and laid the foundation for an efficient interface to the Web of Trust plugin which provides spam resistance.
Freenet has always been free software, but until 2011 it required users to install Java. This problem was solved by making Freenet compatible with OpenJDK, a free and open source implementation of the Java Platform.
On 11 February 2015, Freenet received the SUMA-Award for "protection against total surveillance."
Freenet served as the model for the Japanese peer to peer file-sharing programs Winny, Share and Perfect Dark, but this model differs from p2p networks such as Bittorrent and emule. Freenet separates the underlying network structure and protocol from how users interact with the network; as a result, there are a variety of ways to access content on the Freenet network. The simplest is via FProxy, which is integrated with the node software and provides a web interface to content on the network. Using FProxy, a user can browse freesites (websites that use normal HTML and related tools, but whose content is stored within Freenet rather than on a traditional web server). The web interface is also used for most configuration and node management tasks. Through the use of separate applications or plugins loaded into the node software, users can interact with the network in other ways, such as forums similar to web forums or Usenet or interfaces more similar to traditional P2P "filesharing" interfaces.
While Freenet provides an HTTP interface for browsing freesites, it is not a proxy for the World Wide Web; Freenet can be used to access only the content that has been previously inserted into the Freenet network. In this way, it is more similar to Tor's hidden services than to anonymous proxy software like Tor's proxy.
Freenet's focus lies on free speech and anonymity. Because of that, Freenet acts differently at certain points that are (directly or indirectly) related to the anonymity part. Freenet attempts to protect the anonymity of both people inserting data into the network (uploading) and those retrieving data from the network (downloading). Unlike file sharing systems, there is no need for the uploader to remain on the network after uploading a file or group of files. Instead, during the upload process, the files are broken into chunks and stored on a variety of other computers on the network. When downloading, those chunks are found and reassembled. Every node on the Freenet network contributes storage space to hold files and bandwidth that it uses to route requests from its peers.
As a direct result of the anonymity requirements, the node requesting content does not normally connect directly to the node that has it; instead, the request is routed across several intermediaries, none of which know which node made the request or which one had it. As a result, the total bandwidth required by the network to transfer a file is higher than in other systems, which can result in slower transfers, especially for infrequently accessed content.
Since version 0.7, Freenet offers two different levels of security: Opennet and Darknet. With Opennet, users connect to arbitrary other users. With Darknet, users connect only to "friends" with whom they previously exchanged public keys, named node-references. Both modes can be used together.
Freenet's founders argue that true freedom of speech comes only with true anonymity and that the beneficial uses of Freenet outweigh its negative uses. Their view is that free speech, in itself, is not in contradiction with any other consideration—the information is not the crime. Freenet attempts to remove the possibility of any group imposing its beliefs or values on any data. Although many states censor communications to different extents, they all share one commonality in that a body must decide what information to censor and what information to allow. What may be acceptable to one group of people may be considered offensive or even dangerous to another. In essence, the purpose of Freenet is to ensure that no one is allowed to decide what is acceptable.
Reports of Freenet's use in authoritarian nations is difficult to track due to the very nature of Freenet's goals. One group, "Freenet China", used to introduce the Freenet software to Chinese users starting from 2001 and distribute it within China through e-mails and on disks after the group's website was blocked by the Chinese authorities on the mainland. It was reported that in 2002 "Freenet China" had several thousand dedicated users. However, Freenet opennet traffic is blocked in China around the 2010s.
The Freenet file sharing network stores documents and allows them to be retrieved later by an associated key, as is now possible with protocols such as HTTP. The network is designed to be highly survivable. The system has no central servers and is not subject to the control of anyone, individual or organization, including the designers of Freenet. The software clock is at 192.000 lines of code. Information stored on Freenet is distributed around the network and stored on several different nodes. Encryption of data and relaying of requests makes it difficult to determine who inserted content into Freenet, who requested that content, or where the content was stored. This protects the anonymity of participants, and also makes it very difficult to censor specific content. Content is stored encrypted, making it difficult for even the operator of a node to determine what is stored on that node. This provides plausible deniability, and in combination with the request relaying means that safe harbor laws that protect service providers may also protect Freenet node operators. When asked about the topic, Freenet developers defer to the EFF discussion which says that not being able to filter anything is a safe choice.
Like Winny, Share and Perfect Dark, Freenet not only transmits data between nodes but actually stores them, working as a huge distributed cache. To achieve this, each node allocates some amount of disk space to store data; this is configurable by the node operator, but is typically several GB (or more).
Files on Freenet are typically split into multiple small blocks, with duplicate blocks created to provide redundancy. Each block is handled independently, meaning that a single file may have parts stored on many different nodes.
Information flow in Freenet is different from networks like eMule or BitTorrent; in Freenet:
Two advantages of this design are high reliability and anonymity. Information remains available even if the publisher node goes offline, and is anonymously spread over many hosting nodes as encrypted blocks, not entire files.
The key disadvantage of the storage method is that no one node is responsible for any chunk of data. If a piece of data is not retrieved for some time and a node keeps getting new data, it will drop the old data sometime when its allocated disk space is fully used. In this way Freenet tends to 'forget' data which is not retrieved regularly (see also Effect).
While users can insert data into the network, there is no way to delete data. Due to Freenet's anonymous nature the original publishing node or owner of any piece of data is unknown. The only way data can be removed is if users don't request it.
Typically, a host computer on the network runs the software that acts as a node, and it connects to other hosts running that same software to form a large distributed, variable-size network of peer nodes. Some nodes are end user nodes, from which documents are requested and presented to human users. Other nodes serve only to route data. All nodes communicate with each other identically – there are no dedicated "clients" or "servers". It is not possible for a node to rate another node except by its capacity to insert and fetch data associated with a key. This is unlike most other P2P networks where node administrators can employ a ratio system, where users have to share a certain amount of content before they can download.
Freenet may also be considered a small world network.
The Freenet protocol is intended to be used on a network of complex topology, such as the Internet (Internet Protocol). Each node knows only about some number of other nodes that it can reach directly (its conceptual "neighbors"), but any node can be a neighbor to any other; no hierarchy or other structure is intended. Each message is routed through the network by passing from neighbor to neighbor until it reaches its destination. As each node passes a message to a neighbor, it does not know whether the neighbor will forward the message to another node, or is the final destination or original source of the message. This is intended to protect the anonymity of users and publishers.
Each node maintains a data store containing documents associated with keys, and a routing table associating nodes with records of their performance in retrieving different keys.
The Freenet protocol uses a key-based routing protocol, similar to distributed hash tables. The routing algorithm changed significantly in version 0.7. Prior to version 0.7, Freenet used a heuristic routing algorithm where each node had no fixed location, and routing was based on which node had served a key closest to the key being fetched (in version 0.3) or which is estimated to serve it faster (in version 0.5). In either case, new connections were sometimes added to downstream nodes (i.e. the node that answered the request) when requests succeeded, and old nodes were discarded in least recently used order (or something close to it). Oskar Sandberg's research (during the development of version 0.7) shows that this "path folding" is critical, and that a very simple routing algorithm will suffice provided there is path folding.
The disadvantage of this is that it is very easy for an attacker to find Freenet nodes, and connect to them, because every node is continually attempting to find new connections. In version 0.7, Freenet supports both 'Opennet' (similar to the old algorithms, but simpler), and "Darknet" (all node connections are set up manually, so only your friends know your node's IP address). Darknet is less convenient, but much more secure against a distant attacker.
This change required major changes in the routing algorithm. Every node has a location, which is a number between 0 and 1. When a key is requested, first the node checks the local data store. If it's not found, the key's hash is turned into another number in the same range, and the request is routed to the node whose location is closest to the key. This goes on until some number of hops is exceeded, there are no more nodes to search, or the data is found. If the data is found, it is cached on each node along the path. So there is no one source node for a key, and attempting to find where it is currently stored will result in it being cached more widely. Essentially the same process is used to insert a document into the network: the data is routed according to the key until it runs out of hops, and if no existing document is found with the same key, it is stored on each node. If older data is found, the older data is propagated and returned to the originator, and the insert "collides".
But this works only if the locations are clustered in the right way. Freenet assumes that the Darknet (a subset of the global social network) is a small-world network, and nodes constantly attempt to swap locations (using the Metropolis–Hastings algorithm) in order to minimize their distance to their neighbors. If the network actually is a small-world network, Freenet should find data reasonably quickly; ideally on the order of formula_1 hops in Big O notation. However, it does not guarantee that data will be found at all.
Eventually, either the document is found or the hop limit is exceeded. The terminal node sends a reply that makes its way back to the originator along the route specified by the intermediate nodes' records of pending requests. The intermediate nodes may choose to cache the document along the way. Besides saving bandwidth, this also makes documents harder to censor as there is no one "source node."
Initially, the locations in Darknet are distributed randomly. This means that routing of requests is essentially random. In Opennet connections are established by a join request which provides an optimized network structure if the existing network is already optimized. So the data in a newly started Freenet will be distributed somewhat randomly.
As location swapping (on Darknet) and path folding (on Opennet) progress, nodes which are close to one another will increasingly have close locations, and nodes which are far away will have distant locations. Data with similar keys will be stored on the same node.
The result is that the network will self-organize into a distributed, clustered structure where nodes tend to hold data items that are close together in key space. There will probably be multiple such clusters throughout the network, any given document being replicated numerous times, depending on how much it is used. This is a kind of "spontaneous symmetry breaking", in which an initially symmetric state (all nodes being the same, with random initial keys for each other) leads to a highly asymmetric situation, with nodes coming to specialize in data that has closely related keys.
There are forces which tend to cause clustering (shared closeness data spreads throughout the network), and forces that tend to break up clusters (local caching of commonly used data). These forces will be different depending on how often data is used, so that seldom-used data will tend to be on just a few nodes which specialize in providing that data, and frequently used items will be spread widely throughout the network. This automatic mirroring counteracts the times when web traffic becomes overloaded, and due to a mature network's intelligent routing, a network of size "n" should require only log("n") time to retrieve a document on average.
Keys are hashes: there is no notion of semantic closeness when speaking of key closeness. Therefore, there will be no correlation between key closeness and similar popularity of data as there might be if keys did exhibit some semantic meaning, thus avoiding bottlenecks caused by popular subjects.
There are two main varieties of keys in use on Freenet, the Content Hash Key (CHK) and the Signed Subspace Key (SSK). A subtype of SSKs is the Updatable Subspace Key (USK) which adds versioning to allow secure updating of content.
A CHK is a SHA-256 hash of a document (after encryption, which itself depends on the hash of the plaintext) and thus a node can check that the document returned is correct by hashing it and checking the digest against the key. This key contains the meat of the data on Freenet. It carries all the binary data building blocks for the content to be delivered to the client for reassembly and decryption. The CHK is unique by nature and provides tamperproof content. A hostile node altering the data under a CHK will immediately be detected by the next node or the client. CHKs also reduce the redundancy of data since the same data will have the same CHK and when multiple sites reference the same large files, they can reference to the same CHK.
SSKs are based on public-key cryptography. Currently Freenet uses the DSA algorithm. Documents inserted under SSKs are signed by the inserter, and this signature can be verified by every node to ensure that the data is not tampered with. SSKs can be used to establish a verifiable pseudonymous identity on Freenet, and allow for multiple documents to be inserted securely by a single person. Files inserted with an SSK are effectively immutable, since inserting a second file with the same name can cause collisions. USKs resolve this by adding a version number to the keys which is also used for providing update notification for keys registered as bookmarks in the web interface. Another subtype of the SSK is the Keyword Signed Key, or KSK, in which the key pair is generated in a standard way from a simple human-readable string. Inserting a document using a KSK allows the document to be retrieved and decrypted if and only if the requester knows the human-readable string; this allows for more convenient (but less secure) URIs for users to refer to.
A network is said to be scalable if its performance does not deteriorate even if the network is very large. The scalability of Freenet is being evaluated, but similar architectures have been shown to scale logarithmically. This work indicates that Freenet can find data in formula_2 hops on a small-world network (which includes both opennet and darknet style Freenet networks), when ignoring the caching which could improve the scalability for popular content. However, this scalability is difficult to test without a very large network. Furthermore, the security features inherent to Freenet make detailed performance analysis (including things as simple as determining the size of the network) difficult to do accurately. As of now, the scalability of Freenet has yet to be tested.
As of version 0.7, Freenet supports both "darknet" and "opennet" connections. Opennet connections are made automatically by nodes with opennet enabled, while darknet connections are manually established between users that know and trust each other. Freenet developers describe the trust needed as “will not crack their Freenet node”. Opennet connections are easy to use, but darknet connections are more secure against attackers on the network, and can make it difficult for an attacker (such as an oppressive government) to even determine that a user is running Freenet in the first place.
The core innovation in Freenet 0.7 is to allow a globally scalable darknet, capable (at least in theory) of supporting millions of users. Previous darknets, such as WASTE, have been limited to relatively small disconnected networks. The scalability of Freenet is made possible by the fact that human relationships tend to form small-world networks, a property that can be exploited to find short paths between any two people. The work is based on a speech given at DEF CON 13 by Ian Clarke and Swedish mathematician Oskar Sandberg. Furthermore, the routing algorithm is capable of routing over a mixture of opennet and darknet connections, allowing people who have only a few friends using the network to get the performance from having sufficient connections while still receiving some of the security benefits of darknet connections. This also means that small darknets where some users also have opennet connections are fully integrated into the whole Freenet network, allowing all users access to all content, whether they run opennet, darknet, or a hybrid of the two, except for darknet pockets connected only by a single hybrid node.
Unlike many other P2P applications Freenet does not provide comprehensive functionality itself. Freenet is modular and features an API called Freenet Client Protocol (FCP) for other programs to use to implement services such as message boards, file sharing, or online chat.
Law enforcement agencies have claimed to have successfully infiltrated freenet opennet in order to deanonymize users but no technical details have been given to support these allegations. One report stated that, "A child-porn investigation focused on... [the suspect] when the authorities were monitoring the online network, Freenet." A different report indicated arrests may have been based on the BlackICE project leaks, that are debunked for using bad math.
A recent court case in the Peel Region of Ontario, Canada R. v. Owen, 2017 ONCJ 729 (CanLII), illustrated that Law Enforcement do in fact have a presence, after Peel Regional Police, located who had been downloading illegal material on the Freenet network. The court decision indicates that a Canadian Law Enforcement agency operates nodes running modified Freenet software in the hope of determining who is requesting illegal material.
Freenet has had significant publicity in the mainstream press, including articles in "The New York Times", and coverage on CNN, 60 Minutes II, the BBC, The Guardian, and elsewhere.
Freenet received the SUMA-Award 2014 for "protection against total surveillance."
A "freesite" is a site hosted on the Freenet network. Because it contains only static content, it cannot contain any active content like server side scripts or databases. Freesites are coded in HTML and support as many features as the browser viewing the page allows; however, there are some exceptions where the Freenet software will remove parts of the code that may be used to reveal the identity of the person viewing the page (making a page access something on the internet, for example).
Due to the much slower latency and bandwidth of the Freenet network, complex web technologies such as PHP and MySQL are impossible to use, making Freesites appear very simplistic, they are described by the community as being "90s-style".

</doc>
<doc id="10998" url="https://en.wikipedia.org/wiki?curid=10998" title="Fortified wine">
Fortified wine

Fortified wine is a wine to which a distilled spirit, usually brandy, has been added. In the course of some centuries,
winemakers have developed many different styles of fortified wine, including port, sherry, madeira, Marsala, Commandaria wine, and the aromatised wine vermouth.
One reason for fortifying wine was to preserve it, since ethanol is a natural antiseptic. Even though other preservation methods now exist, fortification continues to be used because the process can add distinct flavors to the finished product.
Although grape brandy is most commonly added to produce fortified wines, the additional alcohol may also be neutral spirit that has been made from grapes, grain, sugar beets or sugarcane. Regional appellation laws may dictate the types of spirit that are permitted for fortification. For example, in the U.S. only spirits from fruit may be used.
The source of the additional alcohol and the method of its distillation can affect the flavour of the fortified wine. If neutral spirit is used, it will usually have been produced with a continuous still, rather than a pot still.
When added to wine before the fermentation process is complete, the alcohol in the distilled beverage kills the yeast and leaves residual sugar behind. The end result is a wine that is both sweeter and stronger, normally containing about 20% alcohol by volume (ABV).
During the fermentation process, yeast cells in the must continue to convert sugar into alcohol until the must reaches an alcohol level of 16–18%. At this level, the alcohol becomes toxic to the yeast and kills it. If fermentation is allowed to run to completion, the resulting wine will (in most cases) be low in sugar and will be considered a dry wine. The earlier in the fermentation process that alcohol is added, the sweeter the resulting wine will be. For drier fortified wine styles, such as sherry, the alcohol is added shortly before or after the end of the fermentation.
In the case of some fortified wine styles (such as late harvest and botrytized wines), a naturally high level of sugar will inhibit the yeast. This causes fermentation to stop before the wine can become dry.
Commandaria is made in Cyprus' unique AOC region north of Limassol from high altitude vines of Mavro and Xynisteri, sun dried and aged in oak barrels. Recent developments have produced different styles of Commandaria, some of which are not fortified.
Madeira is a fortified wine made in the Madeira Islands. The wine is produced in a variety of styles ranging from dry wines which can be consumed on their own as an aperitif, to sweet wines more usually consumed with dessert. Madeira is deliberately heated and oxidised as part of its maturation process, resulting in distinctive flavours and an unusually long lifespan once a bottle is opened.
Marsala wine is a wine from Sicily that is available in both fortified and unfortified versions. It was first produced in 1772 by an English merchant, John Woodhouse, as an inexpensive substitute for sherry and port, and gets its name from the island's port, Marsala. The fortified version is blended with brandy to make two styles, the younger, slightly weaker "Fine", which is at least 17% abv and aged at least four months; and the "Superiore", which is at least 18%, and aged at least two years. The unfortified Marsala wine is aged in wooden casks for five years or more and reaches a strength of 18% by evaporation.
Mistelle (; ; Spanish, Portuguese, Galician and , from Latin / "mix") is sometimes used as an ingredient in fortified wines, particularly Vermouth, Marsala and Sherry, though it is used mainly as a base for apéritifs such as the French Pineau des Charentes. It is produced by adding alcohol to non-fermented or partially fermented grape juice (or apple juice to make pommeau). The addition of alcohol stops the fermentation and, as a consequence Mistelle is sweeter than fully fermented grape juice in which the sugars turn to alcohol.
Moscatel de Setúbal is a Portuguese wine produced around the Setúbal Municipality on the Península de Setúbal. The wine is made primarily from the Muscat of Alexandria grape and typically fortified with aguardente. The style was believed to have been invented by José Maria da Fonseca, the founder of the oldest table wine company in Portugal dating back to 1834.
Port wine (also known simply as port) is a fortified wine from the Douro Valley in the northern provinces of Portugal. It is typically a sweet red wine, but also comes in dry, semi-dry and white varieties.
Sherry is a fortified wine made from white grapes that are grown near the town of Jerez, Spain. The word "sherry" itself is an anglicisation of Jerez. In earlier times, sherry was known as "sack" (from the Spanish "saca", meaning "a removal from the solera"). In the European Union "sherry" is a protected designation of origin; therefore, all wine labelled as "sherry" must legally come from the Sherry Triangle, which is an area in the province of Cádiz between Jerez de la Frontera, Sanlúcar de Barrameda and El Puerto de Santa María.
After fermentation is complete, sherry is fortified with brandy. Because the fortification takes place after fermentation, most sherries are initially dry, with any sweetness being added later. In contrast, port wine (for example) is fortified halfway through its fermentation, which stops the process so that not all of the sugar is turned into alcohol.
Sherry is produced in a variety of styles, ranging from dry, light versions such as finos to much darker and sometimes sweeter versions known as olorosos. Cream sherry is always sweet.
Vermouth is a fortified wine flavoured with aromatic herbs and spices ("aromatised" in the trade) using closely guarded recipes (trade secrets). Some of the herbs and spices used may include cardamom, cinnamon, marjoram and chamomile. Some vermouth is sweetened; however, unsweetened or dry, vermouth tends to be bitter. The person credited with the second vermouth recipe, Antonio Benedetto Carpano from Turin, Italy, chose to name his concoction "vermouth" in 1786 because he was inspired by a German wine flavoured with wormwood, an herb most famously used in distilling absinthe. However, wine flavoured with wormwood goes back to ancient Rome. The modern German word "Wermut" ("Wermuth" in the spelling of Carpano's time) means both "wormwood" and "vermouth". The herbs were originally used to mask raw flavours of cheaper wines, imparting a slightly medicinal "tonic" flavor.
Vins doux naturels are lightly fortified wines typically made from white Muscat grapes or red Grenache grapes in the south of France. The production of vins doux naturels was perfected by Arnaud de Villeneuve at the University of Montpellier in the 13th century and they are now quite common in the Languedoc-Roussillon region of southern France.
As the name suggests, Muscat de Beaumes-de-Venise, Muscat de Rivesaltes and Muscat de Frontignan are all made from the white Muscat grape, whilst Banyuls and Maury are made from red Grenache. Regardless of the grape, fermentation is stopped by the addition of up to 10% of a 190 proof (95%) grape spirit. The Grenache vins doux naturels can be made in an oxidised or unoxidised style whereas the Muscat wines are protected from oxidation to retain their freshness.
Inexpensive fortified wines, such as Thunderbird and Wild Irish Rose, became popular during the Great Depression for their relatively high alcohol content. The term "wino" was coined during this period to describe impoverished people who drank these wines solely for their inebriating effect.
These wines continue to be associated with the homeless, mainly because marketers have been aggressive in targeting low-income communities as ideal consumers of these beverages; organisations in cities such as Los Angeles, San Francisco, Seattle and Portland have urged makers of inexpensive fortified wine, including E & J Gallo Winery, to stop providing such products to liquor stores in impoverished areas. In 2005, the Seattle City Council asked the Washington State Liquor Control Board to prohibit the sale of certain alcohol products in an impoverished "Alcohol Impact Area." Among the products sought to be banned were over two dozen beers, and six fortified wines: Cisco, Gino's Premium Blend, MD 20/20, Night Train, Thunderbird, and Wild Irish Rose. The Liquor Control Board approved these restrictions on August 30, 2006.
"Gwaha-ju" is a fortified rice wine made in Korea.
Although rice wine is not made from grapes, it has a similar alcohol content to grape wine, and the addition of the distilled spirit, soju, and other ingredients like ginseng, jujubes, ginger, etc., to the rice wine, bears similarity to the above-mentioned fortified wines.
Fortified wines are often termed dessert wines in the United States to avoid association with hard drinking. The term "vins de liqueur" is used by the French.
Under European Union legislation, a liqueur wine is a fortified wine that contains 15–22% abv, with Total Alcoholic Strength no less than 17.5%, and that meets many additional criteria. Exemptions are allowed for certain quality liqueur wines.

</doc>
<doc id="11001" url="https://en.wikipedia.org/wiki?curid=11001" title="Fred Hoyle">
Fred Hoyle

Sir Fred Hoyle FRS (24 June 1915 – 20 August 2001) was an English astronomer who formulated the theory of stellar nucleosynthesis. He also held controversial stances on other scientific matters—in particular his rejection of the "Big Bang" theory, a term coined by him on BBC radio, and his promotion of panspermia as the origin of life on Earth. He also wrote science fiction novels, short stories and radio plays, and co-authored twelve books with his son, Geoffrey Hoyle. He spent most of his working life at the Institute of Astronomy at Cambridge and served as its director for six years.
Hoyle was born near Bingley in Gilstead, West Riding of Yorkshire, England. His father, Ben Hoyle, who was a violinist and worked in the wool trade in Bradford, served as a machine gunner in the First World War. His mother, Mabel Pickard, had studied music at the Royal College of Music in London and later worked as a cinema pianist. Hoyle was educated at Bingley Grammar School and read mathematics at Emmanuel College, Cambridge.
In 1936 he won the Mayhew Prize (jointly with George Stanley Rushbrooke).
In late 1940, Hoyle left Cambridge to go to Portsmouth to work for the Admiralty on radar research, for example devising a method to get the altitude of the incoming aeroplanes. He was also put in charge of countermeasures against the radar guided guns found on the "Graf Spee". Britain's radar project employed more personnel than the Manhattan project, and was probably the inspiration for the large British project in "The Black Cloud". Two key colleagues in this war work were Hermann Bondi and Thomas Gold, and the three had many and deep discussions on cosmology. The radar work paid for a couple of trips to North America, where he took the opportunity to visit astronomers. On one trip to the US he learned about supernovae at Caltech and Mount Palomar and, in Canada, the nuclear physics of plutonium implosion and explosion, noticed some similarity between the two and started thinking about supernova nucleosynthesis. He had an intuition at the time "I will make a name for myself if this works out." Eventually (1954) his prescient and ground breaking paper came out. He also formed a group at Cambridge exploring Stellar nucleosynthesis in ordinary stars and was bothered by the paucity of stellar carbon production in existing models. He noticed that one of the existing processes would be made a billion times more productive if the carbon-12 nucleus had a resonance at 7.7 MeV, but the nuclear physicists did not list such a one. On another trip he visited the nuclear physics group at Caltech, spending a few months of sabbatical there and persuaded them against their considerable scepticism to look for and find the Hoyle state in carbon-12, from which developed a full theory of stellar nucleosynthesis, co-authored by Hoyle with some members of the Caltech group.
After the war, in 1945, Hoyle returned to Cambridge University, starting as a lecturer at St John's College, Cambridge. Hoyle's Cambridge years, 1945–1973, saw him rise to the top of world astrophysics theory, on the basis of a startling originality of ideas covering a very wide range of topics. In 1958, Hoyle was appointed to the illustrious Plumian Professor of Astronomy and Experimental Philosophy at Cambridge University. In 1967, he became the founding director of the Institute of Theoretical Astronomy (subsequently renamed the Institute of Astronomy, Cambridge, where Hoyle's innovative leadership quickly led to this institution becoming one of the premier groups in the world for theoretical astrophysics. In 1971 he was invited to deliver the MacMillan Memorial Lecture to the Institution of Engineers and Shipbuilders in Scotland. He chose the subject "Astronomical Instruments and their Construction". Hoyle was knighted in 1972. Hoyle resigned his Plumian professor position in 1972 and his directorship of the institute in 1973, with this move effectively cutting him off from most of his establishment power-base, connections and steady salary.
After his leaving Cambridge, Hoyle wrote many popular science and science fiction books, as well as presenting lectures around the world. Part of the motivation for this was simply to provide a means of support. Hoyle was still a member of the joint policy committee (since 1967), during the planning stage for the 150-inch Anglo-Australian Telescope at Siding Spring Observatory in New South Wales. He became chairman of the Anglo-Australian Telescope board in 1973, and presided at its inauguration in 1974 by Charles, Prince of Wales. After his resignation from Cambridge, Hoyle moved to the Lake District and occupied his time with a mix of treks across the moors, writing books, visiting research centres around the world, and working on science ideas that have been nearly-universally rejected. On 24 November 1997, while hiking across moorlands in west Yorkshire, near his childhood home in Gilstead, Hoyle fell down into a steep ravine called Shipley Glen. Roughly twelve hours later, Hoyle was found by a search dog. He was hospitalised for two months with pneumonia and kidney problems (both resulting from hypothermia), as well as a broken shoulder from the fall. Thereafter he went into marked decline, suffering from memory and mental agility problems. In 2001, he suffered a series of strokes and died in Bournemouth on 20 August of that year.
Hoyle authored the first two research papers ever published on the synthesis of the chemical elements heavier than helium by nuclear reactions in stars. The first of these in 1946 showed that the cores of stars will evolve to temperatures of billions of degrees, much hotter than temperatures considered for thermonuclear origin of stellar power in main sequence stars. Hoyle showed that at such high temperatures the element iron can become much more abundant than other heavy elements owing to thermal equilibrium among nuclear particles, explaining the high natural abundance of iron. This idea would later be called the e Process. Hoyle's second foundational nucleosynthesis publication showed that the elements between carbon and iron cannot be synthesized by such equilibrium processes. He attributed those elements to specific nuclear fusion reactions between abundant constituents in concentric shells of evolved massive, pre-supernova stars. This startlingly modern picture is the accepted paradigm today for the supernova nucleosynthesis of these primary elements. In the mid 1950s, Hoyle became the leader of a group of very talented experimental and theoretical physicists who met in Cambridge: William Alfred Fowler, Margaret Burbidge, and Geoffrey Burbidge. This group systematized basic ideas of how all the chemical elements in our universe were created, with this now being a field called nucleosynthesis. Famously, in 1957, this group produced the BFH paper (known for the initials of the four authors) in which the field of nucleosynthesis was organized into complementary nuclear processes. They also added much new material on the synthesis of heavy elements by neutron-capture reactions, the so-called s process and the r process. So influential did the BFH paper become that for the remainder of the twentieth century it became the default citation of almost all researchers wishing to cite an accepted origin for nucleosynthesis theory, and as a result the path breaking Hoyle 1954 paper fell into obscurity. Historical research in the 21st century has brought Hoyle's 1954 paper back to scientific prominence. Those historical arguments were first presented to a gathering of nucleosynthesis experts attending a 2007 conference at Caltech organized after the deaths of both Fowler and Hoyle to celebrate the 50th anniversary of the publication of BFH. Ironically the BFH paper did not review Hoyle's 1954 supernova-shells attribution of the origin of elements between silicon and iron despite Hoyle's co-authorship of BFH. Based on his many personal discussions with Hoyle Donald D. Clayton has attributed this seemingly inexplicable oversight in BFH to the lack of proofreading by Hoyle of the draft composed at Caltech in 1956 by G.R. Burbidge and E.M. Burbidge.
The second of Hoyle's nucleosynthesis papers also introduced an interesting use of the anthropic principle, which was not then known by that name. In trying to work out the routes of stellar nucleosynthesis, Hoyle calculated that one particular nuclear reaction, the triple-alpha process, which generates carbon from helium, would require the carbon nucleus to have a very specific resonance energy and spin for it to work. The large amount of carbon in the universe, which makes it possible for carbon-based life-forms of any kind to exist, demonstrated to Hoyle that this nuclear reaction must work. Based on this notion, Hoyle therefore predicted the values of the energy, the nuclear spin and the parity of the compound state in the carbon nucleus formed by three alpha particles (helium nuclei), which was later borne out by experiment.
This energy level, while needed to produce carbon in large quantities, was statistically very unlikely to fall where it does in the scheme of carbon energy levels. Hoyle later wrote:
His co-worker William Alfred Fowler eventually won the Nobel Prize for Physics in 1983 (with Subrahmanyan Chandrasekhar), but for some reason Hoyle's original contribution was overlooked by the electors, and many were surprised that such a notable astronomer missed out. Fowler himself in an autobiographical sketch affirmed Hoyle's pioneering efforts:
While having no argument with the Lemaître theory (later confirmed by Edwin Hubble's observations) that the universe was expanding, Hoyle disagreed on its interpretation. He found the idea that the universe had a beginning to be pseudoscience, resembling arguments for a creator, "for it's an irrational process, and can't be described in scientific terms" (see Kalam cosmological argument). Instead, Hoyle, along with Thomas Gold and Hermann Bondi (with whom he had worked on radar in the Second World War), in 1948 began to argue for the universe as being in a "steady state" and formulated their Steady State theory. The theory tried to explain how the universe could be eternal and essentially unchanging while still having the galaxies we observe moving away from each other. The theory hinged on the creation of matter between galaxies over time, so that even though galaxies get further apart, new ones that develop between them fill the space they leave. The resulting universe is in a "steady state" in the same manner that a flowing river is—the individual water molecules are moving away but the overall river remains the same.
The theory was one alternative to the Big Bang which, like the Big Bang, agreed with key observations of the day, namely Hubble's red shift observations, and Hoyle was a strong critic of the Big Bang. He coined the term "Big Bang" on BBC radio's "Third Programme" broadcast on 28 March 1949. It was popularly reported by George Gamov and his opponents that Hoyle intended to be pejorative, and the script from which he read aloud was interpreted by his opponents to be "vain, one-sided, insulting, not worthy of the BBC". Hoyle explicitly denied that he was being insulting and said it was just a striking image meant to emphasize the difference between the two theories for the radio audience. In another BBC interview he said "The reason why scientists like the "big bang" is because they are overshadowed by the Book of Genesis. It is deep within the psyche of most scientists to believe in the first page of Genesis".
Hoyle had a famously heated argument with Martin Ryle of the Cavendish Radio Astronomy Group about Hoyle's steady state theory, which somewhat restricted collaboration between the Cavendish group and the Cambridge Institute of Astronomy during the 1960s.
Hoyle, unlike Gold and Bondi, offered an explanation for the appearance of new matter by postulating the existence of what he dubbed the "creation field", or just the "C-field", which had negative pressure in order to be consistent with the conservation of energy and drive the expansion of the universe. This C-field is the same as the later "de Sitter solution" for cosmic inflation, but the C-field model acts much slower than the de Sitter inflation model. They jointly argued that continuous creation was no more inexplicable than the appearance of the entire universe from nothing, although it had to be done on a regular basis. In the end, mounting observational evidence convinced most cosmologists that the steady state model was incorrect and that the Big Bang was the theory that agreed better with observations, although Hoyle continued to support and develop his theory. In 1993, in an attempt to explain some of the evidence against the steady state theory, he presented a modified version called "quasi-steady state cosmology" (QSS), but the theory is not widely accepted.
The evidence that resulted in the Big Bang's victory over the steady state model included the discovery of the cosmic microwave background radiation in the 1960s, and the distribution of "young galaxies" and quasars throughout the Universe in the 1980s indicate a more consistent age estimate of the universe. Hoyle died in 2001 never accepting the Big Bang theory.
Together with Narlikar, Hoyle developed a particle theory in the 1960s, the Hoyle–Narlikar theory of gravity. It made predictions that were roughly the same as Einstein's general relativity, but it incorporated Mach's Principle, which Einstein had tried but failed to incorporate in his theory. The Hoyle-Narlikar theory fails several tests, including consistency with the microwave background. It was motivated by their belief in the steady state model of the universe.
In his later years, Hoyle became a staunch critic of theories of abiogenesis to explain the origin of life on Earth. With Chandra Wickramasinghe, Hoyle promoted the hypothesis that the first life on Earth began in space, spreading through the universe via panspermia, and that evolution on Earth is influenced by a steady influx of viruses arriving via comets. His belief that comets had a significant percentage of organic compounds was well ahead of his time, as the dominant views in the 1970s and 1980s were that comets largely consisted of water-ice, and the presence of organic compounds was then highly controversial. Wickramasinghe wrote in 2003: "In the highly polarized polemic between Darwinism and creationism, our position is unique. Although we do not align ourselves with either side, both sides treat us as opponents. Thus we are outsiders with an unusual perspective—and our suggestion for a way out of the crisis has not yet been considered."
Hoyle and Wickramasinghe advanced several instances where they say outbreaks of illnesses on Earth are of extraterrestrial origins, including the 1918 flu pandemic, and certain outbreaks of polio and mad cow disease. For the 1918 flu pandemic, they hypothesized that cometary dust brought the virus to Earth simultaneously at multiple locations—a view almost universally dismissed by experts on this pandemic. In 1982 Hoyle presented "Evolution from Space" for the Royal Institution's Omni Lecture. After considering what he thought of as a very remote possibility of Earth-based abiogenesis he concluded:
Published in his 1982/1984 books "Evolution from Space" (co-authored with Chandra Wickramasinghe), Hoyle calculated that the chance of obtaining the required set of enzymes for even the simplest living cell without panspermia was one in 10. Since the number of atoms in the known universe is infinitesimally tiny by comparison (10), he argued that Earth as life's place of origin could be ruled out. He claimed:
Though Hoyle declared himself an atheist, this apparent suggestion of a guiding hand led him to the conclusion that "a superintellect has monkeyed with physics, as well as with chemistry and biology, and ... there are no blind forces worth speaking about in nature." He would go on to compare the random emergence of even the simplest cell without panspermia to the likelihood that "a tornado sweeping through a junk-yard might assemble a Boeing 747 from the materials therein" and to compare the chance of obtaining even a single functioning protein by chance combination of amino acids to a solar system full of blind men solving Rubik's Cubes simultaneously. Those who advocate the intelligent design (ID) belief sometimes cite Hoyle's work in this area to support the claim that the universe was fine tuned in order to allow intelligent life to be possible.
While Hoyle was well-regarded for his works on nucleosynthesis and science popularization, he held controversial positions on a wide range of scientific issues, often in direct opposition to the prevailing theories of the scientific community. Paul Davies describes how he "loved his maverick personality and contempt for orthodoxy", quoting Hoyle as saying "I don't care what they think" about his theories on discrepant redshift, and "it is better to be interesting and wrong than boring and right".
Hoyle often expressed anger against the labyrinthine and petty politics at Cambridge and frequently feuded with members and institutions of all levels of the British astronomy community, leading to his resignation from Cambridge in September 1971 over the way he thought Donald Lynden-Bell was chosen to replace retiring professor Roderick Oliver Redman behind his back. According to biographer Simon Mitton, Hoyle was crestfallen because he felt that his colleagues at Cambridge were unsupportive.
In addition to his views on steady state theory and panspermia, Hoyle also supported the following controversial hypotheses and speculations:
Hoyle was also at the centre of two unrelated controversies involving the politics for selecting the winner of the Nobel Prize for Physics. The first came when the 1974 prize went, in part, to Antony Hewish for his leading role in the discovery of pulsars. Promptly Hoyle made an off-the-cuff remark to a reporter in Montreal that "Yes, Jocelyn Bell was the actual discoverer, not Hewish, who was her supervisor, so she should have been included." This remark received widespread international coverage. Worried about being misunderstood and by British libel laws, Hoyle carefully composed a letter of explanation to "The Times".
The second controversy came when the 1983 prize went in part to William Alfred Fowler "for his theoretical and experimental studies of the nuclear reactions of importance in the formation of the chemical elements in the universe." The controversy arose because Hoyle had been the inventor of the theory of nucleosynthesis in the stars with two research papers published shortly after WWII. So some suspicion arose that Hoyle was denied the third share of this prize because of his earlier public disagreement with the 1974 award. British scientist Harry Kroto later said that the Nobel Prize is not just an award for a piece of work, but a recognition of a scientist's overall reputation and Hoyle's championing many disreputable and disproven ideas may have invalidated him. In "Nature", editor John Maddox called it "shameful" that Fowler had been rewarded with a Nobel prize and Hoyle had not.
Hoyle appeared in a series of radio talks on astronomy for the BBC in the 1950s; these were collected in the book "The Nature of the Universe", and he went on to write a number of other popular science books.
In the play "Sur la route de Montalcino", the character of Fred Hoyle confronts Georges Lemaître on a fictional journey to the Vatican in 1957.
Hoyle also appeared in the 1973 short film "Take the World From Another Point of View".
In the 2004 television movie "Hawking", Fred Hoyle is played by Peter Firth. In the movie, Stephen Hawking (played by Benedict Cumberbatch) publicly confronts Hoyle at a Royal Society lecture in summer 1964, about a mistake he found in his latest publication.
Awards
Named after him
The Fred Hoyle Collection at St John's College Library contains "a pair of walking boots, five boxes of photographs, two ice axes, some dental X-rays, a telescope, ten large film reels and an unpublished opera" in addition to 150 document boxes of papers.
Hoyle also wrote science fiction. In his first novel, "The Black Cloud", most intelligent life in the universe takes the form of interstellar gas clouds; they are surprised to learn that intelligent life can also form on planets. He wrote a television series, "A for Andromeda", which was also published as a novel. His play "Rockets in Ursa Major" had a professional production at the Mermaid Theatre in 1962.
Most of these are independent of each other. "Andromeda Breakthrough" is a sequel to "A for Andromeda" and "Into Deepest Space" is a sequel to "Rockets in Ursa Major". The four Ladybird Books are intended for children.
Some stories of the anthology "Element 79" are fantasy, in particular "Welcome to Slippage City" and "The Judgement of Aphrodite". Both introduce mythological characters.
"The Telegraph" (UK) called him a "masterful" science fiction writer.

</doc>
<doc id="11002" url="https://en.wikipedia.org/wiki?curid=11002" title="French cuisine">
French cuisine

French cuisine consists of the cooking traditions and practices from France.
French cuisine developed throughout the centuries influenced by the many surrounding cultures of Spain, Italy, Switzerland, Germany and Belgium, in addition to its own food traditions on the long western coastlines of the Atlantic, the Channel and of course inland. In the 14th century, Guillaume Tirel, a court chef known as "Taillevent", wrote "Le Viandier", one of the earliest recipe collections of medieval France. In the 17th century, chefs François Pierre La Varenne and Marie-Antoine Carême spearheaded movements that shifted French cooking away from its foreign influences and developed France's own indigenous style. Cheese and wine are a major part of the cuisine. They play different roles regionally and nationally, with many variations and "appellation d'origine contrôlée" (AOC) (regulated appellation) laws.
French cuisine was made important in the 20th century by Auguste Escoffier to become the modern "haute cuisine"; Escoffier, however, left out much of the local culinary character to be found in the regions of France and was considered difficult to execute by home cooks. Culinary tourism and the "Guide Michelin" helped to acquaint people with the "cuisine bourgeoise" of the urban elites and the peasant cuisine of the French countryside starting in the 20th century. Gascon cuisine has also had great influence over the cuisine in the southwest of France. Many dishes that were once regional have proliferated in variations across the country.
Knowledge of French cooking has contributed significantly to Western cuisines. Its criteria are used widely in Western cookery school boards and culinary education. In November 2010, French gastronomy was added by the UNESCO to its lists of the world's "intangible cultural heritage".
In French medieval cuisine, banquets were common among the aristocracy. Multiple courses would be prepared, but served in a style called "service en confusion", or all at once. Food was generally eaten by hand, meats being sliced off in large pieces held between the thumb and two fingers. The sauces were highly seasoned and thick, and heavily flavored mustards were used. Pies were a common banquet item, with the crust serving primarily as a container, rather than as food itself, and it was not until the very end of the Late Middle Ages that the shortcrust pie was developed. Meals often ended with an "issue de table", which later changed into the modern dessert, and typically consisted of "dragées" (in the Middle Ages, meaning spiced lumps of hardened sugar or honey), aged cheese and spiced wine, such as hypocras.
The ingredients of the time varied greatly according to the seasons and the church calendar, and many items were preserved with salt, spices, honey, and other preservatives. Late spring, summer, and autumn afforded abundance, while winter meals were more sparse. Livestock were slaughtered at the beginning of winter. Beef was often salted, while pork was salted and smoked. Bacon and sausages would be smoked in the chimney, while the tongue and hams were brined and dried. Cucumbers were brined as well, while greens would be packed in jars with salt. Fruits, nuts and root vegetables would be boiled in honey for preservation. Whale, dolphin and porpoise were considered fish, so during Lent, the salted meats of these sea mammals were eaten.
Artificial freshwater ponds (often called "stews") held carp, pike, tench, bream, eel, and other fish. Poultry was kept in special yards, with pigeon and squab being reserved for the elite. Game was highly prized, but very rare, and included venison, wild boar, hare, rabbit, and birds. Kitchen gardens provided herbs, including some, such as tansy, rue, pennyroyal, and hyssop, which are rarely used today. Spices were treasured and very expensive at that time – they included pepper, cinnamon, cloves, nutmeg, and mace. Some spices used then, but no longer today in French cuisine are cubebs, long pepper (both from vines similar to black pepper), grains of paradise, and galengale. Sweet-sour flavors were commonly added to dishes with vinegars and "verjus" combined with sugar (for the affluent) or honey. A common form of food preparation was to finely cook, pound and strain mixtures into fine pastes and mushes, something believed to be beneficial to make use of nutrients.
Visual display was prized. Brilliant colors were obtained by the addition of, for example, juices from spinach and the green part of leeks. Yellow came from saffron or egg yolk, while red came from sunflower, and purple came from "Crozophora tinctoria" or "Heliotropium europaeum". Gold and silver leaf were placed on food surfaces and brushed with egg whites. Elaborate and showy dishes were the result, such as "tourte parmerienne" which was a pastry dish made to look like a castle with chicken-drumstick turrets coated with gold leaf. One of the grandest showpieces of the time was roast swan or peacock sewn back into its skin with feathers intact, the feet and beak being gilded. Since both birds are stringy, and taste unpleasant, the skin and feathers could be kept and filled with the cooked, minced and seasoned flesh of tastier birds, like goose or chicken.
The most well known French chef of the Middle Ages was Guillaume Tirel, also known as Taillevent. Taillevent worked in numerous royal kitchens during the 14th century. His first position was as a kitchen boy in 1326. He was chef to Philip VI, then the Dauphin who was son of John II. The Dauphin became King Charles V of France in 1364, with Taillevent as his chief cook. His career spanned sixty-six years, and upon his death he was buried in grand style between his two wives. His tombstone represents him in armor, holding a shield with three cooking pots, "marmites", on it.
Paris was the central hub of culture and economic activity, and as such, the most highly skilled culinary craftsmen were to be found there. Markets in Paris such as "Les Halles", "la Mégisserie", those found along "Rue Mouffetard", and similar smaller versions in other cities were very important to the distribution of food. Those that gave French produce its characteristic identity were regulated by the guild system, which developed in the Middle Ages. In Paris, the guilds were regulated by city government as well as by the French crown. A guild restricted those in a given branch of the culinary industry to operate only within that field.
There were two groups of guilds – first, those that supplied the raw materials; butchers, fishmongers, grain merchants, and gardeners. The second group were those that supplied prepared foods; bakers, pastry cooks, sauce makers, poulterers, and caterers. There were also guilds that offered both raw materials and prepared food, such as the "charcutiers" and "rôtisseurs" (purveyors of roasted meat dishes). They would supply cooked meat pies and dishes as well as raw meat and poultry. This caused issues with butchers and poulterers, who sold the same raw materials. The guilds served as a training ground for those within the industry. The degrees of assistant-cook, full-fledged cook and master chef were conferred. Those who reached the level of master chef were of considerable rank in their individual industry, and enjoyed a high level of income as well as economic and job security. At times, those in the royal kitchens did fall under the guild hierarchy, but it was necessary to find them a parallel appointment based on their skills after leaving the service of the royal kitchens. This was not uncommon as the Paris cooks' guild regulations allowed for this movement.
During the 16th and 17th centuries, French cuisine assimilated many new food items from the New World. Although they were slow to be adopted, records of banquets show Catherine de' Medici (1519–1589?) serving sixty-six turkeys at one dinner. The dish called cassoulet has its roots in the New World discovery of haricot beans, which are central to the dish's creation, but had not existed outside of the New World until its exploration by Christopher Columbus.
"Haute cuisine" (, "high cuisine") has foundations during the 17th century with a chef named La Varenne. As author of works such as "Le Cuisinier françois", he is credited with publishing the first true French cookbook. His book includes the earliest known reference to roux using pork fat. The book contained two sections, one for meat days, and one for fasting. His recipes marked a change from the style of cookery known in the Middle Ages, to new techniques aimed at creating somewhat lighter dishes, and more modest presentations of pies as individual pastries and turnovers. La Varenne also published a book on pastry in 1667 entitled "Le Parfait confitvrier" (republished as "Le Confiturier françois") which similarly updated and codified the emerging "haute cuisine" standards for desserts and pastries.
Chef François Massialot wrote "Le Cuisinier roïal et bourgeois" in 1691, during the reign of Louis XIV. The book contains menus served to the royal courts in 1690. Massialot worked mostly as a freelance cook, and was not employed by any particular household. Massialot and many other royal cooks received special privileges by association with the French royalty. They were not subject to the regulation of the guilds; therefore, they could cater weddings and banquets without restriction. His book is the first to list recipes alphabetically, perhaps a forerunner of the first culinary dictionary. It is in this book that a marinade is first seen in print, with one type for poultry and feathered game, while a second is for fish and shellfish. No quantities are listed in the recipes, which suggests that Massialot was writing for trained cooks.
The successive updates of "Le Cuisinier roïal et bourgeois" include important refinements such as adding a glass of wine to fish stock. Definitions were also added to the 1703 edition. The 1712 edition, retitled "Le Nouveau cuisinier royal et bourgeois", was increased to two volumes, and was written in a more elaborate style with extensive explanations of technique. Additional smaller preparations are included in this edition as well, leading to lighter preparations, and adding a third course to the meal. Ragout, a stew still central to French cookery, makes its first appearance as a single dish in this edition as well; prior to that, it was listed as a garnish.
Shortly before the French Revolution, dishes like bouchées à la Reine gained prominence. Essentially royal cuisine produced by the royal household, this is a chicken-based recipe served on vol-au-vent created under the influence of Queen Marie Leszczyńska, the Polish-born wife of Louis XV. This recipe is still popular today, as are other recipes from Queen Marie Leszczyńska like consommé à la Reine and filet d'aloyau braisé à la royale. Queen Marie is also credited with introducing lentils to the French diet and Polonaise garnishing.
The French Revolution was integral to the expansion of French cuisine, because it abolished the guild system. This meant anyone could now produce and sell any culinary item he wished. Bread was a significant food source among peasants and the working class in the late 18th century, with many of the nation's people being dependent on it. In French provinces, bread was often consumed three times a day by the people of France. According to Brace, bread was referred to as the basic dietary item for the masses, and it was also used as a foundation for soup. In fact, bread was so important that harvest, interruption of commerce by wars, heavy flour exploration, and prices and supply were all watched and controlled by the French Government. Among the underprivileged, constant fear of famine was always prevalent. From 1725 to 1789, there was fourteen years of bad yields to blame for low grain supply. In Bordeaux, during 1708–1789, thirty-three bad harvests occurred.
Marie-Antoine Carême was born in 1784, five years before the Revolution. He spent his younger years working at a "pâtisserie" until he was discovered by Charles Maurice de Talleyrand-Périgord, who would later cook for Napoleon Bonaparte. Prior to his employment with Talleyrand, Carême had become known for his "pièces montées", which were extravagant constructions of pastry and sugar architecture.
More important to Carême's career was his contribution to the refinement of French cuisine. The basis for his style of cooking was his sauces, which he named mother sauces. Often referred to as fonds, meaning "foundations", these base sauces, "espagnole", "velouté", and "béchamel", are still known today. Each of these sauces was made in large quantities in his kitchen, then formed the basis of multiple derivatives. Carême had over one hundred sauces in his repertoire. In his writings, soufflés appear for the first time. Although many of his preparations today seem extravagant, he simplified and codified an even more complex cuisine that existed beforehand. Central to his codification of the cuisine were "Le Maître d'hôtel français" (1822), "Le Cuisinier parisien" (1828) and "L'Art de la cuisine française au dix-neuvième siècle" (1833–5).
Georges Auguste Escoffier is commonly acknowledged as the central figure to the modernization of "haute cuisine" and organizing what would become the national cuisine of France. His influence began with the rise of some of the great hotels in Europe and America during the 1880s – 1890s. The Savoy Hotel managed by César Ritz was an early hotel in which Escoffier worked, but much of his influence came during his management of the kitchens in the Carlton from 1898 until 1921. He created a system of "parties" called the brigade system, which separated the professional kitchen into five separate stations.
These five stations included the "garde manger" that prepared cold dishes; the "entremettier" prepared starches and vegetables, the "rôtisseur" prepared roasts, grilled and fried dishes; the "saucier" prepared sauces and soups; and the "pâtissier" prepared all pastry and desserts items. This system meant that instead of one person preparing a dish on one's own, now multiple cooks would prepare the different components for the dish. An example used is "oeufs au plat Meyerbeer", the prior system would take up to fifteen minutes to prepare the dish, while in the new system, the eggs would be prepared by the entremettier, kidney grilled by the rôtisseur, truffle sauce made by the saucier and thus the dish could be prepared in a shorter time and served quickly in the popular restaurants.
Escoffier also simplified and organized the modern menu and structure of the meal. He published a series of articles in professional journals which outlined the sequence, and he finally published his "Livre des menus" in 1912. This type of service embraced the service à la russe (serving meals in separate courses on individual plates), which Félix Urbain Dubois had made popular in the 1860s. Escoffier's largest contribution was the publication of "Le Guide Culinaire" in 1903, which established the fundamentals of French cookery. The book was a collaboration with Philéas Gilbert, E. Fetu, A. Suzanne, B. Reboul, Ch. Dietrich, A. Caillat and others. The significance of this is to illustrate the universal acceptance by multiple high-profile chefs to this new style of cooking.
"Le Guide Culinaire" deemphasized the use of heavy sauces and leaned toward lighter fumets, which are the essence of flavor taken from fish, meat and vegetables. This style of cooking looked to create garnishes and sauces whose function is to add to the flavor of the dish, rather than mask flavors like the heavy sauces and ornate garnishes of the past. Escoffier took inspiration for his work from personal recipes in addition to recipes from Carême, Dubois and ideas from Taillevent's "Viander", which had a modern version published in 1897. A second source for recipes came from existing peasant dishes that were translated into the refined techniques of "haute cuisine".
Expensive ingredients would replace the common ingredients, making the dishes much less humble. The third source of recipes was Escoffier himself, who invented many new dishes, such as pêche Melba. Escoffier updated "Le Guide Culinaire" four times during his lifetime, noting in the foreword to the book's first edition that even with its 5,000 recipes, the book should not be considered an "exhaustive" text, and that even if it were at the point when he wrote the book, "it would no longer be so tomorrow, because progress marches on each day."
This period is also marked by the appearance of the nouvelle cuisine. The term "nouvelle cuisine" has been used many times in the history of French cuisine which emphasized the freshness, lightness and clarity of flavor and inspired by new movements in world cuisine. In the 1740s, Menon first used the term, but the cooking of Vincent La Chapelle and François Marin was also considered modern. In the 1960s, Henri Gault and Christian Millau revived it to describe the cooking of Paul Bocuse, Jean and Pierre Troisgros, Michel Guérard, Roger Vergé and Raymond Oliver. These chefs were working toward rebelling against the "orthodoxy" of Escoffier's cuisine. Some of the chefs were students of Fernand Point at the "Pyramide" in Vienne, and had left to open their own restaurants. Gault and Millau "discovered the formula" contained in ten characteristics of this new style of cooking.
The first characteristic was a rejection of excessive complication in cooking. Second, the cooking times for most fish, seafood, game birds, veal, green vegetables and pâtés was greatly reduced in an attempt to preserve the natural flavors. Steaming was an important trend from this characteristic. The third characteristic was that the cuisine was made with the freshest possible ingredients. Fourth, large menus were abandoned in favor of shorter menus. Fifth, strong marinades for meat and game ceased to be used. Sixth, they stopped using heavy sauces such as espagnole and béchamel thickened with flour based "roux", in favor of seasoning their dishes with fresh herbs, quality butter, lemon juice, and vinegar. Seventh, they used regional dishes for inspiration instead of "haute cuisine" dishes. Eighth, new techniques were embraced and modern equipment was often used; Bocuse even used microwave ovens. Ninth, the chefs paid close attention to the dietary needs of their guests through their dishes. Tenth and finally, the chefs were extremely inventive and created new combinations and pairings.
Some have speculated that a contributor to nouvelle cuisine was World War II when animal protein was in short supply during the German occupation. By the mid-1980s food writers stated that the style of cuisine had reached exhaustion and many chefs began returning to the "haute cuisine" style of cooking, although much of the lighter presentations and new techniques remained.
There are many dishes that are considered part of French national cuisine today.
A meal often consists of three courses, "hors d'œuvre" or "entrée" (introductory course, sometimes soup), "plat principal" (main course), "fromage" (cheese course) or "dessert", sometimes with a salad offered before the cheese or dessert.
French regional cuisine is characterized by its extreme diversity and style. Traditionally, each region of France has its own distinctive cuisine.
Paris and Île-de-France are central regions where almost anything from the country is available, as all train lines meet in the city. Over 9,000 restaurants exist in Paris and almost any cuisine can be obtained here. High-quality Michelin Guide-rated restaurants proliferate here.
Game and ham are popular in Champagne, as well as the special sparkling wine simply known as "Champagne". Fine fruit preserves are known from Lorraine as well as the quiche Lorraine. Alsace is influenced by the German cuisine, especially the one from the Palatinate and Baden region. As such, beers made in the area are similar to the style of bordering Germany. Dishes like choucroute (the French word for sauerkraut) are also popular. Many "Eaux de Vie" (alcoholic distillation) also called schnaps is from this region, due to a wide variety of local fruits (cherry, raspberry, pear, grapes) and especially prunes (mirabelle, plum).[9]:259,295
The coastline supplies many crustaceans, sea bass, monkfish and herring. Normandy has top quality seafood, such as scallops and sole, while Brittany has a supply of lobster, crayfish and mussels. Normandy is home to a large population of apple trees; apples are often used in dishes, as well as cider and Calvados. The northern areas of this region, especially Nord, grow ample amounts of wheat, sugar beets and chicory. Thick stews are found often in these northern areas as well. The produce of these northern regions is also considered some of the best in the country, including cauliflower and artichokes. Buckwheat grows widely in Brittany as well and is used in the region's "galettes", called "jalet", which is where this dish originated.
High-quality fruits come from the Loire Valley and central France, including cherries grown for the liqueur "Guignolet" and the 'Belle Angevine' pears. The strawberries and melons are also of high quality. Fish are seen in the cuisine, often served with a beurre blanc sauce, as well as wild game, lamb, calves, Charolais cattle, "Géline" fowl, and goat cheeses. Young vegetables are used often in the cuisine, as are the specialty mushrooms of the region, "champignons de Paris". Vinegars from Orléans are a specialty ingredient used as well.
Burgundy and Franche-Comté are known for their wines. Pike, perch, river crabs, snails, game, redcurrants, blackcurrants are from both Burgundy and Franche-Comté. Amongst savorous specialties accounted in the "Cuisine franc-comtoise" from the Franche-Comté region are Croûte aux morilles, Poulet à la Comtoise, trout, smoked meats and cheeses such as Mont d'Or, Comté and Morbier which are at the palate best eaten hot or cold, the exquisite Coq au vin jaune and the special dessert gâteau de ménage. Charolais beef, poultry from Bresse, sea snail, honey cake, Chaource and Epoisses cheese are specialties of the local cuisine of Burgundy. Dijon mustard is also a specialty of Burgundy cuisine. "Crème de cassis" is a popular liquor made from the blackcurrants. Oil are used in the cooking here, types include nut oils and rapeseed oil.
The area covers the old province of Dauphiné, once known as the "larder" of France, that gave its name to Gratin dauphinois. The Gratin Dauphinois is traditionally made in an old large baking dish rubbed with garlic. Layers of successively potatoes, salt, pepper and cream are piled up to the top of the dish. It will be baked in the oven at low temperature for 2 hours. Fruit and young vegetables are popular in the cuisine from the Rhône valley, as are great wines like Hermitage AOC, Crozes-Hermitage AOC and Condrieu AOC. Walnuts and walnut products and oil from Noix de Grenoble AOC, lowland cheeses, like St. Marcellin, St. Félicien and Bleu du Vercors-Sassenage. Poultry from Bresse, guinea fowl from Drôme and fish from the Dombes, a light yeast-based cake, called Pogne de Romans and the regional speciality, Raviole du Dauphiné, and there is the short-crust "Suisse", a Valence biscuit speciality. Lakes and mountain streams in Rhône-Alpes are key to the cuisine as well. Lyon and Savoy supply sausages while the Alpine regions supply their specialty cheeses like Beaufort, Abondance, Reblochon, Tomme and Vacherin. "Mères lyonnaises" are female restaurateurs particular to this region who provide local gourmet establishments. Celebrated chefs from this region include Fernand Point, Paul Bocuse, the Troisgros brothers and Alain Chapel. The Chartreuse Mountains, also in the region, are the source of the green and yellow Digestif liquor, Chartreuse produced by the monks of the Grande Chartreuse.
Since the 2014 administrative reform, the ancient area of Auvergne is now part of the region. One of its leading chefs is Regis Marcon.
Oysters come from the Oléron-Marennes basin, while mussels come from the Bay of Aiguillon. High-quality produce comes from the region's hinterland, especially goat cheese. This region and in the Vendée is grazing ground for "Parthenaise" cattle, while poultry is raised in Challans. The region of Poitou-Charentes purportedly produces the best butter and cream in France. Cognac is also made in the region along the Charente River. Limousin is home to the Limousin cattle, as well as sheep. The woodlands offer game and mushrooms. The southern area around Brive draws its cooking influence from Périgord and Auvergne to produce a robust cuisine.
Bordeaux is known for its wine, with certain areas offering specialty grapes for wine-making. Fishing is popular in the region for the cuisine, sea fishing in the Bay of Biscay, trapping in the Garonne and stream fishing in the Pyrenees. The Pyrenees also support lamb, such as the "Agneau de Pauillac", as well as sheep cheeses. Beef cattle in the region include the "Blonde d'Aquitaine", "Boeuf de Chalosse", "Boeuf Gras de Bazas", and "Garonnaise". Free-range chicken, turkey, pigeon, capon, goose and duck prevail in the region as well. Gascony and Périgord cuisines includes "patés", "terrines", "confits" and "magrets". This is one of the regions notable for its production of foie gras or fattened goose or duck liver. The cuisine of the region is often heavy and farm based. Armagnac is also from this region, as are prunes from Agen.
Gers, a department of France, is within this region and has poultry, while La Montagne Noire and Lacaune area offers hams and dry sausages. White corn is planted heavily in the area both for use in fattening the ducks and geese for foie gras and for the production of "millas", a cornmeal porridge. Haricot beans are also grown in this area, which are central to the dish cassoulet. The finest sausage in France is commonly acknowledged to be the "saucisse de Toulouse", which also finds its way into their version of "cassoulet" of Toulouse. The Cahors area produces a specialty "black wine" as well as truffles and mushrooms.
This region also produces milk-fed lamb. Unpasteurized ewe's milk is used to produce the Roquefort in Aveyron, while in Laguiole is producing unpasteurized cow's milk cheese. The Salers cattle produce milk for cheese, as well as beef and veal products. The volcanic soils create flinty cheeses and superb lentils. Mineral waters are produced in high volume in this region as well. Cabécou cheese is from Rocamadour, a medieval settlement erected directly on a cliff, in the rich countryside of Causses du Quercy. This area is one of the region's oldest milk producers; it has chalky soil, marked by history and human activity, and is favourable for the raising of goats.
Restaurants are popular in the area known as "Le Midi". Oysters come from the Etang de Thau, to be served in the restaurants of Bouzigues, Meze, and Sète. Mussels are commonly seen here in addition to fish specialties of Sète, "Bourride", "Tielles" and "Rouille de seiche". In the Languedoc "jambon cru", sometimes known as "jambon de montagne" is produced. High quality "Roquefort" comes from the "brebis" (sheep) on the Larzac plateau. The Les Cévennes area offers mushrooms, chestnuts, berries, honey, lamb, game, sausages, "pâtés" and goat cheeses. Catalan influence can be seen in the cuisine here with dishes like brandade made from a purée of dried cod wrapped in mangold leaves. Snails are plentiful and are prepared in a specific "Catalan" style known as a "cargolade". Wild boar can be found in the more mountainous regions of the "Midi".
The Provence and Côte d'Azur region is rich in quality citrus, vegetables, fruits and herbs; the region is one of the largest suppliers of all these ingredients in France. The region also produces the largest amount of olives, and creates superb olive oil. Lavender is used in many dishes found in "Haute Provence". Other important herbs in the cuisine include thyme, sage, rosemary, basil, savory, fennel, marjoram, tarragon, oregano, and bay leaf. Honey is a prized ingredient in the region. Seafood proliferates throughout the coastal area and is heavily represented in the cuisine. Goat cheeses, air-dried sausages, lamb, beef, and chicken are popular here. Garlic* and anchovies are used in many of the region's sauces, as in "Poulet Provençal", which uses white wine, tomatoes, herbs, and sometimes anchovies, and Pastis is found everywhere that alcohol is served. The cuisine uses a large amount of vegetables for lighter preparations. Truffles are commonly seen in Provence during the winter. Thirteen desserts in Provence are the traditional Christmas dessert, e.g. quince cheese, biscuits, almonds, nougat, apple, and fougasse.
Rice is grown in the Camargue, which is the northernmost rice growing area in Europe, with Camargue red rice being a specialty. Anibal Camous, a Marseillais who lived to be 104, maintained that it was by eating garlic daily that he kept his "youth" and brilliance. When his eighty-year-old son died, the father mourned: "I always told him he wouldn't live long, poor boy. He ate too little garlic!" ("cited by chef Philippe Gion")
Goats and sheep proliferate on the island of Corsica, and lamb are used to prepare dishes such as "stufato", ragouts and roasts. Cheeses are also produced, with "brocciu" being the most popular. Chestnuts, growing in the Castagniccia forest, are used to produce flour, which is used in turn to make bread, cakes and polenta. The forest provides acorns used to feed the pigs and boars that provide much of the protein for the island's cuisine. Fresh fish and seafood are common. The island's pork is used to make fine hams, sausage and other unique items including "coppa" (dried rib cut), "lonzu" (dried pork fillet), "figatella", "salumu" (a dried sausage) "salcietta", "Panzetta", bacon, "figatellu" (smoked and dried liverwurst) and "prisuttu" (farmer's ham). Clementines (which hold an AOC designation), lemons, nectarines and figs are grown there. Candied citron is used in nougats, while and the aforementioned brocciu and chestnuts are also used in desserts. Corsica offers a variety of wines and fruit liqueurs, including Cap Corse, Patrimonio, "Cédratine", "Bonapartine", "liqueur de myrte", "vins de fruit", "Rappu", and "eau-de-vie de châtaigne".
French Guianan cuisine or Guianan cuisine is a blend of the different cultures that have settled in French Guiana. Creole and Chinese restaurants are common in major cities such as Cayenne, Kourou and Saint-Laurent-du-Maroni. Many indigenous animal species such as caiman and tapir are used in spiced stews.
French cuisine varies according to the season. In summer, salads and fruit dishes are popular because they are refreshing and produce is inexpensive and abundant. Greengrocers prefer to sell their fruits and vegetables at lower prices if needed, rather than see them rot in the heat. At the end of summer, mushrooms become plentiful and appear in stews throughout France. The hunting season begins in September and runs through February. Game of all kinds is eaten, often in elaborate dishes that celebrate the success of the hunt. Shellfish are at their peak when winter turns to spring, and oysters appear in restaurants in large quantities.
With the advent of deep-freeze and the air-conditioned "hypermarché", these seasonal variations are less marked than hitherto, but they are still observed, in some cases due to legal restrictions. Crayfish, for example, have a short season and it is illegal to catch them out of season. Moreover, they do not freeze well.
French regional cuisines use locally grown vegetables, such as "pomme de terre" (potato), "blé" (wheat), "haricots verts" (a type of French green bean), "carotte" (carrot), "poireau" (leek), "navet" (turnip), "aubergine" (eggplant), "courgette" (zucchini), and "échalotte" (shallot).
French regional cuisines use locally grown fungi, such as "truffe" (truffle), "champignon de Paris" (button mushroom), "chanterelle ou girolle" (chanterelle), "pleurote (en huître)" (oyster mushrooms), and "cèpes" (porcini).
Common fruits include oranges, tomatoes, tangerines, peaches, apricots, apples, pears, plums, cherries, strawberries, raspberries, redcurrants, blackberries, grapes, grapefruit, and blackcurrants.
Varieties of meat consumed include "poulet" (chicken), "pigeon" (squab), "canard" (duck), "oie" (goose, the source of foie gras), "bœuf" (beef), "veau" (veal), "porc" (pork), "agneau" (lamb), "mouton" (mutton), "caille" (quail), "cheval" (horse), "grenouille" (frog), and "escargot" (snails). Commonly consumed fish and seafood include cod, canned sardines, fresh sardines, canned tuna, fresh tuna, salmon, trout, mussels, herring, oysters, shrimp and calamari.
Eggs are fine quality and often eaten as: omelettes, hard-boiled with mayonnaise, scrambled plain, scrambled "haute cuisine" preparation, œuf à la coque.
Herbs and seasonings vary by region, and include "fleur de sel", "herbes de Provence", tarragon, rosemary, marjoram, lavender, thyme, fennel, and sage.
Fresh fruit and vegetables, as well as fish and meat, can be purchased either from supermarkets or specialty shops. Street markets are held on certain days in most localities; some towns have a more permanent covered market enclosing food shops, especially meat and fish retailers. These have better shelter than the periodic street markets.
"Le petit déjeuner" (breakfast) is traditionally a quick meal consisting of "tartines" (slices) of French bread with butter and honey or jam (sometimes brioche), along with café au lait (also called "café crème"), or black coffee, or tea and rarely hot chicory. Children often drink hot chocolate in bowls or cups along with their breakfasts. "Croissants", "pain aux raisins" or "pain au chocolat" (also named "chocolatine" in the south-west of France) are mostly included as a weekend treat. Breakfast of some kind is always served in cafés opening early in the day.
There are also savoury dishes for breakfast. An example is "le petit déjeuner gaulois" or "petit déjeuner fermier" with the famous long narrow bread slices with soft white cheese topped or boiled ham, called "mouillettes", which is dipped in a soft-boiled egg and some fruit juice and hot drink.
Another variation called "le petit déjeuner chasseur", meant to be very hearty, is served with pâté and other charcuterie products. A more classy version is called "le petit déjeuner du voyageur", where delicatessens serve gizzard, bacon, salmon, omelet, or croque-monsieur, with or without soft-boiled egg and always with the traditional coffee/tea/chocolate along fruits or fruit juice. When the egg is cooked sunny-side over the croque-monsieur, it is called a croque-madame.
In "Germinal" and other novels, Émile Zola also reported the "briquet": two long bread slices stuffed with butter, cheese and or ham. It can be eaten as a standing/walking breakfast, or meant as a "second" one before lunch.
In the movie "Bienvenue chez les Ch'tis", Philippe Abrams (Kad Merad) and Antoine Bailleul (Dany Boon) share together countless breakfasts consisting of "tartines de Maroilles" (a rather strong cheese) along with their hot chicory.
"Le déjeuner" (lunch) is a two-hour mid-day meal or a one-hour lunch break. In some smaller towns and in the south of France, the two-hour lunch may still be customary. Sunday lunches are often longer and are taken with the family. Restaurants normally open for lunch at noon and close at 2:30 pm. Some restaurants are closed on Monday during lunch hours.
In large cities, a majority of working people and students eat their lunch at a corporate or school cafeteria, which normally serves complete meals as described above; it is not usual for students to bring their own lunch to eat. For companies that do not operate a cafeteria, it is mandatory for white-collar workers to be given lunch vouchers as part of their employee benefits. These can be used in most restaurants, supermarkets and "traiteurs"; however, workers having lunch in this way typically do not eat all three courses of a traditional lunch due to price and time constraints. In smaller cities and towns, some working people leave their workplaces to return home for lunch. Also, an alternative, especially among blue-collar workers, is eating sandwiches followed by a dessert; both dishes can be found ready-made at bakeries and supermarkets at budget prices.
"Le dîner" (dinner) often consists of three courses, "hors d'œuvre" or "entrée" (appetizers or introductory course, sometimes soup), "plat principal" (main course), and a cheese course or dessert, sometimes with a salad offered before the cheese or dessert. Yogurt may replace the cheese course, while a simple dessert would be fresh fruit. The meal is often accompanied by bread, wine and mineral water. Most of the time the bread would be a baguette which is very common in France and is made almost every day. Main meat courses are often served with vegetables, along with potatoes, rice or pasta. Restaurants often open at 7:30 pm for dinner, and stop taking orders between the hours of 10:00 pm and 11:00 pm. Some restaurants close for dinner on Sundays.
In French cuisine, beverages that precede a meal are called apéritifs (literally: "that opens the appetite"), and can be served with amuse-bouches (literally: "mouth amuser"). Those that end it are called digestifs.
The apéritif varies from region to region: Pastis is popular in the south of France, Crémant d'Alsace in the eastern region. Champagne can also be served. Kir, also called "Blanc-cassis", is a common and popular apéritif-cocktail made with a measure of crème de cassis (blackcurrant liqueur) topped up with white wine. The phrase "Kir Royal" is used when white wine is replaced with a "Champagne" wine. A simple glass of red wine, such as Beaujolais nouveau, can also be presented as an apéritif, accompanied by amuse-bouches. Some apéritifs can be fortified wines with added herbs, such as cinchona, gentian and vermouth. Trade names that sell well include Suze (the classic gentiane), Byrrh, Dubonnet, and Noilly Prat.
Digestifs are traditionally stronger, and include Cognac, Armagnac, Calvados, Eau de vie and fruit alcohols.
A typical French Christmas dish is turkey with chestnuts. Other common dishes are smoked salmon, oysters, caviar and "foie gras". The Yule log is a very French tradition during Christmas. Chocolate and cakes also occupy a prominent place for Christmas in France. This cuisine is normally accompanied by Champagne. Tradition says that thirteen desserts complete the Christmas meal in reference to the twelve apostles and Christ.
The modern restaurant has its origins in French culture. Prior to the late 18th century, diners who wished to "dine out" would visit their local guild member's kitchen and have their meal prepared for them. However, guild members were limited to producing whatever their guild registry delegated to them. These guild members offered food in their own homes to steady clientele that appeared day-to-day but at set times. The guest would be offered the meal table d'hôte, which is a meal offered at a set price with very little choice of dishes, sometimes none at all.
The first steps toward the modern restaurant were locations that offered "restorative" bouillons, or "restaurants" – these words being the origin of the name "restaurant". This step took place during the 1760s–1770s. These locations were open at all times of the day, featuring ornate tableware and reasonable prices. These locations were meant more as meal replacements for those who had "lost their appetites and suffered from jaded palates and weak chests."
In 1782 Antoine Beauvilliers, pastry chef to the future Louis XVIII, opened one of the most popular restaurants of the time – the "Grande Taverne de Londres" – in the arcades of the Palais-Royal. Other restaurants were opened by chefs of the time who were leaving the failing monarchy of France, in the period leading up to the French Revolution. It was these restaurants that expanded upon the limited menus of decades prior, and led to the full restaurants that were completely legalized with the advent of the French Revolution and abolition of the guilds. This and the substantial discretionary income of the French Directory's "nouveau riche" helped keep these new restaurants in business.
Larger restaurants and hotels in France employ extensive staff and are commonly referred to as either the "kitchen brigade" for the kitchen staff or "dining room brigade" system for the dining room staff. This system was created by Georges Auguste Escoffier. This structured team system delegates responsibilities to different individuals who specialize in certain tasks. The following is a list of positions held both in the kitchen and dining rooms brigades in France:

</doc>
<doc id="11003" url="https://en.wikipedia.org/wiki?curid=11003" title="Five-spice powder">
Five-spice powder

Five-spice powder () is a spice mixture of five or more spices used predominantly in almost all branches of Chinese cuisines and Vietnamese cuisine. The five flavors of the spices (sweet, bitter, sour, salty, and pungent) refers to the five traditional Chinese elements. 
While there are many variants, a common mix is:
Other recipes may contain anise seed, ginger root, nutmeg, turmeric, "Amomum villosum" pods (), "Amomum cardamomum" pods (), licorice, Mandarin orange peel or galangal.
In Southern China, "Cinnamomum loureiroi" and Mandarin orange peel are commonly used as substitutes for "Cinnamomum cassia" and cloves respectively, producing a slightly different flavour profile for southern five-spice powders. In one study, the potential antioxidant capacities of Chinese five-spice powder (consisting of Szechuan pepper, fennel seed, cinnamon, star anise and clove) with varying proportion of individual spice ingredients was investigated through four standard methods. The results suggest that clove is the major contributor to the high antioxidant capacities of the five-spice powder whereas the other four ingredients contribute to the flavor.
Five spice may be used with fatty meats such as pork, duck or goose. It is used as a spice rub for chicken, duck, pork and seafood, in red cooking recipes, or added to the breading for fried foods. Five spice is used in recipes for Cantonese roasted duck, as well as beef stew. It is used as a marinade for Vietnamese broiled chicken. The five-spice powder mixture has followed the Chinese diaspora and has been incorporated into other national cuisines throughout Asia.
In Hawaii, some restaurants place a shaker of the spice on each patron's table. A seasoned salt can be easily made by dry-roasting common salt with five-spice powder under low heat in a dry pan until the spice and salt are well mixed.
Five spice powder can also add complexity and savoriness to sweets and savory dishes alike. 

</doc>
<doc id="11004" url="https://en.wikipedia.org/wiki?curid=11004" title="Fundamental group">
Fundamental group

In the mathematical field of algebraic topology, the fundamental group of a topological space is the group of the equivalence classes under homotopy of the 
loops contained in the space. It records information about the basic shape, or holes, of the topological space. The fundamental group is the first and simplest homotopy group. The fundamental group is a homotopy invariant—topological spaces that are homotopy equivalent (or the stronger case of homeomorphic) have isomorphic fundamental groups.
Start with a space (for example, a surface), and some point in it, and all the loops both starting and ending at this point—paths that start at this point, wander around and eventually return to the starting point. Two loops can be combined together in an obvious way: travel along the first loop, then along the second.
Two loops are considered equivalent if one can be deformed into the other without breaking. The set of all such loops with this method of combining and this equivalence between them is the fundamental group for that particular space.
Henri Poincaré defined the fundamental group in 1895 in his paper "Analysis situs". The concept emerged in the theory of Riemann surfaces, in the work of Bernhard Riemann, Poincaré, and Felix Klein. It describes the monodromy properties of complex-valued functions, as well as providing a complete topological classification of closed surfaces.
Throughout this article, "X" is a topological space. A typical example is a surface such as the one depicted at the right. Moreover, formula_1 is a point in "X" called the "base-point". (As is explained below, its role is rather auxiliary.) The idea of the definition of the homotopy group is to measure how many (broadly speaking) curves on "X" can be deformed into each other. The precise definition depends on the notion of the homotopy of loops, which is explained first.
Given a topological space "X", a "loop based at formula_1" is defined to be a continuous function (also known as a continuous map)
such that both the starting point formula_4 and the end point formula_5 are both equal to formula_6
A "homotopy" is a continuous interpolation between two loops. More precisely, a homotopy between two loops formula_7 (based at the same point formula_1) is a continuous map
such that
If such a homotopy "h" exists, formula_18 and formula_19 are said to be "homotopic". The relation "formula_18 is homotopic to formula_19" is an equivalence relation so that the set of equivalence classes can be considered:
This set (with the group structure described below) is called the "fundamental group" of the topological space "X" and the base point formula_6 The purpose of considering the equivalence classes of loops up to homotopy, as opposed to the set of all loops (the so-called loop space of "X") is that the latter, while being useful for various purposes, is a rather big and unwieldy object. By contrast the above quotient is, in many cases, more manageable and computable.
By the above definition, formula_24 is just a set. It becomes a group (and therefore deserves the name fundamental "group") using the concatenation of loops. More precisely, given two loops formula_25 their product is defined as the loop
Thus the loop formula_27 first follows the loop formula_28 with "twice the speed" and then follows formula_29 with "twice the speed".
The product of two homotopy classes of loops formula_30 and formula_31 is then defined as formula_32 It can be shown that this product does not depend on the choice of representatives and therefore gives a well-defined operation on the set formula_33 This operation turns formula_24 into a group. Its neutral element is the constant loop, which stays at formula_1 for all times "t". The inverse of a (homotopy class of a) loop is the same loop, but traversed in the opposite direction. More formally,
Given three based loops formula_37 the product
is the concatenation of these loops, traversing formula_28 and then formula_29 with quadruple speed, and then formula_41 with double speed. By comparison,
traverses the same paths (in the same order), but formula_28 with double speed, and formula_44 with quadruple speed. Thus, because of the differing speeds, the two paths are not identical. The associativity axiom
therefore crucially depends on the fact that paths are considered up to homotopy. Indeed, both above composites are homotopic, for example, to the loop that traverses all three loops formula_46 with triple speed. The set of based loops up to homotopy, equipped with the above operation therefore does turn formula_24 into a group.
Although the fundamental group in general depends on the choice of base point, it turns out that, up to isomorphism (actually, even up to "inner" isomorphism), this choice makes no difference as long as the space "X" is path-connected. For path-connected spaces, therefore, many authors therefore write formula_48 instead of formula_49
This section lists some basic examples of fundamental groups. To begin with, in Euclidean space (formula_50) or any convex subset of formula_51 there is only one homotopy class of loops, and the fundamental group is therefore the trivial group with one element. More generally, any star domain and, yet more generally any contractible space has a trivial fundamental group. Thus, the fundamental group does not distinguish between such spaces. 
A path-connected space whose fundamental group is trivial is called simply connected. 
For example, the 2-sphere formula_52 depicted at the left, and also all the higher-dimensional spheres are simply-connected. The figure illustrates a homotopy contracting one particular loop to the constant loop. This idea can be adapted to all loops formula_18 such that there is a point formula_54 that is in the image of formula_55 However, since there are loops such that formula_56 (constructed from the Peano curve, for example), a complete proof requires more careful analysis with tools from algebraic topology, such as the Seifert–van Kampen theorem or the cellular approximation theorem.
The circle (also known as the 1-sphere)
is not simply connected. Instead, each homotopy class consists of all loops that wind around the circle a given number of times (which can be positive or negative, depending on the direction of winding). The product of a loop that winds around "m" times and another that winds around "n" times is a loop that winds around formula_58 times. Therefore, the fundamental group of the circle is isomorphic to formula_59 the additive group of integers. This fact can be used to give proofs of the Brouwer fixed point theorem and the Borsuk–Ulam theorem in dimension 2.
The fundamental group of the figure eight is the free group on two letters. The idea to prove this is as follows: choosing the base point to be the point where the two circles meet (dotted in black in the picture at the right), any loop formula_18 can be decomposed as
where "a" and "b" are the two loops winding around each half of the figure as depicted, and the exponents formula_62 are integers. Unlike formula_63 the fundamental group of the figure eight is "not" abelian: the two ways of composing "a" and "b" are not homotopic to each other:
More generally, the fundamental group of a bouquet of "r" circles is the free group on "r" letters. 
The fundamental group of a wedge sum of two path connected spaces "X" and "Y" can be computed as the free product of the individual fundamental groups:
This generalizes the above observations since the figure eight is the wedge sum of two circles.
The fundamental group of the plane punctured at "n" points is also the free group with "n" generators. The "i"-th generator is the class of the loop that goes around the "i"-th puncture without going around any other punctures.
The fundamental group can be defined for discrete structures too. In particular, consider a connected graph "G" = ("V", "E"), with a designated vertex "v" in "V". The loops in "G" are the cycles that start and end at "v". Let "T" be a spanning tree of "G". Every simple loop in "G" contains exactly one edge in "E" \ "T"; every loop in "G" is a concatenation of such simple loops. Therefore, the fundamental group of a graph is a free group, in which the number of generators is exactly the number of edges in "E" \ "T". This number equals |"E"|-|"V"|+1. 
For example, suppose "G" has 16 vertices arranged in 4 rows of 4 vertices each, with edges connecting vertices that are adjacent horizontally or vertically. Then "G" has 24 edges overall, and the number of edges in each spanning tree is 16-1=15, so the fundamenetal group of "G" is the free group with 9 generators. Note that "G" has 9 "holes", similarly to a bouquet of 9 circles, which has the same fundamental group. 
"Knot groups" are, by definition the fundamental group of the complement of a knot "K" embedded in formula_66 For example, the knot group of the trefoil knot is known to be the braid group formula_67 which gives another example of a non-abelian fundamental group. The Wirtinger presentation explicitly describes knot groups in terms of generators and relations based on a diagram of the knot. Therefore knot groups have some usage in knot theory to distinguish between knots: if formula_68 is not isomorphic to some other knot group formula_69 of another knot "K"', then "K" can not be transformed into formula_70 Thus the trefoil knot can not be continuously transformed into the circle (also known as the unknot), since the latter has knot group formula_71. There are, however, knots that can not be deformed into each other, but have isomorphic knot groups.
The fundamental group of a genus "n" orientable surface can be computed in terms of generators and relations as
This includes the torus, being the case of genus 1, whose fundamental group is
The fundamental group of a topological group "X" (with respect to the base point being the neutral element) is always commutative. In particular, the fundamental group of a Lie group is commutative. In fact, the group structure on "X" endows formula_48 with another group structure: given two loops formula_18 and formula_19 in "X", another loop formula_77 can defined by using the group multiplication in "X": 
This binary operation formula_79 on the set of all loops is "a priori" independent from the one described above. However, the Eckmann–Hilton argument shows that it does in fact agree with the above concatenation of loops, and moreover that the resulting group structure is abelian. 
An inspection of the proof shows that, more generally, formula_48 is abelian for any H-space "X", i.e., the multiplication need not have an inverse, nor does it have to be associative. For example, this shows that the fundamental group of a loop space of another topological space "Y", formula_81 is abelian. Related ideas lead to Heinz Hopf's computation of the cohomology of a Lie group.
If formula_82 is a continuous map, formula_83 and formula_84 with formula_85 then every loop in "X" with base point formula_1 can be composed with "f" to yield a loop in "Y" with base point formula_87 This operation is compatible with the homotopy equivalence relation and with composition of loops. The resulting group homomorphism, called the induced homomorphism, is written as formula_88 or, more commonly,
This mapping from continuous maps to group homomorphisms is compatible with composition of maps and identity morphisms. In the parlance of category theory, the formation of associating to a topological space its fundamental group is therefore a functor
from the category of topological spaces together with a base point to the category of groups. It turns out that this functor does not distinguish maps that are homotopic relative to the base point: if "f", "g" : "X" → "Y" are continuous maps with "f"("x") = "g"("x") = "y", and "f" and "g" are homotopic relative to {"x"}, then "f" = "g". As a consequence, two homotopy equivalent path-connected spaces have isomorphic fundamental groups:
For example, the inclusion of the circle in the punctured plane
is a homotopy equivalence and therefore yields an isomorphism of their fundamental groups.
The fundamental group functor takes products to products and coproducts to coproducts. That is, if "X" and "Y" are path connected, then
As was mentioned above, computing the fundamental group of even relatively simple topological spaces tends to be not entirely trivial, but requires some methods of algebraic topology. 
The abelianization of the fundamental group can be identified with the first homology group of the space. 
A special case of the Hurewicz theorem asserts that the first singular homology group formula_94 is, colloquially speaking, the closest approximation to the fundamental group by means of an abelian group. In more detail, mapping the homotopy class of each loop to the homology class of the loop gives a group homomorphism
from the fundamental group of a topological space "X" to its first singular homology group formula_96 This homomorphism is not in general an isomorphism since the fundamental group may be non-abelian, but the homology group is, by definition, always abelian. This difference is, however, the only one: if "X" is path-connected, this homomorphism is surjective and its kernel is the commutator subgroup of the fundamental group, so that formula_94 is isomorphic to the abelianization of the fundamental group.
Generalizing the statement above, for a family of path connected spaces formula_98 the fundamental group formula_99 is the free product of the fundamental groups of the formula_100 This fact is a special case of the Seifert–van Kampen theorem, which allows to compute, more generally, fundamental groups of spaces that are glued together from other spaces. For example, the 2-sphere formula_101 can be obtained by glueing two copies of slightly overlapping half-spheres along a neighborhood of the equator. In this case the theorem yields formula_102 is trivial, since the two half-spheres are contractible and therefore have trivial fundamental group. The fundamental groups of surfaces, as mentioned above, can also be computed using this theorem.
In the parlance of category theory, the theorem can be concisely stated by saying that the fundamental group functor takes pushouts (in the category of topological spaces) along inclusions to pushouts (in the category of groups).
Given a topological space "B", a continuous map 
is called a "covering" or "E" is called a "covering space" of "B" if every point "b" in "B" admits an open neighborhood "U" such that there is a homeomorphism between the preimage of "U" and a disjoint union of copies of "U" (indexed by some set "I"),
in such a way that formula_105 is the standard projection map formula_106
A covering is called a universal covering of "E" is, in addition to the preceding condition, simply connected. It is universal in the sense that all other coverings can be constructed by suitably identifying points in "E". Knowing a universal covering 
of a topological space "X" is helpful in understanding its fundamental group in several ways: first, formula_48 identifies with the group of deck transformations, i.e., the group of homeomorphisms formula_109 that commute with the map to "X", i.e., formula_110
Another relation to the fundamental group is that formula_111 can be identified with the fiber formula_112 For example, the map
(or, equivalently, formula_114) is a universal covering. The deck transformations are the maps formula_115 for formula_116 This is in line with the identification formula_117 in particular this proves the above claim formula_118
Any path connected, locally path connected and locally simply connected topological space "X" admits a universal covering. An abstract construction proceeds analogously to the fundamental group by taking pairs ("x", γ), where "x" is a point in "X" and γ is a homotopy class of paths from "x" to "x". The passage from a topological space to its universal covering can be used in understanding the geometry of "X". For example, the uniformization theorem shows that any simply connected Riemann surface is (isomorphic to) either formula_119 formula_120 or the upper half plane. General Riemann surfaces then arise as quotients of group actions on these three surfaces.
The quotient of an action of a (discrete) group "G" on a simply connected space "Y" has fundamental group 
As an example, the real "n"-dimensional real projective space formula_122 is obtained as the quotient of the "n"-dimensional sphere formula_123 by the antipodal action of the group formula_124 sending formula_125 to formula_126 As formula_123 is simply connected for "n" ≥ 2, it is a universal cover of formula_122 in these cases, which implies formula_129 for "n" ≥ 2.
Let "G" be a connected, simply connected compact Lie group, for example, the special unitary group SU("n"), and let Γ be a finite subgroup of "G". Then the homogeneous space "X" = "G"/Γ has fundamental group Γ, which acts by right multiplication on the universal covering space "G". Among the many variants of this construction, one of the most important is given by locally symmetric spaces "X" = Γ\"G"/"K", where
In this case the fundamental group is Γ and the universal covering space "G"/"K" is actually contractible (by the Cartan decomposition for Lie groups).
As an example take "G" = SL(2, R), "K" = SO(2) and Γ any torsion-free congruence subgroup of the modular group SL(2, Z).
From the explicit realization, it also follows that the universal covering space of a path connected topological group "H" is again a path connected topological group "G". Moreover, the covering map is a continuous open homomorphism of "G" onto "H" with kernel Γ, a closed discrete normal subgroup of "G":
Since "G" is a connected group with a continuous action by conjugation on a discrete group Γ, it must act trivially, so that Γ has to be a subgroup of the center of "G". In particular π("H") = Γ is an abelian group; this can also easily be seen directly without using covering spaces. The group "G" is called the "universal covering group" of "H".
As the universal covering group suggests, there is an analogy between the fundamental group of a topological group and the center of a group; this is elaborated at Lattice of covering groups.
"Fibrations" provide a very powerful means to compute homotopy groups. A fibration "f" the so-called "total space", and the base space "B" has, in particular, the property that all its fibers formula_131 are homotopy equivalent and therefore can not be distinguished using fundamental groups (and higher homotopy groups), provided that "B" is path-connected. Therefore, the space "E" can be regarded as a "twisted product" of the base space "B" and the fiber formula_132 The great importance of fibrations to the computation of homotopy groups stems from a long exact sequence
provided that "B" is path-connected. The term formula_134 is the second homotopy group of "B", which is defined to be the set of homotopy classes of maps from formula_101 to "B", in direct analogy with the definition of formula_136
If "E" happens to be path-connected and simply connected, this sequence reduces to an isomorphism
which generalizes the above fact about the universal covering (which amounts to the case where the fiber "F" is also discrete). If instead "F" happens to be connected and simply connected, it reduces to an isomorphism
What is more, the sequence can be continued at the left with the higher homotopy groups formula_139 of the three spaces, which gives some access to computing such groups in the same vein.
Such fiber sequences can be used to inductively compute fundamental groups of compact classical Lie groups such as the special unitary group formula_140 with formula_141 This group acts transitively on the unit sphere formula_142 inside formula_143 The stabilizer of a point in the sphere is isomorphic to formula_144 It then can be shown that this yields a fiber sequence
Since formula_146 the sphere formula_142 has dimension at least 3, which implies 
The long exact sequence then shows an isomorphism
Since formula_150 is a single point, so that formula_151 is trivial, this shows that formula_152 is simply connected for all formula_153
The fundamental group of noncompact Lie groups can be reduced to the compact case, since such a group is homotopic to its maximal compact subgroup. These methods give the following results:
A second method of computing fundamental groups applies to all connected compact Lie groups and uses the machinery of the maximal torus and the associated root system. Specifically, let formula_154 be a maximal torus in a connected compact Lie group formula_155 and let formula_156 be the Lie algebra of formula_157 The exponential map
is a fibration and therefore its kernel formula_159 identifies with formula_160 The map 
can be shown to be surjective with kernel given by the set "I" of integer linear combination of coroots. This leads to the computation 
This method shows, for example, that any connected compact Lie group for which the associated root system is of type formula_163 is simply connected. Thus, there is (up to isomorphism) only one connected compact Lie group having Lie algebra of type formula_163; this group is simply connected and has trivial center.
When the topological space is homeomorphic to a simplicial complex, its fundamental group can be described explicitly in terms of generators and relations.
If "X" is a connected simplicial complex, an "edge-path" in "X" is defined to be a chain of vertices connected by edges in "X". Two edge-paths are said to be "edge-equivalent" if one can be obtained from the other by successively switching between an edge and the two opposite edges of a triangle in "X". If "v" is a fixed vertex in "X", an "edge-loop" at "v" is an edge-path starting and ending at "v". The edge-path group "E"("X", "v") is defined to be the set of edge-equivalence classes of edge-loops at "v", with product and inverse defined by concatenation and reversal of edge-loops.
The edge-path group is naturally isomorphic to π(|"X"|, "v"), the fundamental group of the geometric realisation |"X"| of "X". Since it depends only on the 2-skeleton "X" of "X" (that is, the vertices, edges, and triangles of "X"), the groups π(|"X"|,"v") and π(|"X"|, "v") are isomorphic.
The edge-path group can be described explicitly in terms of generators and relations. If "T" is a maximal spanning tree in the 1-skeleton of "X", then "E"("X", "v") is canonically isomorphic to the group with generators (the oriented edge-paths of "X" not occurring in "T") and relations (the edge-equivalences corresponding to triangles in "X"). A similar result holds if "T" is replaced by any simply connected—in particular contractible—subcomplex of "X". This often gives a practical way of computing fundamental groups and can be used to show that every finitely presented group arises as the fundamental group of a finite simplicial complex. It is also one of the classical methods used for topological surfaces, which are classified by their fundamental groups.
The "universal covering space" of a finite connected simplicial complex "X" can also be described directly as a simplicial complex using edge-paths. Its vertices are pairs ("w",γ) where "w" is a vertex of "X" and γ is an edge-equivalence class of paths from "v" to "w". The "k"-simplices containing ("w",γ) correspond naturally to the "k"-simplices containing "w". Each new vertex "u" of the "k"-simplex gives an edge "wu" and hence, by concatenation, a new path γ from "v" to "u". The points ("w",γ) and ("u", γ) are the vertices of the "transported" simplex in the universal covering space. The edge-path group acts naturally by concatenation, preserving the simplicial structure, and the quotient space is just "X".
It is well known that this method can also be used to compute the fundamental group of an arbitrary topological space. This was doubtless known to Eduard Čech and Jean Leray and explicitly appeared as a remark in a paper by André Weil; various other authors such as Lorenzo Calabi, Wu Wen-tsün, and Nodar Berikashvili have also published proofs. In the simplest case of a compact space "X" with a finite open covering in which all non-empty finite intersections of open sets in the covering are contractible, the fundamental group can be identified with the edge-path group of the simplicial complex corresponding to the nerve of the covering.
Roughly speaking, the fundamental group detects the 1-dimensional hole structure of a space, but not holes in higher dimensions such as for the 2-sphere. Such "higher-dimensional holes" can be detected using the higher homotopy groups formula_165, which are defined to consist of homotopy classes of (basepoint-preserving) maps from formula_123 to "X". For example, the Hurewicz theorem implies that the "n"-th homotopy group of the "n"-sphere is (for all formula_167) are
As was mentioned in the above computation of formula_169 of classical Lie groups, higher homotopy groups can be relevant even for computing fundamental groups.
The set of based loops (as is, i.e., not taken up to homotopy) in a pointed space "X", endowed with the compact open topology, is known as the loop space, denoted formula_170 The fundamental group of "X" is in bijection with the set of path components of its loop space:
The "fundamental groupoid" is a variant of the fundamental group that is useful in situations where the choice of a base point formula_172 is undesirable. It is defined by first considering the category of paths in formula_173 i.e., continuous functions
where "r" is an arbitrary non-negative real number. Since the length "r" is variable in this approach, such paths can be concatenated as is (i.e., not up to homotopy) and therefore yield a category. Two such paths formula_175 with the same endpoints and length "r", resp. "r"' are considered equivalent if there exist real numbers formula_176 such that formula_177 and formula_178 are homotopic relative to their end points, where formula_179 The category of paths up to this equivalence relation is denoted formula_180 Each morphism in formula_181 is an isomorphism, with inverse given by the same path traversed in the opposite direction. Such a category is called a groupoid. It reproduces the fundamental group since
More generally, one can consider the fundamental groupoid on a set "A" of base points, chosen according to the geometry of the situation; for example, in the case of the circle, which can be represented as the union of two connected open sets whose intersection has two components, one can choose one base point in each component. The van Kampen theorem admits a version for fundamental groupoids which gives, for example, another way to compute the fundamental group(oid) of formula_183
Generally speaking, representations may serve to exhibit features of a group by its actions on other mathematical objects, often vector spaces. Representations of the fundamental group have a very geometric significance: any "local system" (i.e., a sheaf formula_184 on "X" with the property that locally in a sufficiently small neighborhood "U" of any point on "X", the restriction of "F" is a constant sheaf of the form formula_185) gives rise to the so-called monodromy representation, a representation of the fundamental group on an "n"-dimensional formula_186-vector space. Conversely, any such representation on a path-connected space "X" arises in this manner. This equivalence of categories between representations of formula_48 and local systems is used, for example, in the study of differential equations, such as the Knizhnik–Zamolodchikov equations.
In algebraic geometry, the so-called étale fundamental group is used as a replacement for the fundamental group. Since the Zariski topology on an algebraic variety or scheme "X" is much coarser than, say, the topology of open subsets in formula_188 it is no longer meaningful to consider continuous maps from an interval to "X". Instead, the approach developed by Grothendieck consists in constructing formula_189 by considering all finite étale covers of "X". These serve as an algebro-geometric analogue of coverings with finite fibers. 
This yields a theory applicable in situation where no great generality classical topological intuition whatsoever is available, for example for varieties defined over a finite field. Also, the étale fundamental group of a field is its (absolute) Galois group. On the other hand, for smooth varieties "X" over the complex numbers, the étale fundamental group retains much of the information inherent in the classical fundamental group: the former is the profinite completion of the latter.
The fundamental group of a root system is defined, in analogy to the computation for Lie groups. This allows to define and use the fundamental group of a semisimple linear algebraic group "G", which is a useful basic tool in the classification of linear algebraic groups.
The homotopy relation between 1-simplices of a simplicial set "X" is an equivalence relation if "X" is a Kan complex but not necessarily so in general. Thus, formula_169 of a Kan complex can be defined as the set of homotopy classes of 1-simplices. The fundamental group of an arbitrary simplicial set "X" are defined to be the homotopy group of its topological realization, formula_191 i.e., the topological space obtained by glueing topological simplices as prescribed by the simplicial set structure of "X".

</doc>
<doc id="11006" url="https://en.wikipedia.org/wiki?curid=11006" title="February 19">
February 19


</doc>
<doc id="11007" url="https://en.wikipedia.org/wiki?curid=11007" title="February 24">
February 24


</doc>
<doc id="11008" url="https://en.wikipedia.org/wiki?curid=11008" title="February 23">
February 23


</doc>
<doc id="11009" url="https://en.wikipedia.org/wiki?curid=11009" title="February 22">
February 22


</doc>
<doc id="11010" url="https://en.wikipedia.org/wiki?curid=11010" title="February 21">
February 21


</doc>
<doc id="11011" url="https://en.wikipedia.org/wiki?curid=11011" title="FBI (disambiguation)">
FBI (disambiguation)

FBI is the Federal Bureau of Investigation of the United States.
FBI may also refer to:

</doc>
<doc id="11012" url="https://en.wikipedia.org/wiki?curid=11012" title="Forth (programming language)">
Forth (programming language)

Forth is an imperative stack-based computer programming language and environment originally designed by Chuck Moore. Language features include structured programming, reflection (the ability to examine and modify program structure during execution), concatenative programming (functions are composed with juxtaposition) and extensibility (the programmer can create new commands). Although not an acronym, the language's name is sometimes spelled with all capital letters as FORTH, following the customary usage during its earlier years.
A procedural programming language without type checking, Forth features both interactive execution of commands (making it suitable as a shell for systems that lack a more formal operating system) and the ability to compile sequences of commands for later execution. For much of Forth's existence, the standard technique was to compile to threaded code, but there are modern implementations that generate optimized machine code like other language compilers.
Forth is used in the Open Firmware boot loader, in space applications such as the Philae spacecraft, and in other embedded systems which involve interaction with hardware. The bestselling 1986 computer game "Starflight", from Electronic Arts, was written with a custom Forth.
The free software Gforth implementation is actively maintained, as are several commercially supported systems.
Forth is a simple, yet extensible language; its modularity and extensibility permit writing significant programs.
A Forth environment combines the compiler with an interactive shell, where the user defines and runs subroutines called "words". Words can be tested, redefined, and debugged as the source is entered without recompiling or restarting the whole program. Thus Forth programmers enjoy the immediacy of an interpreter while at the same time the performance and efficiency of a compiler. All syntactic elements, including variables and basic operators, are defined as words. Forth environments vary in how the resulting program is stored, but ideally running the program has the same effect as manually re-entering the source.
The Forth philosophy emphasizes the use of small, simple words (subroutines) that perform the fewest functions possible. Words for bigger tasks would call upon many smaller words that each accomplish a distinct sub-task. These in turn invoke ever-smaller words to contribute to the goals and intermediate goals of the words at higher levels. Thus a large Forth program will consist of a hierarchy of words, each being the smallest and simplest possible for its level. These words, being distinct modules that communicate (pass data) implicitly via a stack mechanism, can be prototyped, built and tested independently. This program structure facilitates fast and easy development, both top-down and bottom-up. It also makes for versatile program modification to meet ever-changing requirements.
The ability to create new words and add them to the environment vocabulary (Forth's "extensibility") permit the programmer to craft solution-oriented code tailored to the specific programming problem at hand. The highest level of Forth code will resemble an English-language description of the application. Forth has been called a “meta-application language” — a language that can be used to create problem-oriented languages.
Most programming environments with recursive subroutines use a stack for control flow. This structure typically also stores local variables, including subroutine parameters (in call by value system such as C). Forth often does not have local variables, however, nor is it call-by-value. Instead, intermediate values are kept in another stack, different from the one it uses for return addresses, loop counters, etc. Words operate directly on the topmost values in the first of these two stacks. It may, therefore, be called the "parameter" or "data" stack, but most often simply "the" stack. The second, function-call stack is then called the "linkage" or "return" stack, abbreviated "rstack". Special rstack manipulation functions provided by the kernel allow it to be used for temporary storage within a word, and it is often used by counted loops, but otherwise it cannot be used to pass parameters or manipulate data.
Most words are specified in terms of their effect on the stack. Typically, parameters are placed on the top of the stack before the word executes. After execution, the parameters have been erased and replaced with any return values. For arithmetic operators, this follows the rule of reverse Polish notation. See below for examples illustrating stack usage.
Forth has been used successfully in large, complex projects, while applications developed by competent, disciplined professionals have proven to be easily maintained on evolving hardware platforms over decades of use. Forth has a niche both in astronomical and space applications. Forth is still used today in many embedded systems (small computerized devices) because of its portability, efficient memory use, short development times, and high execution speed. It has been implemented efficiently on modern reduced instruction set computers, and processors that use Forth as machine language have been produced. Other uses of Forth include the Open Firmware boot ROMs used by Apple, IBM, Sun, and OLPC XO-1.
Forth evolved from Charles H. Moore's personal programming system, which had been in continuous development since 1968. Forth was first exposed to other programmers in the early 1970s, starting with Elizabeth Rather at the United States National Radio Astronomy Observatory (NRAO). After their work at NRAO, Charles Moore and Elizabeth Rather formed FORTH, Inc. in 1973, refining and porting Forth systems to dozens of other platforms in the next decade.
Forth is so-named, because in 1968 "the file holding the interpreter was labeled FOURTH, for 4th (next) generation software, but the IBM 1130 operating system restricted file names to five characters." Moore saw Forth as a successor to compile-link-go third-generation programming languages, or software for "fourth generation" hardware, not a fourth-generation programming language as the term has come to be used.
As Charles Moore frequently moved from job to job over his career, an early pressure on the developing language was ease of porting to different computer architectures. A Forth system has often been used to bring up new hardware. For example, Forth was the first resident software on the new Intel 8086 chip in 1978 and MacFORTH was the first resident development system for the 128K Macintosh in 1984.
FORTH, Inc.'s microFORTH was developed for the Intel 8080, Motorola 6800, and Zilog Z80 microprocessors, starting in 1976. MicroFORTH was later used by hobbyists to generate Forth systems for other architectures, such as the 6502 in 1978. Wide dissemination finally led to standardization of the language. Common practice was codified in the de facto standards FORTH-79 and FORTH-83 in the years 1979 and 1983, respectively. These standards were unified by ANSI in 1994, commonly referred to as ANS Forth.
Forth became popular in the 1980s because it was well suited to the small microcomputers of that time, being compact and portable. Forth is also easy to implement, leading to a large number of implementations. At least one home computer, the British Jupiter ACE, had Forth in its ROM-resident operating system. The Canon Cat also used Forth for its system programming, and Rockwell produced single-chip microcomputers with resident Forth kernels, the R65F11 and R65F12. Insoft GraFORTH is a version of Forth with graphics extensions for the Apple II. ASYST was a Forth expansion for measuring and controlling on PCs.
As of 2018, the source for the original 1130 version of FORTH has been recovered, and is now being updated to run on a restored or emulated 1130 system.
Forth relies heavily on explicit use of a data stack and reverse Polish notation (RPN or postfix notation), commonly used in calculators from Hewlett-Packard. In RPN, the operator is placed after its operands, as opposed to the more common infix notation where the operator is placed between its operands. Postfix notation makes the language easier to parse and extend; Forth's flexibility makes a static BNF grammar inappropriate, and it does not have a monolithic compiler. Extending the compiler only requires writing a new word, instead of modifying a grammar and changing the underlying implementation.
Using RPN, one could get the result of the mathematical expression codice_1 this way:
This command line first puts the numbers 25 and 10 on the implied stack.
<br>The word codice_2 multiplies the two numbers on the top of the stack and replaces them with their product.
Then the number 50 is placed on the stack.
<br>The word codice_3 adds it to the previous product. The codice_4 moves the output to a new line (it is only for formatting purposes and could be omitted but—in most implementations—without it the output would occur on the same line as the input and would be less readable in the example). Finally, the codice_5 command prints the result to the user's terminal. As everything has completed successfully at that point, the text interpreter then outputs the prompt codice_6 and moves to a new line to get more input without needing anything explicit to do that.
Even Forth's structural features are stack-based. For example:
The colon indicates the beginning of a new definition, in this case a new word (again, "word" is the term used for a subroutine) called codice_7. The text in parentheses is a comment, advising that this word expects a number on the stack and will return a possibly changed number (on the stack).
The subroutine uses the following commands: codice_8 duplicates the number on the stack; codice_9 pushes a 6 on top of the stack; codice_10 compares the top two numbers on the stack (6 and the codice_8ed input), and replaces them with a true-or-false value; codice_12 takes a true-or-false value and chooses to execute commands immediately after it or to skip to the codice_13; codice_14 discards the value on the stack; codice_15 pushes a 5 on top of the stack; and codice_16 ends the conditional.
The codice_7 word is equivalent to this function written in the C programming language using the '?:'
int floor5(int v) {
This function is written more succinctly as:
You could run this word as follows:
First the interpreter pushes a number (1 or 8) onto the stack, then it calls codice_7 , which pops off this number again and pushes the result. The codice_4 moves the output to a new line (again, this is only here for readability). Finally, a call to codice_5 pops the result and prints it to the user's terminal.
Forth has no explicit grammar. The interpreter reads a line of input from the user input device, which is then parsed for a word using spaces as a delimiter; some systems recognise additional whitespace characters. When the interpreter finds a word, it looks the word up in the "dictionary". If the word is found, the interpreter executes the code associated with the word, and then returns to parse the rest of the input stream. If the word isn't found, the word is assumed to be a number and an attempt is made to convert it into a number and push it on the stack; if successful, the interpreter continues parsing the input stream. Otherwise, if both the lookup and the number conversion fail, the interpreter prints the word followed by an error message indicating the word is not recognised, flushes the input stream, and waits for new user input.
The definition of a new word is started with the word codice_21 (colon) and ends with the word codice_22 (semi-colon). For example,
will compile the word codice_23, and makes the name findable in the dictionary. When executed by typing codice_24 at the console this will print codice_25.
Most Forth systems include an assembler that allows one to specify words using the processor's facilities at its lowest level. Mostly the assembler is tucked away in a separate namespace ("wordlist") as relatively few users want to use it. Forth assemblers may use a reverse-polish syntax in which the parameters of an instruction precede the instruction, but designs vary widely and are specific to the Forth implementation. A typical reverse-polish assembler prepares the operands on the stack and have the mnemonic copy the whole instruction into memory as the last step. A Forth assembler is by nature a macro assembler, so that it is easy to define an alias for registers according to their role in the Forth system: e.g. "datastackpointer" for the register used as a stack pointer.
Most Forth systems run under a host operating system such as Microsoft Windows, Linux or a version of Unix and use the host operating system's file system for source and data files; the ANSI Forth Standard describes the words used for I/O. All modern Forth systems use normal text files for source, even if they are embedded. An embedded system with a resident compiler gets its source via a serial line.
Classic Forth systems traditionally use neither operating system nor file system. Instead of storing code in files, source code is stored in disk blocks written to physical disk addresses. The word codice_26 is employed to translate the number of a 1K-sized block of disk space into the address of a buffer containing the data, which is managed automatically by the Forth system. Block use has become rare since the mid-1990s. In a hosted system those blocks too are allocated in a normal file in any case.
Multitasking, most commonly cooperative round-robin scheduling, is normally available (although multitasking words and support are not covered by the ANSI Forth Standard). The word codice_27 is used to save the current task's execution context, to locate the next task, and restore its execution context. Each task has its own stacks, private copies of some control variables and a scratch area. Swapping tasks is simple and efficient; as a result, Forth multitaskers are available even on very simple microcontrollers, such as the Intel 8051, Atmel AVR, and TI MSP430.
Other non-standard facilities include a mechanism for issuing calls to the host OS or windowing systems, and many provide extensions that employ the scheduling provided by the operating system. Typically they have a larger and different set of words from the stand-alone Forth's codice_27 word for task creation, suspension, destruction and modification of priority.
A full-featured Forth system with all source code will compile itself, a technique commonly called meta-compilation or self-hosting, by Forth programmers (although the term doesn't exactly match meta-compilation as it is normally defined). The usual method is to redefine the handful of words that place compiled bits into memory. The compiler's words use specially named versions of fetch and store that can be redirected to a buffer area in memory. The buffer area simulates or accesses a memory area beginning at a different address than the code buffer. Such compilers define words to access both the target computer's memory, and the host (compiling) computer's memory.
After the fetch and store operations are redefined for the code space, the compiler, assembler, etc. are recompiled using the new definitions of fetch and store. This effectively reuses all the code of the compiler and interpreter. Then, the Forth system's code is compiled, but this version is stored in the buffer. The buffer in memory is written to disk, and ways are provided to load it temporarily into memory for testing. When the new version appears to work, it is written over the previous version.
Numerous variations of such compilers exist for different environments. For embedded systems, the code may instead be written to another computer, a technique known as cross compilation, over a serial port or even a single TTL bit, while keeping the word names and other non-executing parts of the dictionary in the original compiling computer. The minimum definitions for such a Forth compiler are the words that fetch and store a byte, and the word that commands a Forth word to be executed. Often the most time-consuming part of writing a remote port is constructing the initial program to implement fetch, store and execute, but many modern microprocessors have integrated debugging features (such as the Motorola CPU32) that eliminate this task.
The basic data structure of Forth is the "dictionary" which maps "words" to executable code or named data structures. The dictionary is laid out in memory as a tree of linked lists with the links proceeding from the latest (most recently) defined word to the oldest, until a sentinel value, usually a NULL pointer, is found. A context switch causes a list search to start at a different leaf. A linked list search continues as the branch merges into the main trunk leading eventually back to the sentinel, the root.
There can be several dictionaries. In rare cases such as meta-compilation a dictionary might be isolated and stand-alone.
The effect resembles that of nesting namespaces and can overload keywords depending on the context.
A defined word generally consists of "head" and "body" with the head consisting of the "name field" (NF) and the "link field" (LF), and body consisting of the "code field" (CF) and the "parameter field" (PF).
Head and body of a dictionary entry are treated separately because they may not be contiguous. For example, when a Forth program is recompiled for a new platform, the head may remain on the compiling computer, while the body goes to the new platform. In some environments (such as embedded systems) the heads occupy memory unnecessarily. However, some cross-compilers may put heads in the target if the target itself is expected to support an interactive Forth.
The exact format of a dictionary entry is not prescribed, and implementations vary. However, certain components are almost always present, though the exact size and order may vary. Described as a structure, a dictionary entry might look this way:
The name field starts with a prefix giving the length of the word's name (typically up to 32 bytes), and several bits for flags. The character representation of the word's name then follows the prefix. Depending on the particular implementation of Forth, there may be one or more NUL ('\0') bytes for alignment.
The link field contains a pointer to the previously defined word. The pointer may be a relative displacement or an absolute address that points to the next oldest sibling.
The code field pointer will be either the address of the word which will execute the code or data in the parameter field or the beginning of machine code that the processor will execute directly. For colon defined words, the code field pointer points to the word that will save the current Forth instruction pointer (IP) on the return stack, and load the IP with the new address from which to continue execution of words. This is the same as what a processor's call/return instructions do.
The compiler itself is not a monolithic program. It consists of Forth words visible to the system, and usable by a programmer. This allows a programmer to change the compiler's words for special purposes.
The "compile time" flag in the name field is set for words with "compile time" behavior. Most simple words execute the same code whether they are typed on a command line, or embedded in code. When compiling these, the compiler simply places code or a threaded pointer to the word.
The classic examples of compile-time words are the control structures such as codice_12 and codice_30. Almost all of Forth's control structures and almost all of its compiler are implemented as compile-time words. Apart from some rarely used control flow words only found in a few implementations, such as a conditional return, all of Forth's control flow words are executed during compilation to compile various combinations of primitive words along with their branch addresses. For instance, codice_12 and codice_30, and the words that match with those, set up codice_33 (unconditional branch) and codice_34 (pop a value off the stack, and branch if it is false). Counted loop control flow words work similarly but set up combinations of primitive words that work with a counter, and so on. During compilation, the data stack is used to support control structure balancing, nesting, and back-patching of branch addresses. The snippet:
would be compiled to the following sequence inside a definition:
The numbers after codice_33 represent relative jump addresses. codice_36 is the primitive word for pushing a "literal" number onto the data stack.
The word codice_21 (colon) parses a name as a parameter, creates a dictionary entry (a "colon definition") and enters compilation state. The interpreter continues to read space-delimited words from the user input device. If a word is found, the interpreter executes the "compilation semantics" associated with the word, instead of the "interpretation semantics". The default compilation semantics of a word are to append its interpretation semantics to the current definition.
The word codice_22 (semi-colon) finishes the current definition and returns to interpretation state. It is an example of a word whose compilation semantics differ from the default. The interpretation semantics of codice_22 (semi-colon), most control flow words, and several other words are undefined in ANS Forth, meaning that they must only be used inside of definitions and not on the interactive command line.
The interpreter state can be changed manually with the words codice_40 (left-bracket) and codice_41 (right-bracket) which enter interpretation state or compilation state, respectively. These words can be used with the word codice_42 to calculate a value during a compilation and to insert the calculated value into the current colon definition. codice_42 has the compilation semantics to take an object from the data stack and to append semantics to the current colon definition to place that object on the data stack.
In ANS Forth, the current state of the interpreter can be read from the flag codice_44 which contains the value true when in compilation state and false otherwise. This allows the implementation of so-called "state-smart words" with behavior that changes according to the current state of the interpreter.
The word codice_45 marks the most recent colon definition as an "immediate word", effectively replacing its compilation semantics with its interpretation semantics. Immediate words are normally executed during compilation, not compiled, but this can be overridden by the programmer in either state. codice_22 is an example of an immediate word. In ANS Forth, the word codice_47 takes a name as a parameter and appends the compilation semantics of the named word to the current definition even if the word was marked immediate. Forth-83 defined separate words codice_48 and codice_49 to force the compilation of non-immediate and immediate words, respectively.
In ANS Forth, unnamed words can be defined with the word codice_50 which compiles the following words up to the next codice_22 (semi-colon) and leaves an "execution token" on the data stack. The execution token provides an opaque handle for the compiled semantics, similar to the function pointers of the C programming language.
Execution tokens can be stored in variables. The word codice_52 takes an execution token from the data stack and performs the associated semantics. The word codice_53 (compile-comma) takes an execution token from the data stack and appends the associated semantics to the current definition.
The word codice_54 (tick) takes the name of a word as a parameter and returns the execution token associated with that word on the data stack. In interpretation state, codice_55 is equivalent to codice_56.
The words codice_21 (colon), codice_47, codice_54 (tick) are examples of "parsing words" that take their arguments from the user input device instead of the data stack. Another example is the word codice_60 (paren) which reads and ignores the following words up to and including the next right parenthesis and is used to place comments in a colon definition. Similarly, the word codice_61 (backslash) is used for comments that continue to the end of the current line. To be parsed correctly, codice_60 (paren) and codice_61 (backslash) must be separated by whitespace from the following comment text.
In most Forth systems, the body of a code definition consists of either machine language, or some form of threaded code. The original Forth which follows the informal FIG standard (Forth Interest Group), is a TIL (Threaded Interpretive Language). This is also called indirect-threaded code, but direct-threaded and subroutine threaded Forths have also become popular in modern times. The fastest modern Forths use subroutine threading, insert simple words as macros, and perform peephole optimization or other optimizing strategies to make the code smaller and faster.
When a word is a variable or other data object, the CF points to the runtime code associated with the defining word that created it. A defining word has a characteristic "defining behavior" (creating a dictionary entry plus possibly allocating and initializing data space) and also specifies the behavior of an instance of the class of words constructed by this defining word. Examples include:
Forth also provides a facility by which a programmer can define new application-specific defining words, specifying both a custom defining behavior and instance behavior. Some examples include circular buffers, named bits on an I/O port, and automatically indexed arrays.
Data objects defined by these and similar words are global in scope. The function provided by local variables in other languages is provided by the data stack in Forth (although Forth also has real local variables). Forth programming style uses very few named data objects compared with other languages; typically such data objects are used to contain data which is used by a number of words or tasks (in a multitasked implementation).
Forth does not enforce consistency of data type usage; it is the programmer's responsibility to use appropriate operators to fetch and store values or perform other operations on data.
Words written in Forth are compiled into an executable form. The classical "indirect threaded" implementations compile lists of addresses of words to be executed in turn; many modern systems generate actual machine code (including calls to some external words and code for others expanded in place). Some systems have optimizing compilers. Generally speaking, a Forth program is saved as the memory image of the compiled program with a single command (e.g., RUN) that is executed when the compiled version is loaded.
During development, the programmer uses the interpreter in REPL mode to execute and test each little piece as it is developed. Most Forth programmers therefore advocate a loose top-down design, and bottom-up development with continuous testing and integration.
The top-down design is usually separation of the program into "vocabularies" that are then used as high-level sets of tools to write the final program. A well-designed Forth program reads like natural language, and implements not just a single solution, but also sets of tools to attack related problems.
One possible implementation:
The word codice_4 (Carriage Return) causes the following output to be displayed on a new line. The parsing word codice_70 (dot-quote) reads a double-quote delimited string and appends code to the current definition so that the parsed string will be displayed on execution. The space character separating the word codice_70 from the string codice_72 is not included as part of the string. It is needed so that the parser recognizes codice_70 as a Forth word.
A standard Forth system is also an interpreter, and the same output can be obtained by typing the following code fragment into the Forth console:
codice_74 (dot-paren) is an immediate word that parses a parenthesis-delimited string and displays it. As with the word codice_70 the space character separating codice_74 from codice_72 is not part of the string.
The word codice_4 comes before the text to print. By convention, the Forth interpreter does not start output on a new line. Also by convention, the interpreter waits for input at the end of the previous line, after an codice_79 prompt. There is no implied "flush-buffer" action in Forth's codice_4, as sometimes is in other programming languages.
Here is the definition of a word codice_81 which when executed emits the single character codice_82:
This definition was written to use the ASCII value of the codice_82 character (81) directly. The text between the parentheses is a comment and is ignored by the compiler. The word codice_84 takes a value from the data stack and displays the corresponding character.
The following redefinition of codice_81 uses the words codice_40 (left-bracket), codice_41 (right-bracket), codice_88 and codice_42 to temporarily switch to interpreter state, calculate the ASCII value of the codice_82 character, return to compilation state and append the calculated value to the current colon definition:
The parsing word codice_88 takes a space-delimited word as parameter and places the value of its first character on the data stack. The word codice_92 is an immediate version of codice_88. Using codice_92, the example definition for codice_81 could be rewritten like this:
This definition used codice_61 (backslash) for the describing comment.
Both codice_88 and codice_92 are predefined in ANS Forth. Using codice_45 and codice_47, codice_92 could have been defined like this:
In 1987, Ron Rivest developed the RC4 cipher-system for RSA Data Security, Inc. The code is extremely simple and can be written by most programmers from the description:
We have an array of 256 bytes, all different. Every time the array is used it changes by swapping two bytes. The swaps are controlled by counters "i" and "j", each initially 0. To get a new "i", add 1. To get a new "j", add the array byte at the new "i". Exchange the array bytes at "i" and "j". The code is the array byte at the sum of the array bytes at "i" and "j". This is XORed with a byte of the plaintext to encrypt, or the ciphertext to decrypt. The array is initialized by first setting it to 0 through 255. Then step through it using "i" and "j", getting the new "j" by adding to it the array byte at "i" and a key byte, and swapping the array bytes at "i" and "j". Finally, "i" and "j" are set to 0. All additions are modulo 256.
The following Standard Forth version uses Core and Core Extension words only.
0 value ii 0 value jj
0 value KeyAddr 0 value KeyLen
create SArray 256 allot \ state array of 256 bytes
This is one of many ways to test the code:
hex
create AKey 61 c, 8A c, 63 c, D2 c, FB c,
AKey 5 rc4_init
2C F9 4C EE DC 5 test \ output should be: F1 38 29 C9 DE
Because the Forth virtual machine is simple to implement and has no standard reference implementation, there are numerous implementations of the language. In addition to supporting the standard varieties of desktop computer systems (POSIX, Microsoft Windows, Mac OS X), many of these Forth systems also target a variety of embedded systems. Listed here are some of the more prominent systems which conform to the 1994 ANS Forth standard.

</doc>
<doc id="11015" url="https://en.wikipedia.org/wiki?curid=11015" title="Francesco Algarotti">
Francesco Algarotti

Count Francesco Algarotti (11 December 1712 – 3 May 1764) was a Venetian polymath, philosopher, poet, essayist, anglophile, art critic and art collector. He was a man of broad knowledge, an expert in Newtonianism, architecture and opera. He was a friend of Frederick the Great and leading authors of his times: Voltaire, Jean-Baptiste de Boyer, Marquis d'Argens, Pierre-Louis de Maupertuis and the atheist Julien Offray de La Mettrie. Lord Chesterfield, Thomas Gray, George Lyttelton, Thomas Hollis, Metastasio, Benedict XIV and Heinrich von Brühl were among his correspondents.
Algarotti was born in Venice as the son of a rich merchant. His father and uncle were art collectors. Unlike his older brother Bonomo he did not step into the company, but decided to become an author. Francesco studied natural sciences and mathematics in Rome and Bologna under Francesco Maria Zanotti and in 1728 he experimented with optics. (Zanotti became a lifelong friend.) First he travelled in the North of Italy, but moved to Florence, and Rome. At the age of twenty, he went to Cirey and Paris, where he became friendly with Voltaire and Émilie du Châtelet. Two years later he was in London, where he was made a fellow of the Royal Society. He became embroiled in a lively bisexual love-triangle with the politician John Hervey, and Lady Mary Wortley Montagu. Algarotti left for Italy and finished his "Neutonianismo per le dame" ("Newtonism for Ladies") (1737 – dedicated to Bernard le Bovier de Fontenelle) – a work consisting of information on astronomy, physics, mathematics, women and science and education.
In the meantime Algarotti had made acquaintance with Antiochus Kantemir, a Moldavian diplomat, poet and composer. He was invited to visit Russia for the wedding of Duke Anthony Ulrich of Brunswick. In 1739 he left with Lord Baltimore from Sheerness to Newcastle upon Tyne. Because of a heavy storm the ship sheltered in Harlingen. Algarotti was discovering "this new city", which he called the great window ... to which Russia looks on Europe. Returning from Saint Petersburg, they visited Frederick the Great in Rheinsberg. Algarotti had obligations in England and came back the year after. Then Algarotti went together with Frederick to Königsberg where he was crowned.
Frederick, who was impressed with this walking encyclopedia, made him and his brother Bonomo Prussian counts in 1740. Algarotti accompanied Frederick to Bayreuth, Kehl, Strasbourg and Moyland Castle where they met with Voltaire, who was taking baths in Kleve for his health. In 1741 Algarotti went to Turin as his diplomat. Frederick had offered him a salary, but Algarotti refused. First, he went to Dresden and Venice, where he bought 21 paintings, a few by Jean-Étienne Liotard and Giovanni Battista Tiepolo for the court of Augustus III of Poland. Algarotti did not succeed in inducing the Kingdom of Sardinia to launch a treacherous attack upon Austria.
Algarotti's choice of works reflects the encyclopedic interests of the Neoclassic era; he was uninterested in developing a single unitary stylistic collection, and envisioned a modern museum, a catalog of styles from across the ages. For contemporary commissions, he wrote up a list of paintings he recommended commissioning, including history paintings from Tiepolo, Pittoni, and Piazzetta; scenes with animals from Castiglione, and veduta with ruins from Pannini. He wanted "suggetti graziosi e leggeri" from Balestra, Boucher, and Donato Creti. Other artists he supported were Giuseppe Nogari, Bernardo Bellotto, and Francesco Pavona.
In 1747 Algarotti went back to Potsdam and became court chamberlain, but left to visit the archeological diggings at Herculaneum. In 1749 he moved to Berlin. Algarotti was involved in finishing the architectural designs of Georg Wenzeslaus von Knobelsdorff who had fallen ill. In February 1753, after several years residing in Prussia, he returned to Italy, living most of the time in Bologna. In 1759 Algarotti was involved in a new opera-style in the city of Parma. He influenced Guillaume du Tillot and the Duke of Parma.
Algarotti's "Essay on the Opera" (1755) was a major influence on the librettist Carlo Innocenzo Frugoni and the composer Tommaso Traetta, and in the development of Gluck's reformist ideology. Algarotti proposed a heavily simplified model of "opera seria", with the drama pre-eminent, instead of the music, ballet or staging. The drama itself should "delight the eyes and ears, to rouse up and to affect the hearts of an audience, without the risk of sinning against reason or common sense". Algarotti's ideas influenced both Gluck and his librettist Calzabigi, writing their "Orfeo ed Euridice".
In 1762 Algarotti moved to Pisa, where he died of tuberculosis. Frederick the Great, who several times had needed Algarotti for writing texts in Latin, sent in a text for a monument to his memory on the Campo Santo.

</doc>
<doc id="11016" url="https://en.wikipedia.org/wiki?curid=11016" title="Francisco Álvares">
Francisco Álvares

Francisco Álvares ( – 1536-1541) was a Portuguese missionary and explorer. In 1515 he traveled to Ethiopia as part of the Portuguese embassy to emperor Lebna Dengel accompanied by returning Ethiopian ambassador Matheus. The embassy arrived only in 1520 to Ethiopia where he joined long sought Portuguese envoy Pêro da Covilhã. There he remained six years, returning to Lisbon in 1526-27 having written a report entitled "Verdadeira Informação das Terras do Preste João das Indias" ("A True Relation of the Lands of Prester John of the Indies").
Francisco Álvares was a chaplain-priest and almoner to King Manuel I of Portugal. He was sent in 1515 as part of the Portuguese embassy to the nəgusä nägäst (Emperor of Ethiopia), accompanied by the Ethiopian ambassador Matheus. Their first attempt to reach the port of Massawa failed due to the actions of Lopo Soares de Albergaria, governor of Portuguese India, which got no closer than the Dahlak Archipelago and was aborted with the death of the Portuguese ambassador, old Duarte Galvão at Kamaran. Álvares and Mattheus were forced to wait until the arrival of Soares' replacement, Diogo Lopes de Sequeira, who successfully sent the embassy on, with Dom Rodrigo de Lima replacing Duarte Galvão. The party at last reached Massawa on April 9, 1520, and reached the court of Lebna Dengel where he befriended several Europeans who had gained the favor of the Emperor, which included Pêro da Covilhã and Nicolao Branceleon. Father Álvares remained six years in Ethiopia, returning to Lisbon in either 1526 or 1527.
In 1533 he was allowed to accompany Dom Martinho de Portugal to Rome on an embassy to Pope Clement VII, to whom Father Álvares delivered the letter Lebna Dengel had written to the Pope. The precise date of Francisco Álvares death, like that of his birth, is unknown, but the writer of the 1911 Encyclopædia Britannica article concludes it was later than 1540, in which year an account of his travels were published at Lisbon. In the introduction of their translation of Álvares work, C.F. Beckingham and G.W.B. Huntingford furnish evidence that points to Álvares death in Rome, and admit that he may have died before his work was published.
In 1540, Luís Rodrigues published a version of Álvares account in a one volume folio, entitled "Verdadeira Informação das Terras do Preste João das Indias" ("A True Relation of the Lands of Prester John of the Indies"). C.F. Beckingham and G.W.B. Huntingford cite evidence, based in part on the earlier work of Professor Roberto Almagia, showing that Rodrigues's publication is only a part of Álvares's entire account. Another version of what Álvares wrote was included in an anthology of travel narratives, "Navigationi et Viaggi" assembled and published by Giovanni Battista Ramusio, and published in 1550. Almagia also identified three manuscripts in the Vatican Library which contain versions of excerpts from the original manuscript.
Francisco Álvares' work has been translated into English at least twice. The first time was the work of the Henry Stanley, 3rd Baron Stanley of Alderley for the Hakluyt Society in 1881. This translation was revised and augmented with notes by C. F. Beckingham and G. W. B. Huntingford, "The Prester John of the Indies" (Cambridge: Hakluyt Society, 1961).
The author of the 1911 Encyclopædia Britannica article was critical of the information it contained, believing it should "be received with caution, as the author is prone to exaggerate, and does not confine himself to what came within his own observation." Beckingham and Huntingford, however, have a higher opinion of Álvares testimony, stating that not only is it "incomparably more detailed than any earlier account of Ethiopia that has survived; it is also a very important source for Ethiopian history, for it was written just before the country was devastated by the Muslim Somali and pagan Galla invasions of the second quarter of the sixteenth century." He provides the first recorded and detailed descriptions of Axum and Lalibela. They continue...

</doc>
<doc id="11017" url="https://en.wikipedia.org/wiki?curid=11017" title="Francesco Andreini">
Francesco Andreini

Francesco Andreini (c. 1548 – 1624) was an Italian actor mainly of commedia dell'arte plays. He began his career playing the role of the unsophisticated love-stricken young man. Later he played the role of Capitan Spavento ("Captain Fright"), a Pickwickian character of excessive fatigue
He published his dialogue as Captain Spavento as "La bravura del capitano Spavento." This dialogue takes place between the captain and his servant, Trappola. 
Andreini was born at Pistoia. He was a member of the company of i Gelosi which Henry IV of France summoned to Paris to his bride, the young queen Marie de Medici, thus introducing the commedia dell'arte style to France.
Andreini married sixteen-year-old Isabella Andreini in 1578, when he was 30. She and their son, Giambattista Andreini, were also distinguished in the arts.

</doc>
<doc id="11018" url="https://en.wikipedia.org/wiki?curid=11018" title="Fifth Monarchists">
Fifth Monarchists

The Fifth Monarchists or Fifth Monarchy Men were an extreme Puritan sect active from 1649 to 1660 during the Commonwealth, following the English Civil Wars of the 17th century. They took their name from a prophecy in the Book of Daniel that four ancient monarchies (Babylonian, Persian, Macedonian, and Roman) would precede the kingdom of Christ. They also referred to the year 1666 and its relationship to the biblical Number of the Beast indicating the end of earthly rule by carnal human beings. They were one of a number of nonconformist dissenting groups that emerged around this time.
After the Restoration on 14 October 1660 Major-General Thomas Harrison was the first person to be found guilty of the regicide of Charles I. He had been the seventeenth of fifty-nine commissioners (judges) to sign the death warrant of the king in 1649. He was the first regicide to be hanged, drawn and quartered because he was considered by the new government to represent a continued real threat to the re-established order. This threat was realised when, on 6 January 1661, fifty Fifth Monarchists, headed by a wine-cooper named Thomas Venner, and thought to have been hiding out in Norton Folgate, made an effort to attain possession of London in the name of "King Jesus". Most of the fifty were either killed or taken prisoner, and on 19 and 21 January Venner and ten others were hanged, drawn and quartered for high treason.
The failure of Venner's Rising led to repressive legislation to suppress non-conformist sects. Although some physical events such as the Great Plague of London and the Great Fire of London continued to encourage belief in "the end of the world" ruled by carnal human beings, the doctrine of the sect either died out or became merged in a milder form of Millenarianism.

</doc>
<doc id="11020" url="https://en.wikipedia.org/wiki?curid=11020" title="February 15">
February 15


</doc>
<doc id="11021" url="https://en.wikipedia.org/wiki?curid=11021" title="February 6">
February 6


</doc>
<doc id="11022" url="https://en.wikipedia.org/wiki?curid=11022" title="Francis Hopkinson">
Francis Hopkinson

Francis Hopkinson (September 21, 1737 – May 9, 1791) was an author and composer. He designed Continental paper money, the first United States coin, and two early versions of the American flag, one for the United States and one for the United States Navy. He was also one of the signers of the Declaration of Independence in July 1776, as a delegate from New Jersey. He served in various roles in the early United States government including as a member of the Second Continental Congress and as a member of the Navy Board. He later became the first federal judge of the Eastern District Court of Pennsylvania on September 30, 1789.
Born on October 2, 1737 (Gregorian), September 21, 1737 (Julian) in Philadelphia, Province of Pennsylvania, British America, Hopkinson received an Artium Baccalaureus degree in 1757 from the College of Philadelphia (now the University of Pennsylvania) and an Artium Magister degree in 1760 from the same institution. He was the first native American composer of a secular song in 1759. He was Secretary of a Commission of the Provincial Council of Pennsylvania which made a treaty between the Province and certain Indian tribes in 1761. He entered private practice in Philadelphia, Province of Pennsylvania from 1761 to 1766. He was Collector of Customs in Salem, Province of New Jersey in 1763. Hopkinson spent from May 1766 to August 1767 in England in hopes of becoming Commissioner of Customs for North America. Although unsuccessful, he spent time with the future Prime Minister Lord North, Hopkinson's cousin James Johnson and the painter Benjamin West. He was a merchant in Philadelphia, Province of Pennsylvania, who sold varieties of fabric and port wine, starting in 1768. Additionally, in 1768 he was elected to the revived American Philosophical Society, and would go on to serve as the Society's Curator from 1776 to 1782. He was Collector of Customs for New Castle, Delaware Colony from 1772 to 1773. He resumed private practice in Bordentown from 1773 to 1774. He was a member of the New Jersey Provincial Council from 1774 to 1776. He was a member of the Executive Council of New Jersey from January 13, 1775, to November 15, 1775. He was admitted to practice before the bar of the Supreme Court of New Jersey on May 8, 1775. He was elected an Associate Justice of that court in 1776, but declined the office. He was a delegate to the Second Continental Congress (Continental Congress) from June 22, 1776, to November 30, 1776. He was a signer of the United States Declaration of Independence. He was a member of the Navy Board in Philadelphia from 1776 to 1777. He was Treasurer for the Continental Loan Office in Philadelphia from 1778 to 1781. He was Judge of the Admiralty Court of Pennsylvania from 1779 to 1789. He was a member of the Pennsylvania Convention which ratified the United States Constitution.
Hopkinson was nominated by President George Washington on September 24, 1789, to the United States District Court for the District of Pennsylvania, to a new seat authorized by . He was confirmed by the United States Senate on September 26, 1789, and received his commission the same day. His service terminated on May 9, 1791, due to his death in Philadelphia, Pennsylvania, of a sudden apoplectic seizure. He was interred in Christ Church Burial Ground in Philadelphia.
Hopkinson was the son of Thomas Hopkinson and Mary Johnson Hopkinson. He married Ann Borden on September 1, 1768. They would have five children. He was the father of Joseph Hopkinson, who was a member of the United States House of Representatives and also became a federal judge.
Hopkinson wrote popular airs and political satires ("jeux d'esprit") in the form of poems and pamphlets. Some were widely circulated, and powerfully assisted in arousing and fostering the spirit of political independence that issued in the American Revolution. His principal writings are "A Pretty Story . . ." (1774), a satire about King George, "The Prophecy" (1776), and "The Political Catechism" (1777).
Other notable essays are "Typographical Method of conducting a Quarrel", "Essay on White Washing", and "Modern Learning". Many of his writings can be found in "Miscellaneous Essays and Occasional Writings", published at Philadelphia in three volumes in 1792 (see Bibliography).
Hopkinson began to play the harpsichord at age seventeen and, during the 1750s, hand-copied arias, songs, and instrumental pieces by many European composers. He is credited as being the first American born composer to commit a composition to paper with his 1759 composition "My Days Have Been So Wondrous Free." By the 1760s he was good enough on the harpsichord to play with professional musicians in concerts. Some of his more notable songs include "The Treaty", "The Battle of the Kegs", and "The New Roof, a song for Federal Mechanics". He also played organ at Philadelphia's Christ Church and composed or edited a number of hymns and psalms including: "A Collection of Psalm Tunes with a few Anthems and Hymns Some of them Entirely New, for the Use of the United Churches of Christ Church and St. Peter's Church in Philadelphia" (1763), "A psalm of thanksgiving, Adapted to the Solemnity of Easter: To be performed on Sunday, the 30th of March, 1766, at Christ Church, Philadelphia" (1766), and "The Psalms of David, with the Ten Commandments, Creed, Lord's Prayer, &c. in Metre" (1767). In the 1780s, Hopkinson modified a glass harmonica to be played with a keyboard and invented the Bellarmonic, an instrument that utilized the tones of metal balls.
At his alma mater, University of Pennsylvania, one of the buildings in the Fisher-Hassenfeld College House is named after him.
On March 25, 1780, Congress created a second committee to design the Great Seal of the United States. Before he worked as a consultant to a committee working on the design of the Great Seal. Hopkinson had designed the Great Seal of New Jersey with assistance from Pierre Eugene du Simitiere in 1776. 
Fourteen men worked on the Great Seal, including two other consultants – Pierre Eugene du Simitiere (first Great Seal committee) and William Barton (third committee). The Seal was not finalized until June 20, 1782.
On today's Great Seal of the United States, the 13 stars (constellation) representing the 13 original states have five points. They are arranged in the shape of a larger star with six points. The constellation comprising 13 smaller stars symbolizes the national motto, "E pluribus unum." Originally, the design had individual stars with six points, but this was changed in 1841 when a new die was cast. This seal is now impressed upon the reverse of the United States one-dollar bill. The reverse of the seal, designed by William Barton, contains an unfinished pyramid below a radiant eye. The unfinished pyramid was an image used by Hopkinson when he designed the Continental $50 currency bill.
On Saturday, June 14, 1777, the Second Continental Congress adopted the Stars and Stripes as the first official national flag of the newly independent United States (later celebrated as Flag Day). The resolution creating the flag came from the Continental Marine Committee. Hopkinson became a member of the committee in 1776. At the time of the flag's adoption, he was the Chairman of the Navy Board, which was under the Marine Committee. Today, that office and responsibility/power would be residing in the United States Secretary of the Navy.
Hopkinson is recognized as the designer of the Flag of the United States, and the journals of the Continental Congress support this. On May 25, 1780, Hopkinson wrote a letter to the Continental Board of Admiralty mentioning several patriotic designs he had completed during the previous three years. One was his Board of Admiralty seal, which contained a shield of seven red and six white stripes on a blue field. Others included the Treasury Board seal, "7 devices for the Continental Currency," and "the Flag of the United States of America." Hopkinson noted that he hadn't asked for any compensation for the designs, but was now looking for a reward: "a Quarter Cask of the public Wine." The board sent that letter on to Congress. While the request for the wine might seem comical in today's world, in the Revolutionary War period, the quarter cask of wine would not be subject to inflation.
Hopkinson submitted another bill on June 24 for his "drawings and devices." In this second letter, Hopkinson did not mention designing the flag of the United States. Instead, the first item listed was "the great Naval Flag of the United States" along with the other contributions. This flag with its red outer stripes was designed to show up well on ships at sea. A parallel flag for the national flag was most likely intended by Hopkinson with white outer stripes as on the Great Seal of the United States and on the Bennington flag, which commemorated 50th anniversary of the founding of the United States (1826). Ironically, the Navy flag was preferred as the national flag. For the various designs, Hopkinson asked for cash in the amount of £2,700. The Auditor General, James Milligan, commissioned an evaluation of the request for payment. The report from the commissioner of the Chamber of Accounts said that the bill was reasonable and ought to be paid. Congress used the usual bureaucratic tactics of asking for an itemized bill for payment in cash. Hopkinson requested £9 for the naval flag. A committee investigated Hopkinson's charges that his payment was being delayed for arbitrary reasons. The Treasury Board turned down the request in an October 27, 1780, report to Congress. The Board cited several reasons for its action, including the fact that Hopkinson "was not the only person consulted on those exhibitions of Fancy [that were incidental to the Board (among them, the U.S. flag, the Navy flag, the Admiralty seal, and the Great Seal with a reverse)], and therefore cannot claim the sole merit of them and not entitled in this respect to the full sum charged." The reference to the work of others is most probably a reference to his work on the Great Seal. For that work, he was not the only contributor, but served as a consultant to the second committee that worked on the Great Seal of the United States. Therefore, he would not be eligible to be paid for the Great Seal. Furthermore, the Great Seal project was still a work in progress. No known committee of the Continental Congress was ever documented with the assignment to design the national flag or naval flag.. Hence, there was no evidence of collaboration with others on Hopkinson's flag design.
There is no known sketch of a Hopkinson flag—either U.S. or naval—in existence today. However, he incorporated elements of the two flags he designed in his rough sketches of the Great Seal of the United States and his design for the Admiralty Board Seal. The rough sketch of his second Great Seal proposal has 7 white stripes and 6 red stripes. The impression of Hopkinson's Admiralty Board Seal has a chevron with 7 red stripes and 6 white stripes. The Great Seal reflects Hopkinson's design for a governmental flag and the Admiralty Board Seal reflects Hopkinson's design for a naval flag. The predominance of red stripes made the naval flag more visible against the sky on a ship at sea. Both flags were intended to have 13 stripes. Because the original stars used in the Great Seal had six points, Hopkinson's U.S. flag might also have intended the use of 6-pointed stars. This is bolstered by his original sketch for the Great Seal that featured a U.S. flag with six-pointed asterisks for stars.

</doc>
<doc id="11023" url="https://en.wikipedia.org/wiki?curid=11023" title="Honorius (emperor)">
Honorius (emperor)

Flavius Honorius (9 September 384 – 15 August 423) was Roman emperor from 393 to 423. He was the younger son of emperor Theodosius I and his first wife Aelia Flaccilla, and brother of Arcadius, who ruled the eastern half of the empire from 395, when their father died, until his death in 408. In 410, during Honorius's reign over the western Roman Empire, Rome was sacked for the first time in almost 800 years.
Even by the standards of the rapidly declining Western Empire, Honorius's reign was precarious and chaotic. His reign was supported by his principal general, Stilicho, who was successively Honorius's guardian (during his childhood) and his father-in-law (after the emperor became an adult). Stilicho's generalship helped preserve some level of stability, but with his execution in 408, the western Roman Empire moved closer to collapse.
After holding the consulate at the age of two, Honorius was declared "Augustus" by his father Theodosius I, and thus co-ruler, on 23 January 393 after the death of Valentinian II and the usurpation of Eugenius. When Theodosius died, in January 395, Honorius and Arcadius divided the Empire, so that Honorius became Western Roman Emperor at the age of ten.
During the early part of his reign Honorius depended on the military leadership of the general Stilicho, who had been appointed by Theodosius and was of mixed Vandal and Roman ancestry. To strengthen his bonds with the young emperor, Stilicho married his daughter Maria to him. The epithalamion written for the occasion by Stilicho's court poet Claudian survives. Honorius was also greatly influenced by the Popes of Rome, who sought to extend their influence through his youth and weak character. So it was that Pope Innocent I contrived to have Honorius write to his brother, condemning the deposition of John Chrysostom in 407.
At first Honorius based his capital in Milan, but when the Visigoths under King Alaric I entered Italy in 401 he moved his capital to the coastal city of Ravenna, which was protected by a ring of marshes and strong fortifications. While the new capital was easier to defend, it was poorly situated to allow Roman forces to protect Central Italy from the increasingly regular threat of barbarian incursions. It was significant that the Emperor's residence remained in Ravenna until the overthrow of the last western Roman Emperor in 476. That was probably the reason why Ravenna was chosen not only as the capital of the Ostrogothic Kingdom in Italy, but also for the seat of the Byzantine exarchs as well.
Honorius' reign was plagued by almost constant barbarian incursions into Gaul, Italy and Hispania. At the same time, a host of usurpers rose up due to the apparent inability of the Emperor to see to the Empire's defences.
The first crisis faced by Honorius was a revolt led by Gildo, the "Comes Africae" and "Magister utriusque militiae per Africam", in Northern Africa, which lasted for two years (397–398). It was eventually subdued by Stilicho, under the local command of Mascezel, the very brother of Gildo.
The next crisis was the Visigothic invasion of Italy in 402 under the formidable command of their king, Alaric. Stilicho was absent in Raetia in the latter months of 401, when Alaric, who was also the Eastern Empire's magister militum in Illyricum, suddenly marched with a large army through the Julian Alps and entered Italy.
Stilicho hurried back to protect Honorius and the legions of Gaul and Britain were summoned to defend Italy. Honorius, slumbering at Milan, was caught unaware and quickly fled to Asti, only to be pursued by Alaric, who marched into Liguria. Stilicho defeated Alaric at Pollentia, on the river Tanarus on Easter Day (6 April 402). Alaric retreated to Verona, where Stilicho attacked him again. The Visigoths, weakened, were allowed to retreat back to Illyricum. In 405 Stilicho met an invasion of Italy led across the Danube by Radagaisus. They brought devastation to the heart of the Empire, until Stilicho defeated them in 406 and recruited most of them into his forces. Then, in 405/6, a tribal confedration, composed of Suevians, Vandals, Alans and Goths crossed the frozen Rhine and invaded Gaul.
The situation in Britain was even more difficult. The British provinces were isolated, lacking support from the Empire, and the soldiers supported the revolts of Marcus (406–407), Gratian (407), and Constantine III. Constantine invaded Gaul in 407, occupying Arles, and while Constantine was in Gaul, his son Constans ruled over Britain. By 410, Britain was effectively told to look after its own affairs and expect no aid from Rome.
There was good reason for this as the western empire was effectively overstretched due to the massive invasion of Alans, Suebi and Vandals who, although they had been repulsed from Italy in 406, moved into Gaul on 31 December 406, and arrived in Hispania in 409. In early 408, Stilicho attempted to strengthen his position at court by marrying his second daughter, Thermantia, to Honorius after the death of the Empress Maria in 407 making Honorius the last Western Roman Emperor to have multiple wives. Another invasion by Alaric was prevented in 408 by Stilicho when he forced the Roman Senate to pay 4,000 pounds of gold to persuade the Goths to leave Italy.
Honorius, in the meantime, was at Bononia, on his way from Ravenna to Ticinum, when the news reached him of his brother's death in May 408. He at first was planning to go to Constantinople to help set up the court during the transition from Arcadius to Theodosius II. Summoned from Ravenna for advice, Stilicho advised Honorius not to go, and proceeded to go himself. In Stilicho's absence, a minister named Olympius gained the confidence of Honorius. He convinced the emperor that his Arian father-in-law was conspiring with the barbarians to overthrow him.
On his return to Ravenna, Honorius ordered the arrest and execution of Stilicho. With Stilicho's fall, Honorius moved against all of his former father-in-law's allies, killing and torturing key individuals and ordering the confiscation of the property of anyone who had borne any office while Stilicho was in command. Honorius's wife Thermantia, daughter of Stilicho, was taken from the imperial throne and given over to her mother; Eucherius, the son of Stilicho, was put to death. The purge also massacred the families of Stilicho's foederati troops, and they defected en masse to Alaric.
In 409, Alaric returned to Italy to claim more gold and land to settle in, as feudatory vassals of the Empire, which Stilicho had promised him. Honorius refused to fulfill his former general's promises and Alaric marched on Rome, which bought him off after a short siege with Rome on the verge of famine.
A palace revolution in Honorius' court led meanwhile to a change of ministers, and those hostile to the Goths were replaced by officers favorable to Alaric, who began peace negotiations. While the embassy was absent, a new change occurred at Ravenna, and Honorius disclaimed the peace which was on the verge of being concluded. The enraged Alaric returned to Rome and forced the Senate to elect Priscus Attalus as emperor, who ratified Alaric's former treaty with Stilicho.
In 410, the Eastern Roman Empire sent six legions (6,000 men; due to changes in tactics, legions of this period were about 1,000 soldiers, down from the 6,000-soldier legions of the Republic and early Empire periods) from Ravenna to aid Honorius, but Alaric ambushed the legions on the way, and only a handful of them reached Rome. To counter Attalus, Honorius tried to negotiate with Alaric in addition to restricting grain shipments to Rome from North Africa. Attalus dispatched an army to conquer Africa and restore the grain supply to Rome, but the governor, Heraclian, who was loyal to Honorius, wiped out this force as soon as it landed on the coast. As Rome was dependent on North African grain for sustenance, the populace was faced with the prospect of famine, and they blamed Attalus for the impending calamity. Growing desperate, Attalus searched for means of pacifying the people, but found himself, in consequence of conciliatory expenditures, incapable of satisfying his debt to Alaric, and thus alienated both Romans and Goths. In turn he came out to be exploited in political terms.
Confronted with the increasing unpopularity and truculence of Attalus, Alaric dethroned him in 410 and proposed to renew negotiations with Honorius. Honorius, overconfident at Attalus' fall and the victory of his general Heraclian over Attalus' African expeditionary force, refused negotiation, and declared Alaric the eternal enemy of the Republic. The infuriated Alaric turned on the defenseless Rome and sacked the city.
The revolt of Constantine III in the west continued through this period. In 409, Gerontius, Constantine III's general in Hispania, rebelled against him, proclaimed Maximus Emperor, and besieged Constantine at Arles. Honorius now found himself an able commander, Constantius, who defeated Maximus and Gerontius, and then Constantine, in 411.
Gaul was again a source of troubles for Honorius: just after Constantius's troops had returned to Italy, Jovinus revolted in northern Gaul, with the support of Alans, Burgundians, and the nobility of Gallic descent. Jovinus tried to negotiate with the invading Goths of Ataulf (412), but his proclamation of his brother Sebastianus as Augustus made Ataulf seek alliance with Honorius. Honorius had Ataulf defeat and execute Jovinus in 413. At the same time, Heraclianus raised the standard of revolt in North Africa, but failed during an invasion of Italy. Defeated, he fled back to Carthage and was killed.
In 414, Constantius attacked Ataulf, who proclaimed Priscus Attalus emperor again. Constantius drove Ataulf into Hispania, and Attalus, having again lost Visigoth support, was captured and deposed once again. In the eleventh consulship of Honorius and the second of Constantius, the Emperor entered Rome in triumph, with Attalus at the wheels of his chariot. Honorius punished Attalus by cutting off his right finger and thumb, inflicting the same fate with which Attalus had threatened Honorius. Remembering how Attalus had suggested that Honorius should retire to some small island, he returned the favor by banishing Attalus to the island of Lipara.
Northeastern Gaul became subject to even greater Frankish influence, while a treaty signed in 418 granted to the Visigoths southwestern Gaul, the former Gallia Aquitania. Under the influence of Constantius, Honorius issued the Edict of 418, which was designed to enable the Empire to retain a hold on the lands which were to be surrendered to the Goths. This edict relaxed the administrative bonds that connected all the Seven Provinces (The Maritime Alps, Narbonensis Prima, Narbonensis Secunda, Novempopulania, Aquitania Prima, Aquitania Secunda and Viennensis) with the central government. It removed the imperial governors and allowed the inhabitants, as a dependent federation, to conduct their own affairs, for which purpose representatives of all the towns were to meet every year in Arles.
In 417, Constantius married Honorius's sister, Galla Placidia, much against her will. In 421, Honorius recognized him as co-emperor Constantius III; however, when the announcement of his elevation was sent to Constantinople, Theodosius refused to recognise him. Constantius, enraged, began preparations for a military conflict with the eastern empire but before he could commence it, he died early in 422.
In 420–422, another Maximus (or perhaps the same) gained and lost power in Hispania. By the time of Honorius's death in 423, Britain, Spain and Gaul had been ravaged by barbarians. In his final years, Honorius reportedly developed a physical attraction to his half-sister, and in order to escape his unwelcome attentions, Galla Placidia and her children, the future emperor Valentinian III and his sister, Honoria, fled to Constantinople.
Honorius died of edema on 15 August 423, leaving no heir. In the subsequent interregnum Joannes was nominated Emperor. The following year, however, the Eastern Emperor Theodosius II installed his cousin Valentinian III, son of Galla Placidia and Constantius III, as Emperor.
The Mausoleum of Honorius was located on the Vatican Hill, accessed from the transept of the Old Saint Peter's Basilica. It was first used for Maria. Probably Thermantia and Honorius's sister Galla Placidia, and perhaps other imperial family members, were later buried there. In the 8th century it was transformed into a church, the Chapel of St Petronilla, which held the relics of the saint and was . The mausoleum was demolished when the New St Peter's was erected.
The most notable event of his reign was the assault and Sack of Rome on 24 August 410 by the Visigoths under Alaric.
The city had been under Visigothic siege since shortly after Stilicho's deposition and execution in the summer of 408. Lacking a strong general to control the by-now mostly Germanic Roman army, Honorius could do little to attack Alaric's forces directly, and apparently adopted the only strategy he could in the situation: wait passively for the Visigoths to grow weary and spend the time marshalling what forces he could. Unfortunately, this course of action appeared to be the product of Honorius' indecisive character and he suffered much criticism for it both from contemporaries and later historians.
Whether this plan could have worked is perhaps debatable. In any case, it was overtaken by events. Stricken by starvation, somebody opened Rome's defenses to Alaric and the Goths poured in. The city had not been under the control of a foreign force since an invasion of Gauls some eight centuries before. The sack itself was notably mild as sacks go. For example, churches and religious statuary went unharmed. The psychological blow to the contemporary Roman world was considerably more painful. The shock of this event reverberated from Britain to Jerusalem, and inspired Augustine to write his magnum opus, "The City of God".
The year 410 also saw Honorius reply to a British plea for assistance against local barbarian incursions, called the "Rescript of Honorius". Preoccupied with the Visigoths, Honorius lacked any military capability to assist the distant province. According to the sixth century Byzantine scholar Zosimus, "Honorius wrote letters to the cities in Britain, bidding them to guard themselves." This sentence is located randomly in the middle of a discussion of southern Italy; no further mention of Britain is made, which has led some modern academics to suggest that the rescript does not apply to Britain, but to Bruttium in Italy.
In his "History of the Wars", Procopius mentions a story (which Edward Gibbon disbelieved) where, on hearing the news that Rome had "perished", Honorius was initially shocked; thinking the news was in reference to a favourite chicken he had named "Roma".
"At that time they say that the Emperor Honorius in Ravenna received the message from one of the eunuchs, evidently a keeper of the poultry, that Rome had perished. And he cried out and said, 'And yet it has just eaten from my hands!' For he had a very large cock, Rome by name; and the eunuch comprehending his words said that it was the city of Rome which had perished at the hands of Alaric, and the emperor with a sigh of relief answered quickly: 'But I thought that my fowl Rome had perished.' So great, they say, was the folly with which this emperor was possessed."<br>
—Procopius, "The Vandalic War (III.2.25–26)"
Summarising his account of Honorius's reign, the historian J.B. Bury wrote: "His name would be forgotten among the obscurest occupants of the Imperial throne were it not that his reign coincided with the fatal period in which it was decided that western Europe was to pass from the Roman to the Teuton." After listing the disasters of those 28 years, Bury concluded:"[Honorius] himself did nothing of note against the enemies who infested his realm, but personally he was extraordinarily fortunate in occupying the throne till he died a natural death and witnessing the destruction of the multitude of tyrants who rose up against him."
Honorius issued a decree during his reign, prohibiting men from wearing trousers in Rome. The last known gladiatoral games took place during the reign of Honorius, who banned the practice in 399 and again in 404, reportedly due to the martyrdom of a Christian monk named Telemachus while he was protesting a gladiator fight.

</doc>
<doc id="11024" url="https://en.wikipedia.org/wiki?curid=11024" title="Formant">
Formant

In speech science and phonetics, a formant is the broad spectral maximum that results from an acoustic resonance of the human vocal tract. In acoustics, a formant is usually defined as a broad peak, or local maximum, in the spectrum. For harmonic sounds, with this definition, the formant frequency is sometimes taken as that of the harmonic partial that is most augmented by a resonance. The difference between these two definitions resides in whether "formants" characterise the production mechanisms of a sound or the produced sound itself. In practice, the frequency of a spectral peak differs from the associated resonance frequency, except when, by luck, harmonics are aligned with the resonance frequency.
A room can be said to have formants characteristic of that particular room, due to the way sound reflects from its walls and objects. Room formants of this nature reinforce themselves by emphasizing specific frequencies and absorbing others, as exploited, for example, by Alvin Lucier in his piece "I Am Sitting in a Room".
From an acoustic point of view, phonetics had a serious problem with the idea that the effective length of vocal tract changed vowels. It was unclear how they could depend on frequencies when everyone from bass to soprano can make the same vowels. There had to be some way to normalize the frequencies. Hermann suggested a solution to this problem in 1894, coining the term “formant”. A vowel, according to him, is a special acoustic phenomenon, depending on the intermittent production of a special partial, or “formant”, or “characteristique”. The frequency of the “formant” may vary a little without altering the character of the vowel. For "a", for example, the “formant” may vary from 350 to 440 Hz even in the same person.
Formants are distinctive frequency components of the acoustic signal produced by speech or singing. The information that humans require to distinguish between speech sounds can be represented purely quantitatively by specifying peaks in the amplitude or frequency spectrum.
Most of these formants are produced by tube and chamber resonance, but a few whistle tones derive from periodic collapse of Venturi effect low-pressure zones. The formant with the lowest frequency is called "F", the second "F", and the third "F". Most often the two first formants, "F" and "F", are sufficient to identify the vowel. The relationship between the perceived vowel quality and the first two formant frequencies can be appreciated by listening to "artificial vowels" that are generated by passing a click train (to simulate the glottal pulse train) through a pair of bandpass filters (to simulate vocal tract resonances).
Nasal consonants usually have an additional formant around 2500 Hz. The liquid usually has an extra formant at 1500 Hz, whereas the English "r" sound () is distinguished by a very low third formant (well below 2000 Hz).
Plosives (and, to some degree, fricatives) modify the placement of formants in the surrounding vowels. Bilabial sounds (such as and in "ball" or "sap") cause a lowering of the formants; velar sounds ( and in English) almost always show "F" and "F" coming together in a 'velar pinch' before the velar and separating from the same 'pinch' as the velar is released; alveolar sounds (English and ) cause fewer systematic changes in neighbouring vowel formants, depending partially on exactly which vowel is present. The time course of these changes in vowel formant frequencies are referred to as 'formant transitions'.
If the fundamental frequency of the underlying vibration is higher than a resonance frequency of the system, then the formant usually imparted by that resonance will be mostly lost. This is most apparent in the example of soprano opera singers, who sing high enough that their vowels become very hard to distinguish.
Control of resonances is an essential component of the vocal technique known as overtone singing, in which the performer sings a low fundamental tone, and creates sharp resonances to select upper harmonics, giving the impression of several tones being sung at once.
Spectrograms may be used to visualise formants. In spectrograms, it can be hard to distinguish formants from naturally occurring harmonics when one sings. However, one can hear the natural formants in a vowel shape through atonal techniques such as vocal fry.
Formants, whether they are seen as acoustic resonances of the vocal tract, or as local maxima in the speech spectrum, like band-pass filters, are defined by their frequency and by their spectral width.
Different methods exist to obtain these informations. Formant frequencies, in their acoustic definition, can be estimated from the frequency spectrum of the sound, using a spectrogram (in the figure) or a spectrum analyzer. However, to estimate the acoustic resonances of the vocal tract (i.e. the speech definition of formants) from a speech recording, one can use "linear predictive coding". An intermediate approach consists in extracting the spectral envelope by neutralizing the fundamental frequency, and only then looking for local maxima in the spectral envelope.
The first two formants are important in determining the quality of vowels, and are frequently said to correspond to the open/close and front/back dimensions (which have traditionally, though not entirely accurately, been associated with the shape and position of the tongue). Thus the first formant "F" has a higher frequency for an open vowel (such as ) and a lower frequency for a close vowel (such as or ); and the second formant "F" has a higher frequency for a front vowel (such as ) and a lower frequency for a back vowel (such as ). as can be seen in Fig. 1. Vowels will almost always have four or more distinguishable formants; sometimes there are more than six. However, the first two formants are most important in determining vowel quality, and this is often displayed in terms of a plot of the first formant against the second formant, though this is not sufficient to capture some aspects of vowel quality, such as rounding. An example of how the vowels of a language or dialect may be plotted on a traditional auditory vowel chart and also on a formant plot may be seen in the case of Norwegian.
Many writers have addressed the problem of finding an optimal alignment of the positions of vowels on formant plots with those on the conventional vowel quadrilateral. The pioneering work of Ladefoged used the Mel scale because this scale was claimed to correspond more closely to the auditory scale of pitch than to the acoustic measure of fundamental frequency expressed in Hertz as in Fig. 1. Two alternatives to the Mel scale are the Bark scale and the ERB-rate scale. A comparison of these three scales is shown by Hayward, p. 141, and formant plots based on the Hertz scale and on the Bark scale are compared on p. 153. Another strategy for improving formant plots that has been widely adopted is to plot on the horizontal axis not the value of F2 but the difference between F1 and F2 for a given vowel.
Studies of the frequency spectrum of trained classical singers, especially male singers, indicate a clear formant around 3000 Hz (between 2800 and 3400 Hz) that is absent in speech or in the spectra of untrained singers. It is thought to be associated with one or more of the higher resonances of the vocal tract. It is this increase in energy at 3000 Hz which allows singers to be heard and understood over an orchestra. This formant is actively developed through vocal training, for instance through so-called "voce di strega" or "witch's voice" exercises and is caused by a part of the vocal tract acting as a resonator. In classical music and vocal pedagogy, this phenomenon is also known as "squillo".

</doc>
<doc id="11025" url="https://en.wikipedia.org/wiki?curid=11025" title="February 20">
February 20


</doc>
<doc id="11026" url="https://en.wikipedia.org/wiki?curid=11026" title="List of programmers">
List of programmers

This is a list of programmers notable for their contributions to software, either as original author or architect, or for later additions. All entries must already have associated articles.

</doc>
<doc id="11028" url="https://en.wikipedia.org/wiki?curid=11028" title="Film stock">
Film stock

Film stock is an analog medium that is used for recording motion pictures or animation. It is recorded on by a movie camera, developed, 
edited, and projected onto a screen using a movie projector. It is a strip or sheet of transparent plastic film base coated on one side with a gelatin emulsion containing microscopically small light-sensitive silver halide crystals. The sizes and other characteristics of the crystals determine the sensitivity, contrast and resolution of the film. The emulsion will gradually darken if left exposed to light, but the process is too slow and incomplete to be of any practical use. Instead, a very short exposure to the image formed by a camera lens is used to produce only a very slight chemical change, proportional to the amount of light absorbed by each crystal. This creates an invisible latent image in the emulsion, which can be chemically developed into a visible photograph. In addition to visible light, all films are sensitive to X-rays and high-energy particles. Most are at least slightly sensitive to invisible ultraviolet (UV) light. Some special-purpose films are sensitive into the infrared (IR) region of the spectrum.
In black-and-white photographic film there is usually one layer of silver salts. When the exposed grains are developed, the silver salts are converted to metallic silver, which blocks light and appears as the black part of the film negative. Color film has at least three sensitive layers. Dyes, which adsorb to the surface of the silver salts, make the crystals sensitive to different colors. Typically the blue-sensitive layer is on top, followed by the green and red layers. During development, the exposed silver salts are converted to metallic silver, just as with black-and-white film. But in a color film, the by-products of the development reaction simultaneously combine with chemicals known as color couplers that are included either in the film itself or in the developer solution to form colored dyes. Because the by-products are created in direct proportion to the amount of exposure and development, the dye clouds formed are also in proportion to the exposure and development. Following development, the silver is converted back to silver salts in the "bleach step". It is removed from the film in the "fix step" and is sometimes recovered for subsequent use or sale. Fixing leaves behind only the formed color dyes, which combine to make up the colored visible image. Later color films, like Kodacolor II, have as many as 12 emulsion layers, with upwards of 20 different chemicals in each layer. Photographic film and film stock tend to be similar in composition and speed, but often not in other parameters such as frame size and length.
Early motion picture experiments in the 1880s were performed using a fragile paper roll film, with which it was difficult to view a single, continuously moving image without a complex apparatus. The first transparent and flexible film base material was celluloid, which was discovered and refined for photographic use by John Carbutt, Hannibal Goodwin, and George Eastman. Eastman Kodak made celluloid film commercially available in 1889; Thomas Henry Blair, in 1891, was his first competitor. The stock had a frosted base to facilitate easier viewing by transmitted light. Emulsions were orthochromatic. By November 1891 William Dickson, at Edison's laboratory, was using Blair's stock for Kinetoscope experiments. Blair's company supplied film to Edison for five years. Between 1892 and 1893, Eastman experienced problems with production. Because of patent lawsuits in 1893, Blair left his American company and established another in Britain. Eastman became Edison's supplier of film.
Blair's new company supplied European filmmaking pioneers, including Birt Acres, Robert Paul, George Albert Smith, Charles Urban, and the Lumière Brothers. By 1896 the new movie projector required a fully transparent film base that Blair's American operation could not supply. Eastman shortly thereafter bought the company out and became the leading supplier of film stock. Louis Lumière worked with Victor Planchon to adapt the Lumière "Blue Label" (Etiquette Bleue) photographic plate emulsion for use on celluloid roll film, which began in early 1896.
Eastman's first motion picture film stock was offered in 1889. At first the film was the same as photographic film. By 1916, separate "Cine Type" films were offered. From 1895, Eastman supplied their motion picture roll film in rolls of 65 feet, while Blair's rolls were 75 feet. If longer lengths were needed, the unexposed negative rolls could be cemented in a darkroom, but this was largely undesirable by most narrative filmmakers. The makers of Actuality films were much more eager to undertake this method, however, in order to depict longer actions. They created cemented rolls as long as 1000 feet. American Mutoscope and Biograph was the first known company to use such film for the Jeffries-Sharkey fight on November 3, 1899.
As the quantity of film and filmmakers grew, the demand for standardization increased. Between 1900 and 1910, film formats gradually became standardized and film stocks improved. A number of film gauges were made. Eastman increased the length of rolls to 200 feet without major adjustments to the emulsion, retaining a large market share. Lumière reformulated its stock to match the speed of Eastman film, naming it 'Etiquette Violette' (Violet Label). Blair sold his English company to Pathé in 1907 and retired to the US. Pathe began to supplement its operation in 1910 by purchasing film prints, stripping the emulsion from the film base and re-coating it. 35mm film began to become the dominant gauge because of the popularity of Edison's and Lumière's cameras. Consumers usually purchased unperforated film and had to punch it by perforators that were often imprecise, causing difficulty in making prints for the opposite perforation format. In 1908, the perforators began to be made by Bell and Howell. Eastman Kodak used the Bell and Howell's machine to perforate its films. In 1909, Edison's organization of the Motion Picture Patents Trust agreed to what would become the standard: 35 mm gauge, with Edison perforations and a 1.33 aspect ratio.
Agfa began to produce motion picture film in 1913, but remained a largely local supplier until World War I boycotts of popular French, American and Italian film stocks allowed the UFA film studio to flourish, boosting Agfa's orders. All film stocks were manufactured on a nitrate film base, which is highly flammable. Nitrate film fires were difficult to extinguish. A significant number of fatal accidents occurred in theatrical projection booths, where the heat of the projector lamp made ignition most likely. Amateur filmmaking (home movies) slowly developed during this period. Kodak developed a heat-resistant 'safety base' for home projection.
In 1909, tests showed cellulose diacetate to be a viable replacement base, and Kodak began selling acetate-base films the following year in 22 mm widths for Edison's work on the Home Kinetoscope, which was commercially released in 1912. Eastman Kodak introduced a non-flammable 35 mm film stock in 1909. The plasticizers used to make the film flexible evaporated quickly, making the film dry and brittle, causing splices to part and perforations to tear. In 1911 the major American film studios returned to using nitrate stock. More amateur formats began to use acetate-based film, and several, including Kodak's own 16 mm format, were designed specifically to be manufactured with safety base. Kodak released Cine Negative Film Type E in 1916 and Type F (later known as Negative Film Par Speed Type 1201) in 1917. As both of these orthochromatic films were no faster than previous offerings, the improvements were in granularity and sharpness.
Film stock manufacturers began to diversify their products. Each manufacturer had previously offered one negative stock (usually orthochromatic) and one print stock. In 1920, a variant of Type F film known as X-back was introduced to counteract the effects of static electricity on the film, which can cause sparking and create odd exposure patterns on the film. A resin backing was used on the film, which rendered the film too opaque to allow focusing through the back of the film, a common technique for many cameras of that era. The X-back stock was popular on the east coast of the US. Other manufacturers were established in the 1920s, including American E.I. Dupont de Nemours in 1926 and Belgian Gevaert in 1925. Panchromatic film stock became more common. Created in 1913 for use in color film processes such as Kinemacolor, panchromatic was first used in a black-and-white film for exterior sequences in "Queen of the Sea" (1918) and originally available as a special order product. The stock's increased sensitivity to red light made it an attractive option for day for night shooting. Kodak financed a feature in 1922, shot entirely with panchromatic stock, "The Headless Horseman", to promote the film when Kodak introduced it as a standard option. Panchromatic film stock was expensive and no motion pictures were produced in entirety on it for several years. The cross-cutting between panchromatic and orthochromatic stocks caused continuity problems with costume tones and panchromatic film was often avoided.
Orthochromatic film remained dominant until the mid-1920s due to Kodak's lack of competition in the panchromatic market. In 1925, Gevaert introduced an orthochromatic stock with limited color sensitivity and a fully panchromatic stock, Pan-23. In 1926, Kodak lowered the price of panchromatic stock to parity with its orthochromatic offering and the panchromatic stock began to overtake the orthochromatic stock's market share within a few years. As similar panchromatic film stocks were also manufactured by Agfa and Pathé, making the shift to panchromatic stocks largely complete by 1928, Kodak discontinued orthochromatic stock in 1930.
Experiments with colour films were made as early as the late 19th century, but practical colour film was not commercially viable until 1908, and for amateur use when Kodak introduced Kodachrome for 16 mm in 1935 and 8 mm in 1936. Before 1941, commercially successful colour processes used special cameras loaded with black-and-white separation stocks rather than colour negative. Kinemacolor (1908–1914), Technicolor processes 1 through 4 (1917–1954), and Cinecolor used one, two or three strips of monochrome film stock sensitized to certain primary colours or exposed behind colour filters in special cameras. Technicolor introduced a colour reversal stock, called Monopack, for location shooting in 1941; it was ultimately a 35 mm version of Kodachrome that could be used in standard motion picture cameras.
Eastman Kodak introduced their first 35mm colour negative stock, Eastman Colour Negative film 5247, in 1950. A higher quality version in 1952, Eastman Colour Negative film 5248, was quickly adopted by Hollywood for colour motion picture production, replacing both the expensive three-strip Technicolor process and Monopack.
There are several variables in classifying stocks; in practice, one orders raw stock by a code number, based on desired sensitivity to light.
A piece of film consists of a light-sensitive emulsion applied to a tough, transparent base, sometimes attached to anti-halation backing or "rem-jet" layer (now only on camera films). Originally the highly flammable cellulose nitrate was used. In the 1930s, film manufacturers introduced "safety film" with a cellulose triacetate plastic base. All amateur film stocks were safety film, but the use of nitrate persisted for professional releases. Kodak discontinued the manufacture of nitrate base in 1951, and the industry transitioned entirely to safety film in 1951 in the United States and by 1955 internationally. Since the late 1990s, almost all release prints have used polyester film stock.
The emulsion consists of silver halide grains suspended in a gelatin colloid; in the case of color film, there are three layers of silver halide, which are mixed with color couplers and interlayers that filter specific light spectra. These end up creating yellow, cyan, and magenta layers in the negative after development.
Development chemicals applied to an appropriate film can produce either a positive (showing the same densities and colors as the subject) or negative image (with dark highlights, light shadows, and, in principle, complementary colors). The first films were darkened by light: negative films. Later films that produce a positive image became known as reversal films; processed transparent film of this type can be projected onto a screen. Negative images need to be transferred onto photographic paper or other substrate which reverses the image again, producing a final positive image. Creating a positive image from a negative film can also be done by scanning the negative to create a computer file which can then be reversed by software.
Different emulsions and development processes exist for a variety of image recording possibilities: the two most common of which are black and white, and color. However, there are also variant types, such as infrared film (in black and white or false color); specialist technical films, such as those used for X-rays; and obsolete processes, such as orthochromatic film. Generally, however, the vast majority of stock used today is "normal" (visible spectrum) color, although "normal" black and white also commands a significant minority percentage.
Film is also classified according to its gauge and the arrangement of its perforations— gauges range from 8 mm to 70 mm or more, while perforations may vary in shape, pitch, and positioning. The film is also distinguished by how it is wound with regard to perforations and base or emulsion side, as well as whether it is packaged around a core, a daylight spool, or within a cartridge. Depending on the manufacturing processes and camera equipment, lengths can vary anywhere from 25 to 2000 feet. Common lengths include 25 feet for 8 mm, 50 feet for Super 8, 100 and 400 feet for 16 mm, 400 and 1000 feet for 35 mm, and 1000 for 65/70 mm.
A critical property of a stock is its film speed, determined by ASA or its sensitivity to light listed by a measurement on the raw stock which must be chosen with care. Speed determines the range of lighting conditions under which the film can be shot, and is related to granularity and contrast, which influence the look of the image. The stock manufacturer will usually give an exposure index (EI) number equal to the ASA which they recommend exposing for. However, factors such as forced or non-standard development (such as bleach bypass or cross processing), compensation for filters or shutter angle, as well as intended under- and over-exposure may cause the cinematographer to actually "rate" the stock differently from the EI. This new rating is not a change to the stock itself — it is merely a way of calculating exposure without figuring out the compensation after each light reading.
Another important quality of colour film stock in particular is its colour balance, which is defined by the colour temperature at which it accurately records white. Tungsten lighting is defined at 3200 K, which is considered "warmer" in tone and shifted towards orange; daylight is defined at 5600 K, which is considered "colder" and shifted towards blue. This means that unfiltered tungsten stock will look normal shot under tungsten lights, but blue if shot during daylight. Conversely, daylight stock shot in daylight will look normal, but orange if shot under tungsten lights. Colour temperature issues such as these can be compensated for by other factors such as lens filters and colour gels placed in front of the lights. The colour temperature of a film stock is generally indicated next to the film speed number — e.g. 500T stock is colour film stock with an ASA of 500 and balanced for tungsten light; 250D would have an ASA of 250 and be balanced for daylight. While black-and-white film has no colour temperature itself, the silver halide grains themselves tend to be slightly more responsive to blue light, and therefore will have daylight and tungsten speeds — e.g. Kodak's Double-X stock is rated 250D/200T, since the tungsten light will give slightly less exposure than an equivalent amount of daylight.
All plastic is subject to deterioration through physical or chemical means, and thus, motion picture film is at risk for the same reason. Films deteriorate over time, which can damage individual frames or even lead to the entire film being destroyed. Cellulose nitrate, cellulose diacetate and triacetate are known to be unstable media: improperly preserved film can deteriorate in a period of time much faster than many photographs or other visual presentations. Cellulose nitrate, because of its unstable chemistry, eventually breaks down, releasing nitric acid, further catalyzing the decomposition. In the final stages of celluloid decomposition, the film turns into a rust-like powder. Likewise, tri-acetate stock is also vulnerable to deterioration. Because of the small gauge of the film, owners of home-made films often find that their film can become shrunken and brittle to the point where the film is unwatchable in the space of a few years. In general, decaying acetate film breaks down into acetic acid, and similar to celluloid decomposition, leads to an auto-catylictic breakdown of the base that cannot be reversed. The result of the acetic acid released is a strong odor of vinegar, which is why the decay process in the archival community is known as "vinegar syndrome". Modern polyester-based stocks are far more stable by comparison and are rated to last hundreds of years if stored properly.
The distinction between camera stocks and print stocks involves a difference in the recording process. When the work print or edit master has been approved, the Original Camera Negative (OCN) is assembled by a negative cutter using the edited work print or EDL (edit decision list) as a guide. A series of Answer Prints are then made from the OCN. During the Answer Print stage, corrections in the film's density and color are corrected (timed) to the filmmakers' tastes. Interpositive (IP) prints are struck from the OCN, checked to make sure they look the same as the custom timed Answer Print, and then each IP is used to make one or more Dupe Negative (DN) copies. The release prints are then generated from the DN(s). Recently, with the development of digital intermediate (DI), it has become possible to completely edit, composite visual effects, and color grade the image digitally at full resolution and bit-depth. In this workflow, the answer print is generated digitally and then written out to the IP stage using a laser film printer.
Due to the specialized nature of the exposure and the higher degree of control afforded by the film lab equipment, these intermediate and release stocks are specially designed solely for these applications and are generally not feasible for camera shooting. Because intermediates only function to maintain the image information accurately across duplication, each manufacturer tends to only produce one or two different intermediate stocks. Similarly, release print stocks usually are available only in two varieties: a "normal" print or a deluxe print (on more-costly print film like Kodak Vision Premiere) with slightly greater saturation and contrast.
Use of film remained the dominant form of cinematography until the early 21st century when digital formats supplanted the use of film in many applications. This has also led to the replacement of film projectors with digital projection.
Yet, digital videos are sometimes deliberately altered to achieve a film look, such as adding film grain or other noise for artistic effect.

</doc>
<doc id="11029" url="https://en.wikipedia.org/wiki?curid=11029" title="Form 1040">
Form 1040

Form 1040 (officially, the "U.S. Individual Income Tax Return") is an IRS tax form used for personal federal income tax returns filed by United States residents. The form calculates the total taxable income of the taxpayer and determines how much is to be paid or refunded by the government. 
Income tax returns for individual calendar year taxpayers are due by Tax Day, which is usually April 15 of the next year, except when April 15 falls on a Saturday, Sunday, or a legal holiday. In those circumstances, the returns are due on the next business day. An automatic extension until October 15 to file Form 1040 can be obtained by filing Form 4868.
Form 1040 consists of two pages (23 lines in total) not counting attachments. The first page collects information about the taxpayer(s) and dependents. In particular, the taxpayer specifies his/her filing status on this page. The second page reports income, calculates the allowable deductions and credits, figures the tax due given adjusted income, and applies funds already withheld from wages or estimated payments made towards the tax liability. On the right side of the first page is the presidential election campaign fund checkoff, which allows individuals to designate that the federal government give $3 of the tax it receives to the Presidential election campaign fund. Altogether, 142 million individual income tax returns were filed for the tax year 2018 (filing season 2019), 92% of which were filed electronically.
Form 1040 (or a variant thereof) is the main tax form filed by individuals who are deemed residents of the United States for tax purposes. The corresponding main form filed by businesses is Form 1120, also called the U.S. Corporation Income Tax Return.
An individual is considered a resident of the United States for tax purposes if he or she is a citizen of the United States or a resident alien of the United States for tax purposes. An individual is a resident alien of the United States if he or she passes either the Substantial Presence Test or the Green Card Test, although there are also some other cases; individuals who have taxable income in the United States but fail the criteria for being resident aliens must file as nonresident aliens for tax purposes. While residents of the United States for tax purposes file Form 1040, nonresident aliens must file Form 1040NR or 1040NR-EZ. There is also a "dual status alien" for aliens whose status changed during the year.
Resident aliens of the United States for tax purposes must generally file if their income crosses a threshold where their taxable income is likely to be positive, but there are many other cases where it may be legally desirable to file. For instance, even if not required, individuals can file a return in order to receive a refund on withheld income or to receive certain credits (e.g. earned income tax credit).
The form may be filed either by paper or online.
Paper filing is the universally accepted filing method. Form 1040, along with its variants, schedules, and instructions, can be downloaded as PDFs from the Internal Revenue Service website. Finalized versions of the forms for the tax year (which in the US is the same as the calendar year) are released near the end of January of the following year.
Paper forms can be filled and saved electronically using a compatible PDF reader, and then printed. This way, it is easy to keep electronic copies of one's filled forms despite filing by paper. Alternatively, they can be printed out and filled by hand. A combination of the approaches may also be used, with some content filled in electronically and additional content written in by hand. As a general rule, where possible, it makes sense to fill electronically, but in some cases filling by hand may be necessary (for instance, if additional notes of explanation need to be added, or the font used for electronic filling is too large to fit the information in the space provided.
The only parts of the form that cannot be filled electronically are the signature lines.
The paper Form 1040, along with all relevant schedules and additional forms, must be sent in a single packet by mail or courier to an IRS address determined by the US state the taxpayer is filing from and whether or not a payment is enclosed.
The IRS accepts returns that are stapled or paperclipped together. However, any check or payment voucher, as well as accompanying Form 1040-V, must "not" be stapled or paperclipped with the rest of the return, since payments are processed separately.
The IRS allows US residents for tax purposes to file electronically in three ways:
Many paid tax preparers are required to file individual tax returns electronically, and most tax compliance software file electronically on the taxpayer's behalf. Even tax preparers who are not so required, must file Form 8948 if they choose paper filing, providing an explanation for why they are not filing electronically.
If one is not eligible for IRS Free File, it might cost hundreds of dollars to file electronically, whereas paper filing has no costs beyond those of printing and mailing. Furthermore, the available existing electronic filing options may not offer sufficient flexibility with respect to arranging one's tax return, adding attachments, or putting written notes of explanation that can help preempt IRS questions. Filing electronically also exposes the taxpayer's data to the risk of accidental loss or identity theft.
Form 1040 must be signed and dated in order to be considered valid. If filing jointly with a spouse, both must sign and date. If a return is submitted electronically, individuals must use either a Self-Select PIN or Practitioner PIN.
If an individual decides not to file a return, the IRS may (after it has sent several reminders) file a substitute return.
For filing the regular tax return, in addition to the standard Form 1040, there are also two variants: the 1040NR and the 1040X.
Form 1040NR is used by taxpayers who are considered "non-resident aliens" for tax purposes.
Form 1040X (officially, the "Amended U.S. Individual Tax Return") is used to make corrections on Form 1040, Form 1040A, and Form 1040EZ tax returns that have been previously filed (note: the 1040A and 1040EZ were discontinued starting with tax year 2018, but a 1040X may still be filed amending one of these tax forms filed for previous years).
The 1040-V (officially, the "Payment Voucher for Form 1040") is used as an optional payment voucher to be sent in along with a payment for any balance due on the "Amount you owe" line of the 1040.
The form is entirely optional. The IRS will accept payment without the 1040V form. However including the 1040-V allows the IRS to process payments more efficiently.
Form 1040-V and any accompanying payment should be included in the same packet as the tax return, but should "not" be stapled or paper-clipped along with the tax return, since it is processed separately.
Since 1961 Form 1040 has had various separate attachments to the form. These attachments are usually called "schedules" because prior to the 1961, the related sections were schedules on the main form identified by letter. Form 1040 currently has 20 attachments, which may need to be filed depending on the taxpayer. For 2009 and 2010 there was an additional form, , due to the "Making Work Pay" provision of the American Recovery and Reinvestment Act of 2009 ("the stimulus").
Starting in 2018, 1040 was "simplified" by separating out 6 new schedules numbers Schedule 1 through Schedule 6 to make parts of the main form optional. The new schedules had the prior old 1040 line numbers to make transition easier.
In addition to the listed schedules, there are dozens of other forms that may be required when filing a personal income tax return. Typically these will provide additional details for deductions taken or income earned that are listed either on form 1040 or its subsequent schedules.
In 2014 there were two additions to Form 1040 due to the implementation of the Affordable Care Act—the premium tax credit and the individual mandate.
In most situations, other Internal Revenue Service or Social Security Administration forms such as Form W-2 must be attached to the Form 1040, in addition to the Form 1040 schedules. There are over 100 other specialized forms that may need to be completed along with Schedules and the Form 1040. However, Form 1099 need not be attached if no tax was withheld. In general, employer-sent forms are used to substantiate claims of withholding, so only forms that involve withholding need to be attached.
For most individuals, withholding is the main way through which taxes are paid. However, income that is not subject to withholding must be estimated using Form 1040-ES. (It may be possible to avoid filing Form 1040-ES by increasing one's withholding and instead filing a Form W-4.)
Estimated payments can be made using the Electronic Federal Tax Payment System.
There is a three-year limit to when individuals can claim a tax refund. However, payments that are due must be paid immediately.
In addition it is possible to apply one's refunds to next year's taxes and also to change one's mind later.
An automatic extension until October 15 to file Form 1040 can be obtained by filing Form 4868. There is a penalty for not filing a tax return by April 15 that depends on whether the individual got a filing extension and the amount of unpaid taxes. However, since the maximum penalty is 25% of unpaid taxes, if an individual has paid all their taxes, there is no penalty for not filing.
In addition to making sure that one pays one's taxes for the year by Tax Day, it is also important to make sure that one has paid partial taxes throughout the tax year in the form of estimated tax payments or employer tax withholding. If one has not done so, then a tax penalty may be assessed. The minimum amount of estimated taxes that need to be paid to avoid penalties depends on a variety of factors, including one's income in the tax year in question as well as one's income in the previous year (in general, if one pays 90% of the current year's tax liability or 100% of the previous year's tax liability during the tax year, one is not subject to estimated tax penalty even if this year's taxes are higher, but there are some caveats to that rule). Employer withholding is also treated differently from estimated tax payment, in that for the latter, the time of the year when the payment was made matters, whereas for the former, all that matters is how much has been withheld as of the end of the year (though there are other restrictions on how one can adjust one's withholding pattern that need to be enforced by the employer).
When filing Form 1040, the penalty for failing to pay estimated taxes must be included on the form (on line 79) and included in the total on line 78 (if a net payment is due). The taxpayer is not required to compute other interest and penalties (such as penalty for late filing or late payment of taxes). If the taxpayer does choose to compute these, the computed penalty can be listed on the bottom margin of page 2 of the form, but should not be included on the amount due line (line 78).
Each state has separate tax codes in addition to federal taxes. Form 1040 is only used for federal taxes, and state taxes should be filed separately based on the individual state's form. Some states do not have any income tax. Although state taxes are filed separately, many state tax returns will reference items from Form 1040. For example, California's 540 Resident Income Tax form makes a reference to Form 1040's line 37 in line 13.
Certain tax filing software, such as TurboTax, will simultaneously file state tax returns using information filled in on the 1040 form.
The Federal government allows individuals to deduct their state income tax or their state sales tax from their federal tax through Schedule A of Form 1040, but not both. In addition to deducting either income tax or sales tax, an individual can further deduct any state real estate taxes or private property taxes.
One argument used by tax protesters against the legitimacy of the 1040 Form is the OMB Control Number of the Paperwork Reduction Act argument. Tax protesters contend that Form 1040 does not contain an "OMB Control Number" which is issued by the U.S. Office of Management and Budget under the Paperwork Reduction Act.
The relevant clauses of the Paperwork Reduction Act state that:
The Courts have responded to the OMB Control Number arguments with the following arguments. 1) Form 1040, U.S. Individual Income Tax Return has contained the OMB Control number since 1981. 2) As ruled in a number of cases, the absence of an OMB Control number does not eliminate the legal obligation to file or pay taxes.
Cases involving the OMB Control Number Argument include:
The United States Court of Appeals for the Sixth Circuit argues that the provisions on the Paperwork Reduction Act are not relevant as the act applies only to information requests made after December 31, 1981 and tax returns starting from 1981 contained an OMB Control Number.
The United States Court of Appeals for the Seventh Circuit rejected the convicted taxpayer's OMB control number argument by stating "Finally, we have no doubt that the IRS has complied with the Paperwork Reduction Act. Form 1040 bears a control number from OMB, as do the other forms the IRS commonly distributes to taxpayers. That this number has been constant since 1981 does not imply that OMB has shirked its duty." 
In this Case, IRS agents who had calculated Mr. Lawrence's tax liability had made an error and it was discovered that Mr. Lawrence owed less taxes than originally determined. Lawrence asked the trial court to order the government to reimburse him for his legal fees, to which the trial court ruled against him. He appealed to the United States Court of Appeals for the Seventh Circuit, contending that the government's conduct against him had been "vexatious, frivolous, or in bad faith." and also raising the OMB Control Number Argument.
The United States Court of Appeals for the Seventh Circuit rejected the OMB argument stating that
According to Lawrence, the Paperwork Reduction Act of 1995 (PRA) required the Internal Revenue Service to display valid Office of Management and Budget (OMB) numbers on its Form 1040…. Lawrence argues that the PRA by its terms prohibits the government from imposing a criminal penalty upon a citizen for the failure to complete a form where the information request at issue does not comply with the PRA... Yet Lawrence conceded at oral argument that no case from this circuit establishes such a proposition, and in fact Lawrence cites no caselaw from any jurisdiction that so holds. In contrast, the government referenced numerous cases supporting its position that the PRA does not present a defense to a criminal action for failure to file income taxes." 
The first Form 1040 was published for use for the tax years 1913, 1914, and 1915;
the number 1040 was simply the next number in the sequential numbering of forms. For 1913, taxes applied only from March 1 to December 31. The original Form 1040, available on the IRS website as well as elsewhere, is three pages and 31 lines long, with the first page focused on computing one's income tax, the second page focused on more detailed documentation of one's income and the third page describing deductions and including a signature area. There is an additional page of instructions. The main rules were:
Just over 350,000 forms were filed in 1914 and all were audited.
For 1916, Form 1040 was converted to an annual form (i.e., updated each year with the new tax year printed on the form). Initially, the IRS mailed tax booklets (Form 1040, instructions, and most common attachments) to all households. As alternative delivery methods (CPA/Attorneys, Internet forms) increased in popularity, the IRS sent fewer packets via mail. In 2009 this practice was discontinued.
With the Current Tax Payment Act of 1943, income tax withholding was introduced. The Individual Income Tax Act of 1944 created standard deductions on the 1040.
The tax return deadline was original set at March 1. This was changed to March 15 in the Revenue Act of 1918, and in the Internal Revenue Code of 1954, the tax return deadline was changed to April 15 from March 15, as part of a large-scale overhaul of the tax code. The reason for March 1 was not explained in the law, but was presumably to give time after the end of the tax (and calendar) year to prepare tax returns. The two-week extension from March 1 to March 15 occurred after the Revenue Act of 1918 was passed in February 1919, given only a few weeks to complete returns under the new law. The month extension from March 15 to April 15 was to give additional time for taxpayers and accountants to prepare taxes, owing to the more complex tax code, and also helped spread work by the IRS over a longer time, as it would receive returns over a longer time.
The 1040A was introduced by the 1930s to simplify the filing process and discontinued after tax year 2017. It was limited to taxpayers with taxable income below $100,000 who take the standard deduction instead of itemizing deductions.
The 1040EZ was used for tax years 1982-2017. Its use was limited to taxpayers with no dependents to claim, with taxable income below $100,000 who take the standard deduction instead of itemizing deductions.
Electronic filing was introduced in a limited form in 1986, with the passage of the Tax Reform Act of 1986, and starting 1992, taxpayers who owed money were allowed to file electronically. The Electronic Federal Tax Payment System, jointly managed by the IRS and Financial Management Service, started in 1996 and allowed people to make estimated payments.
With the passage of the Tax Cuts and Jobs Act of 2017, a new, redesigned Form 1040 was released for tax year 2018. It reduced the number of lines from 79 to 23, removed two of the variants (1040A and 1040EZ) in favor of the redesigned Form 1040, and redesigned the supplemental schedules.
The complexity and compliance burden of the form and its associated instructions have increased considerably since 1913. The National Taxpayers Union has documented the steady increase in complexity from a 34-line form in 1935 to a 79-line form in 2014, decreasing to 23 lines in 2018. "Quartz" created an animated GIF showing the gradual changes to the structure and complexity of the form. The NTU table is below with data through 2014:
The number of pages in the federal tax law grew from 400 in 1913 to over 72,000 in 2011. The increase in complexity can be attributed to an increase in the number and range of activities being taxed, an increase in the number of exemptions, credits, and deductions available, an increase in the subtlety of the rules governing taxation and the edge cases explicitly spelled out based on historical experience, and an increase in the base of taxpayers making it necessary to offer longer, more explicit instructions for less sophisticated taxpayers. As an example, whereas the initial versions of Form 1040 came only with a rate schedule included in the tax form itself, the IRS now publishes a complete tax table for taxable income up to $100,000 so that people can directly look up their tax liability from their taxable income without having to do complicated arithmetic calculations based on the rate schedule. The IRS still publishes its rate schedule so that people can quickly compute their approximate tax liability, and lets people with incomes of over $100,000 compute their taxes directly using the Tax Computation Worksheet.
In addition to an increase in the complexity of the form, the tax rates have also increased, though the increase in tax rates has not been steady (with huge upswings and downswings) in contrast with the steady increase in tax complexity.
For tax return preparation, Americans spent roughly 20% of the amount collected in taxes (estimating the compliance costs and efficiency costs is difficult because neither the government nor taxpayers maintain regular accounts of these costs). As of 2013, there were more tax preparers in the US (1.2 million) than there were law enforcement officers (765 thousand) and firefighters (310,400) combined. The National Taxpayers Union estimated the 2018 compliance cost at 11 hours per form 1040 vs. 12 hours in 2017, with a total of $92.5 billion spent in individual income tax compliance vs. $94.27 billion in 2017. 
In 2008, 57.8% of tax returns were filed with assistance from paid tax preparers, compared to about 20% of taxpayers employing a paid preparer in the 1950s.

</doc>
